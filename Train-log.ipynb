{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b2f878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m********************************************************************************\n",
      "CIFAR10\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[32m********************************************************************************\n",
      "Creating model: vit-Base--CIFAR10-LR[0.001]-Seed0\n",
      "Number of params: 2,692,426\n",
      "Initial learning rate: 0.001000\n",
      "Start training for 100 epochs\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "label smoothing used\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Stochastic depth(0.1) used \n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Cutmix used\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Mixup used\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Repeated Aug(3) used\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Autoaugmentation used\n",
      "CIFAR Policy\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Random erasing(0.25) used \n",
      "********************************************************************************\u001b[0m\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Rearrange-1               [-1, 64, 48]               0\n",
      "            Linear-2              [-1, 64, 192]           9,408\n",
      "           Dropout-3              [-1, 65, 192]               0\n",
      "         LayerNorm-4              [-1, 65, 192]             384\n",
      "            Linear-5              [-1, 65, 576]         110,592\n",
      "           Softmax-6           [-1, 12, 65, 65]               0\n",
      "            Linear-7              [-1, 65, 192]          37,056\n",
      "           Dropout-8              [-1, 65, 192]               0\n",
      "         Attention-9              [-1, 65, 192]               0\n",
      "          PreNorm-10              [-1, 65, 192]               0\n",
      "         DropPath-11              [-1, 65, 192]               0\n",
      "        LayerNorm-12              [-1, 65, 192]             384\n",
      "           Linear-13              [-1, 65, 384]          74,112\n",
      "             GELU-14              [-1, 65, 384]               0\n",
      "          Dropout-15              [-1, 65, 384]               0\n",
      "           Linear-16              [-1, 65, 192]          73,920\n",
      "          Dropout-17              [-1, 65, 192]               0\n",
      "      FeedForward-18              [-1, 65, 192]               0\n",
      "          PreNorm-19              [-1, 65, 192]               0\n",
      "         DropPath-20              [-1, 65, 192]               0\n",
      "        LayerNorm-21              [-1, 65, 192]             384\n",
      "           Linear-22              [-1, 65, 576]         110,592\n",
      "          Softmax-23           [-1, 12, 65, 65]               0\n",
      "           Linear-24              [-1, 65, 192]          37,056\n",
      "          Dropout-25              [-1, 65, 192]               0\n",
      "        Attention-26              [-1, 65, 192]               0\n",
      "          PreNorm-27              [-1, 65, 192]               0\n",
      "         DropPath-28              [-1, 65, 192]               0\n",
      "        LayerNorm-29              [-1, 65, 192]             384\n",
      "           Linear-30              [-1, 65, 384]          74,112\n",
      "             GELU-31              [-1, 65, 384]               0\n",
      "          Dropout-32              [-1, 65, 384]               0\n",
      "           Linear-33              [-1, 65, 192]          73,920\n",
      "          Dropout-34              [-1, 65, 192]               0\n",
      "      FeedForward-35              [-1, 65, 192]               0\n",
      "          PreNorm-36              [-1, 65, 192]               0\n",
      "         DropPath-37              [-1, 65, 192]               0\n",
      "        LayerNorm-38              [-1, 65, 192]             384\n",
      "           Linear-39              [-1, 65, 576]         110,592\n",
      "          Softmax-40           [-1, 12, 65, 65]               0\n",
      "           Linear-41              [-1, 65, 192]          37,056\n",
      "          Dropout-42              [-1, 65, 192]               0\n",
      "        Attention-43              [-1, 65, 192]               0\n",
      "          PreNorm-44              [-1, 65, 192]               0\n",
      "         DropPath-45              [-1, 65, 192]               0\n",
      "        LayerNorm-46              [-1, 65, 192]             384\n",
      "           Linear-47              [-1, 65, 384]          74,112\n",
      "             GELU-48              [-1, 65, 384]               0\n",
      "          Dropout-49              [-1, 65, 384]               0\n",
      "           Linear-50              [-1, 65, 192]          73,920\n",
      "          Dropout-51              [-1, 65, 192]               0\n",
      "      FeedForward-52              [-1, 65, 192]               0\n",
      "          PreNorm-53              [-1, 65, 192]               0\n",
      "         DropPath-54              [-1, 65, 192]               0\n",
      "        LayerNorm-55              [-1, 65, 192]             384\n",
      "           Linear-56              [-1, 65, 576]         110,592\n",
      "          Softmax-57           [-1, 12, 65, 65]               0\n",
      "           Linear-58              [-1, 65, 192]          37,056\n",
      "          Dropout-59              [-1, 65, 192]               0\n",
      "        Attention-60              [-1, 65, 192]               0\n",
      "          PreNorm-61              [-1, 65, 192]               0\n",
      "         DropPath-62              [-1, 65, 192]               0\n",
      "        LayerNorm-63              [-1, 65, 192]             384\n",
      "           Linear-64              [-1, 65, 384]          74,112\n",
      "             GELU-65              [-1, 65, 384]               0\n",
      "          Dropout-66              [-1, 65, 384]               0\n",
      "           Linear-67              [-1, 65, 192]          73,920\n",
      "          Dropout-68              [-1, 65, 192]               0\n",
      "      FeedForward-69              [-1, 65, 192]               0\n",
      "          PreNorm-70              [-1, 65, 192]               0\n",
      "         DropPath-71              [-1, 65, 192]               0\n",
      "        LayerNorm-72              [-1, 65, 192]             384\n",
      "           Linear-73              [-1, 65, 576]         110,592\n",
      "          Softmax-74           [-1, 12, 65, 65]               0\n",
      "           Linear-75              [-1, 65, 192]          37,056\n",
      "          Dropout-76              [-1, 65, 192]               0\n",
      "        Attention-77              [-1, 65, 192]               0\n",
      "          PreNorm-78              [-1, 65, 192]               0\n",
      "         DropPath-79              [-1, 65, 192]               0\n",
      "        LayerNorm-80              [-1, 65, 192]             384\n",
      "           Linear-81              [-1, 65, 384]          74,112\n",
      "             GELU-82              [-1, 65, 384]               0\n",
      "          Dropout-83              [-1, 65, 384]               0\n",
      "           Linear-84              [-1, 65, 192]          73,920\n",
      "          Dropout-85              [-1, 65, 192]               0\n",
      "      FeedForward-86              [-1, 65, 192]               0\n",
      "          PreNorm-87              [-1, 65, 192]               0\n",
      "         DropPath-88              [-1, 65, 192]               0\n",
      "        LayerNorm-89              [-1, 65, 192]             384\n",
      "           Linear-90              [-1, 65, 576]         110,592\n",
      "          Softmax-91           [-1, 12, 65, 65]               0\n",
      "           Linear-92              [-1, 65, 192]          37,056\n",
      "          Dropout-93              [-1, 65, 192]               0\n",
      "        Attention-94              [-1, 65, 192]               0\n",
      "          PreNorm-95              [-1, 65, 192]               0\n",
      "         DropPath-96              [-1, 65, 192]               0\n",
      "        LayerNorm-97              [-1, 65, 192]             384\n",
      "           Linear-98              [-1, 65, 384]          74,112\n",
      "             GELU-99              [-1, 65, 384]               0\n",
      "         Dropout-100              [-1, 65, 384]               0\n",
      "          Linear-101              [-1, 65, 192]          73,920\n",
      "         Dropout-102              [-1, 65, 192]               0\n",
      "     FeedForward-103              [-1, 65, 192]               0\n",
      "         PreNorm-104              [-1, 65, 192]               0\n",
      "        DropPath-105              [-1, 65, 192]               0\n",
      "       LayerNorm-106              [-1, 65, 192]             384\n",
      "          Linear-107              [-1, 65, 576]         110,592\n",
      "         Softmax-108           [-1, 12, 65, 65]               0\n",
      "          Linear-109              [-1, 65, 192]          37,056\n",
      "         Dropout-110              [-1, 65, 192]               0\n",
      "       Attention-111              [-1, 65, 192]               0\n",
      "         PreNorm-112              [-1, 65, 192]               0\n",
      "        DropPath-113              [-1, 65, 192]               0\n",
      "       LayerNorm-114              [-1, 65, 192]             384\n",
      "          Linear-115              [-1, 65, 384]          74,112\n",
      "            GELU-116              [-1, 65, 384]               0\n",
      "         Dropout-117              [-1, 65, 384]               0\n",
      "          Linear-118              [-1, 65, 192]          73,920\n",
      "         Dropout-119              [-1, 65, 192]               0\n",
      "     FeedForward-120              [-1, 65, 192]               0\n",
      "         PreNorm-121              [-1, 65, 192]               0\n",
      "        DropPath-122              [-1, 65, 192]               0\n",
      "       LayerNorm-123              [-1, 65, 192]             384\n",
      "          Linear-124              [-1, 65, 576]         110,592\n",
      "         Softmax-125           [-1, 12, 65, 65]               0\n",
      "          Linear-126              [-1, 65, 192]          37,056\n",
      "         Dropout-127              [-1, 65, 192]               0\n",
      "       Attention-128              [-1, 65, 192]               0\n",
      "         PreNorm-129              [-1, 65, 192]               0\n",
      "        DropPath-130              [-1, 65, 192]               0\n",
      "       LayerNorm-131              [-1, 65, 192]             384\n",
      "          Linear-132              [-1, 65, 384]          74,112\n",
      "            GELU-133              [-1, 65, 384]               0\n",
      "         Dropout-134              [-1, 65, 384]               0\n",
      "          Linear-135              [-1, 65, 192]          73,920\n",
      "         Dropout-136              [-1, 65, 192]               0\n",
      "     FeedForward-137              [-1, 65, 192]               0\n",
      "         PreNorm-138              [-1, 65, 192]               0\n",
      "        DropPath-139              [-1, 65, 192]               0\n",
      "       LayerNorm-140              [-1, 65, 192]             384\n",
      "          Linear-141              [-1, 65, 576]         110,592\n",
      "         Softmax-142           [-1, 12, 65, 65]               0\n",
      "          Linear-143              [-1, 65, 192]          37,056\n",
      "         Dropout-144              [-1, 65, 192]               0\n",
      "       Attention-145              [-1, 65, 192]               0\n",
      "         PreNorm-146              [-1, 65, 192]               0\n",
      "        DropPath-147              [-1, 65, 192]               0\n",
      "       LayerNorm-148              [-1, 65, 192]             384\n",
      "          Linear-149              [-1, 65, 384]          74,112\n",
      "            GELU-150              [-1, 65, 384]               0\n",
      "         Dropout-151              [-1, 65, 384]               0\n",
      "          Linear-152              [-1, 65, 192]          73,920\n",
      "         Dropout-153              [-1, 65, 192]               0\n",
      "     FeedForward-154              [-1, 65, 192]               0\n",
      "         PreNorm-155              [-1, 65, 192]               0\n",
      "        DropPath-156              [-1, 65, 192]               0\n",
      "     Transformer-157              [-1, 65, 192]               0\n",
      "       LayerNorm-158                  [-1, 192]             384\n",
      "          Linear-159                   [-1, 10]           1,930\n",
      "================================================================\n",
      "Total params: 2,679,754\n",
      "Trainable params: 2,679,754\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 21.79\n",
      "Params size (MB): 10.22\n",
      "Estimated Total Size (MB): 32.02\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Beginning training\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/100][T][1170]   Loss: 2.2505e+00   Top-1:  17.54   LR: 0.0001009        Epoch 1/100][T][0]   Loss: 3.1346e+00   Top-1:  10.94   LR: 0.0000011        \n",
      "[Epoch 1][V][78]   Loss: 1.8974e+00   Top-1:  35.94   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.250474261382418\n",
      "T Top-1\t17.535760034158837\n",
      "V Loss\t1.8973592422485352\n",
      "V Top-1\t35.94\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 35.94\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 2/100][T][1170]   Loss: 2.0921e+00   Top-1:  25.30   LR: 0.0002008        Epoch 2/100][T][0]   Loss: 2.2649e+00   Top-1:  14.06   LR: 0.0001010        \n",
      "[Epoch 2][V][78]   Loss: 1.7166e+00   Top-1:  45.50   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.0920500877471584\n",
      "T Top-1\t25.304894321093084\n",
      "V Loss\t1.7166284938812255\n",
      "V Top-1\t45.5\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 45.50\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 3/100][T][1170]   Loss: 1.9728e+00   Top-1:  31.30   LR: 0.0003007        Epoch 3/100][T][0]   Loss: 2.0266e+00   Top-1:  29.69   LR: 0.0002009        \n",
      "[Epoch 3][V][78]   Loss: 1.5381e+00   Top-1:  53.47   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.9728415814553644\n",
      "T Top-1\t31.295367207514943\n",
      "V Loss\t1.538080843925476\n",
      "V Top-1\t53.47\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 53.47\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 4/100][T][1170]   Loss: 1.8999e+00   Top-1:  35.52   LR: 0.0004006        Epoch 4/100][T][0]   Loss: 2.1327e+00   Top-1:  28.91   LR: 0.0003008        \n",
      "[Epoch 4][V][78]   Loss: 1.4425e+00   Top-1:  57.79   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.8999199833654115\n",
      "T Top-1\t35.520521989752346\n",
      "V Loss\t1.4425483123779297\n",
      "V Top-1\t57.79\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 57.79\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 5/100][T][1170]   Loss: 1.8381e+00   Top-1:  38.16   LR: 0.0005005        Epoch 5/100][T][0]   Loss: 1.7570e+00   Top-1:  42.19   LR: 0.0004007        \n",
      "[Epoch 5][V][78]   Loss: 1.3683e+00   Top-1:  60.87   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.8381003214700113\n",
      "T Top-1\t38.16382899231426\n",
      "V Loss\t1.3682823657989502\n",
      "V Top-1\t60.87\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 60.87\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 6/100][T][1170]   Loss: 1.7852e+00   Top-1:  40.59   LR: 0.0006004        Epoch 6/100][T][0]   Loss: 1.6336e+00   Top-1:  53.91   LR: 0.0005006        \n",
      "[Epoch 6][V][78]   Loss: 1.3178e+00   Top-1:  64.40   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.785190818556143\n",
      "T Top-1\t40.591641759180185\n",
      "V Loss\t1.3177614089965821\n",
      "V Top-1\t64.4\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 64.40\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 7/100][T][1170]   Loss: 1.7336e+00   Top-1:  43.82   LR: 0.0007003        Epoch 7/100][T][0]   Loss: 1.9767e+00   Top-1:  41.41   LR: 0.0006005        \n",
      "[Epoch 7][V][78]   Loss: 1.2847e+00   Top-1:  64.70   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.733556668642339\n",
      "T Top-1\t43.816716481639624\n",
      "V Loss\t1.284723405456543\n",
      "V Top-1\t64.7\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 64.70\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 8/100][T][1170]   Loss: 1.7144e+00   Top-1:  44.54   LR: 0.0008002        Epoch 8/100][T][0]   Loss: 1.6771e+00   Top-1:  46.09   LR: 0.0007004        \n",
      "[Epoch 8][V][78]   Loss: 1.2303e+00   Top-1:  68.60   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.714422896415326\n",
      "T Top-1\t44.543926131511526\n",
      "V Loss\t1.2303453992843627\n",
      "V Top-1\t68.6\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 68.60\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 9/100][T][1170]   Loss: 1.6632e+00   Top-1:  46.96   LR: 0.0009001        Epoch 9/100][T][0]   Loss: 1.4816e+00   Top-1:  54.69   LR: 0.0008003        \n",
      "[Epoch 9][V][78]   Loss: 1.1856e+00   Top-1:  69.75   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.6631825180973994\n",
      "T Top-1\t46.95639410760034\n",
      "V Loss\t1.1855929502487184\n",
      "V Top-1\t69.75\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 69.75\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 10/100][T][1170]   Loss: 1.6531e+00   Top-1:  47.24   LR: 0.0010000        poch 10/100][T][0]   Loss: 1.4106e+00   Top-1:  59.38   LR: 0.0009002        \n",
      "[Epoch 10][V][78]   Loss: 1.1802e+00   Top-1:  70.85   LR: 0.0010\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.653083383171707\n",
      "T Top-1\t47.24394214346712\n",
      "V Loss\t1.1802019207000733\n",
      "V Top-1\t70.85\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 70.85\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 11/100][T][1170]   Loss: 1.6202e+00   Top-1:  48.98   LR: 0.0009997        poch 11/100][T][0]   Loss: 1.3461e+00   Top-1:  60.16   LR: 0.0010000        \n",
      "[Epoch 11][V][78]   Loss: 1.1267e+00   Top-1:  72.84   LR: 0.0010\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.6201692535569787\n",
      "T Top-1\t48.97590200683177\n",
      "V Loss\t1.1266547723770142\n",
      "V Top-1\t72.84\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 72.84\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 12/100][T][1170]   Loss: 1.5828e+00   Top-1:  50.95   LR: 0.0009988        poch 12/100][T][0]   Loss: 1.3361e+00   Top-1:  64.84   LR: 0.0009997        \n",
      "[Epoch 12][V][78]   Loss: 1.0865e+00   Top-1:  74.65   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.5828053337401757\n",
      "T Top-1\t50.946706874466265\n",
      "V Loss\t1.0864521545410157\n",
      "V Top-1\t74.65\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 74.65\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 13/100][T][1170]   Loss: 1.5606e+00   Top-1:  51.00   LR: 0.0009973        poch 13/100][T][0]   Loss: 1.3628e+00   Top-1:  58.59   LR: 0.0009988        \n",
      "[Epoch 13][V][78]   Loss: 1.0502e+00   Top-1:  76.05   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.5605883784623964\n",
      "T Top-1\t51.00408304867634\n",
      "V Loss\t1.0501767581939698\n",
      "V Top-1\t76.05\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 76.05\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 14/100][T][1170]   Loss: 1.5248e+00   Top-1:  54.09   LR: 0.0009951        poch 14/100][T][0]   Loss: 1.2470e+00   Top-1:  68.75   LR: 0.0009973        \n",
      "[Epoch 14][V][78]   Loss: 1.0171e+00   Top-1:  77.34   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.5247529923254155\n",
      "T Top-1\t54.091721818958156\n",
      "V Loss\t1.0170807676315308\n",
      "V Top-1\t77.34\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 77.34\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 15/100][T][1170]   Loss: 1.5189e+00   Top-1:  53.57   LR: 0.0009924        poch 15/100][T][0]   Loss: 1.2039e+00   Top-1:  67.19   LR: 0.0009951        \n",
      "[Epoch 15][V][78]   Loss: 1.0117e+00   Top-1:  77.15   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.5188961778310097\n",
      "T Top-1\t53.56999893253629\n",
      "V Loss\t1.01169203748703\n",
      "V Top-1\t77.15\n",
      "\n",
      "Best acc1 77.34\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 16/100][T][1170]   Loss: 1.5033e+00   Top-1:  54.40   LR: 0.0009891        poch 16/100][T][0]   Loss: 2.0360e+00   Top-1:  39.06   LR: 0.0009924        \n",
      "[Epoch 16][V][78]   Loss: 9.8404e-01   Top-1:  78.64   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T Loss\t1.5033039617090527\n",
      "T Top-1\t54.395948975234845\n",
      "V Loss\t0.9840381418228149\n",
      "V Top-1\t78.64\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 78.64\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 17/100][T][1170]   Loss: 1.4814e+00   Top-1:  55.44   LR: 0.0009852        poch 17/100][T][0]   Loss: 1.0967e+00   Top-1:  75.00   LR: 0.0009891        \n",
      "[Epoch 17][V][78]   Loss: 1.0102e+00   Top-1:  77.31   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.4813993671936017\n",
      "T Top-1\t55.44339773697694\n",
      "V Loss\t1.0101693866729737\n",
      "V Top-1\t77.31\n",
      "\n",
      "Best acc1 78.64\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 18/100][T][1170]   Loss: 1.4582e+00   Top-1:  57.14   LR: 0.0009807        poch 18/100][T][0]   Loss: 1.7595e+00   Top-1:  60.94   LR: 0.0009852        \n",
      "[Epoch 18][V][78]   Loss: 9.6793e-01   Top-1:  79.93   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.458193948796221\n",
      "T Top-1\t57.13666204099061\n",
      "V Loss\t0.9679298168182373\n",
      "V Top-1\t79.93\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 79.93\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 19/100][T][1170]   Loss: 1.4523e+00   Top-1:  57.19   LR: 0.0009756        poch 19/100][T][0]   Loss: 1.9641e+00   Top-1:  31.25   LR: 0.0009806        \n",
      "[Epoch 19][V][78]   Loss: 9.2668e-01   Top-1:  81.50   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.4522506548847325\n",
      "T Top-1\t57.19270388556789\n",
      "V Loss\t0.9266829402923584\n",
      "V Top-1\t81.5\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 81.50\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 20/100][T][1170]   Loss: 1.4393e+00   Top-1:  56.69   LR: 0.0009699        poch 20/100][T][0]   Loss: 1.0945e+00   Top-1:  73.44   LR: 0.0009755        \n",
      "[Epoch 20][V][78]   Loss: 9.1848e-01   Top-1:  81.93   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.439339585532295\n",
      "T Top-1\t56.690995943637915\n",
      "V Loss\t0.9184792728424073\n",
      "V Top-1\t81.93\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 81.93\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 21/100][T][1170]   Loss: 1.4178e+00   Top-1:  58.97   LR: 0.0009636        poch 21/100][T][0]   Loss: 2.0070e+00   Top-1:  25.00   LR: 0.0009699        \n",
      "[Epoch 21][V][78]   Loss: 9.1620e-01   Top-1:  82.58   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.4178201263018086\n",
      "T Top-1\t58.96936379163108\n",
      "V Loss\t0.9161968230247498\n",
      "V Top-1\t82.58\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 82.58\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 22/100][T][1170]   Loss: 1.4129e+00   Top-1:  59.42   LR: 0.0009568        poch 22/100][T][0]   Loss: 1.1067e+00   Top-1:  74.22   LR: 0.0009636        \n",
      "[Epoch 22][V][78]   Loss: 9.2270e-01   Top-1:  81.82   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.4128728187766266\n",
      "T Top-1\t59.41569705380017\n",
      "V Loss\t0.9227023456573487\n",
      "V Top-1\t81.82\n",
      "\n",
      "Best acc1 82.58\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 23/100][T][1170]   Loss: 1.4021e+00   Top-1:  59.27   LR: 0.0009494        poch 23/100][T][0]   Loss: 1.1661e+00   Top-1:  67.97   LR: 0.0009568        \n",
      "[Epoch 23][V][78]   Loss: 8.9874e-01   Top-1:  82.81   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.4021354515159568\n",
      "T Top-1\t59.27225661827498\n",
      "V Loss\t0.8987409000396729\n",
      "V Top-1\t82.81\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 82.81\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 24/100][T][1170]   Loss: 1.4140e+00   Top-1:  58.62   LR: 0.0009415        poch 24/100][T][0]   Loss: 1.1234e+00   Top-1:  70.31   LR: 0.0009494        \n",
      "[Epoch 24][V][78]   Loss: 8.9473e-01   Top-1:  83.20   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.4140197371031666\n",
      "T Top-1\t58.61910226302306\n",
      "V Loss\t0.8947263465881348\n",
      "V Top-1\t83.2\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 83.20\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 25/100][T][1170]   Loss: 1.3965e+00   Top-1:  59.39   LR: 0.0009331        poch 25/100][T][0]   Loss: 1.8501e+00   Top-1:  55.47   LR: 0.0009415        \n",
      "[Epoch 25][V][78]   Loss: 8.7508e-01   Top-1:  83.78   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3964989084040986\n",
      "T Top-1\t59.386341801878736\n",
      "V Loss\t0.8750804381370545\n",
      "V Top-1\t83.78\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 83.78\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 26/100][T][1170]   Loss: 1.3791e+00   Top-1:  60.10   LR: 0.0009241        poch 26/100][T][0]   Loss: 1.0266e+00   Top-1:  71.88   LR: 0.0009331        \n",
      "[Epoch 26][V][78]   Loss: 8.8713e-01   Top-1:  83.58   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3791412396068556\n",
      "T Top-1\t60.10020815542271\n",
      "V Loss\t0.8871282409667969\n",
      "V Top-1\t83.58\n",
      "\n",
      "Best acc1 83.78\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 27/100][T][1170]   Loss: 1.3856e+00   Top-1:  59.60   LR: 0.0009146        poch 27/100][T][0]   Loss: 1.2346e+00   Top-1:  67.19   LR: 0.0009241        \n",
      "[Epoch 27][V][78]   Loss: 8.8696e-01   Top-1:  83.14   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3856491941249238\n",
      "T Top-1\t59.59516438941076\n",
      "V Loss\t0.8869584426879883\n",
      "V Top-1\t83.14\n",
      "\n",
      "Best acc1 83.78\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 28/100][T][1170]   Loss: 1.3722e+00   Top-1:  59.95   LR: 0.0009046        poch 28/100][T][0]   Loss: 1.9593e+00   Top-1:  49.22   LR: 0.0009146        \n",
      "[Epoch 28][V][78]   Loss: 8.6552e-01   Top-1:  84.83   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3721675077225794\n",
      "T Top-1\t59.95209756618275\n",
      "V Loss\t0.8655201482772827\n",
      "V Top-1\t84.83\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 84.83\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 29/100][T][1170]   Loss: 1.3686e+00   Top-1:  60.45   LR: 0.0008941        poch 29/100][T][0]   Loss: 9.8182e-01   Top-1:  82.81   LR: 0.0009046        \n",
      "[Epoch 29][V][78]   Loss: 8.5961e-01   Top-1:  84.54   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.368642235596296\n",
      "T Top-1\t60.45113684884714\n",
      "V Loss\t0.8596122212409973\n",
      "V Top-1\t84.54\n",
      "\n",
      "Best acc1 84.83\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 30/100][T][1170]   Loss: 1.3382e+00   Top-1:  61.68   LR: 0.0008831        poch 30/100][T][0]   Loss: 1.8388e+00   Top-1:  22.66   LR: 0.0008941        \n",
      "[Epoch 30][V][78]   Loss: 8.4590e-01   Top-1:  85.55   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3382322199827172\n",
      "T Top-1\t61.684057429547394\n",
      "V Loss\t0.845898715209961\n",
      "V Top-1\t85.55\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 85.55\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 31/100][T][1170]   Loss: 1.3459e+00   Top-1:  61.60   LR: 0.0008717        poch 31/100][T][0]   Loss: 1.8283e+00   Top-1:  53.12   LR: 0.0008831        \n",
      "[Epoch 31][V][78]   Loss: 8.3866e-01   Top-1:  85.74   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3459260090612937\n",
      "T Top-1\t61.59799316823228\n",
      "V Loss\t0.8386630579948425\n",
      "V Top-1\t85.74\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 85.74\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 32/100][T][1170]   Loss: 1.3569e+00   Top-1:  60.71   LR: 0.0008598        poch 32/100][T][0]   Loss: 1.5522e+00   Top-1:  71.88   LR: 0.0008717        \n",
      "[Epoch 32][V][78]   Loss: 8.4810e-01   Top-1:  86.25   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3569336541181543\n",
      "T Top-1\t60.70866246797609\n",
      "V Loss\t0.8480967276573181\n",
      "V Top-1\t86.25\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 86.25\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 33/100][T][1170]   Loss: 1.3353e+00   Top-1:  62.13   LR: 0.0008475        poch 33/100][T][0]   Loss: 1.9721e+00   Top-1:  42.97   LR: 0.0008598        \n",
      "[Epoch 33][V][78]   Loss: 8.2659e-01   Top-1:  86.07   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3353488271710814\n",
      "T Top-1\t62.13039069171648\n",
      "V Loss\t0.8265907174110413\n",
      "V Top-1\t86.07\n",
      "\n",
      "Best acc1 86.25\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 34/100][T][1170]   Loss: 1.3209e+00   Top-1:  63.03   LR: 0.0008347        poch 34/100][T][0]   Loss: 1.0044e+00   Top-1:  78.12   LR: 0.0008475        \n",
      "[Epoch 34][V][78]   Loss: 8.2897e-01   Top-1:  86.13   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.320902189721825\n",
      "T Top-1\t63.02906169940222\n",
      "V Loss\t0.8289726627349854\n",
      "V Top-1\t86.13\n",
      "\n",
      "Best acc1 86.25\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 35/100][T][1170]   Loss: 1.3042e+00   Top-1:  63.89   LR: 0.0008216        poch 35/100][T][0]   Loss: 1.8520e+00   Top-1:  58.59   LR: 0.0008347        \n",
      "[Epoch 35][V][78]   Loss: 8.1579e-01   Top-1:  86.52   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3041879988755047\n",
      "T Top-1\t63.88770281810419\n",
      "V Loss\t0.8157932437896729\n",
      "V Top-1\t86.52\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 86.52\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 36/100][T][1170]   Loss: 1.2920e+00   Top-1:  63.88   LR: 0.0008080        poch 36/100][T][0]   Loss: 9.5226e-01   Top-1:  82.03   LR: 0.0008216        \n",
      "[Epoch 36][V][78]   Loss: 8.0757e-01   Top-1:  87.04   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.291990063263757\n",
      "T Top-1\t63.88236549957301\n",
      "V Loss\t0.8075671722412109\n",
      "V Top-1\t87.04\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 87.04\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 37/100][T][1170]   Loss: 1.2987e+00   Top-1:  63.95   LR: 0.0007941        poch 37/100][T][0]   Loss: 1.5475e+00   Top-1:  14.06   LR: 0.0008080        \n",
      "[Epoch 37][V][78]   Loss: 8.1300e-01   Top-1:  86.83   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2987077415854782\n",
      "T Top-1\t63.95241780529462\n",
      "V Loss\t0.8129990978240966\n",
      "V Top-1\t86.83\n",
      "\n",
      "Best acc1 87.04\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 38/100][T][1170]   Loss: 1.3183e+00   Top-1:  62.70   LR: 0.0007798        poch 38/100][T][0]   Loss: 1.2564e+00   Top-1:  82.81   LR: 0.0007941        \n",
      "[Epoch 38][V][78]   Loss: 7.9576e-01   Top-1:  87.41   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3183414743152264\n",
      "T Top-1\t62.70281810418446\n",
      "V Loss\t0.7957579406738281\n",
      "V Top-1\t87.41\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 87.41\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 39/100][T][1170]   Loss: 1.2879e+00   Top-1:  63.79   LR: 0.0007652        poch 39/100][T][0]   Loss: 1.1529e+00   Top-1:  84.38   LR: 0.0007798        \n",
      "[Epoch 39][V][78]   Loss: 8.3878e-01   Top-1:  87.09   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.287895533338151\n",
      "T Top-1\t63.793632578992316\n",
      "V Loss\t0.8387823631286621\n",
      "V Top-1\t87.09\n",
      "\n",
      "Best acc1 87.41\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 40/100][T][1170]   Loss: 1.2910e+00   Top-1:  64.67   LR: 0.0007503        poch 40/100][T][0]   Loss: 1.8929e+00   Top-1:  42.19   LR: 0.0007652        \n",
      "[Epoch 40][V][78]   Loss: 7.8591e-01   Top-1:  88.00   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2909535108011672\n",
      "T Top-1\t64.67495730145176\n",
      "V Loss\t0.7859075500488282\n",
      "V Top-1\t88.0\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 88.00\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 41/100][T][1170]   Loss: 1.3068e+00   Top-1:  63.81   LR: 0.0007350        poch 41/100][T][0]   Loss: 1.9802e+00   Top-1:  32.81   LR: 0.0007502        \n",
      "[Epoch 41][V][78]   Loss: 7.7893e-01   Top-1:  87.72   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3068347232246074\n",
      "T Top-1\t63.814981853117\n",
      "V Loss\t0.7789268871307373\n",
      "V Top-1\t87.72\n",
      "\n",
      "Best acc1 88.00\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 42/100][T][1170]   Loss: 1.2817e+00   Top-1:  63.98   LR: 0.0007195        poch 42/100][T][0]   Loss: 1.5805e+00   Top-1:  13.28   LR: 0.0007350        \n",
      "[Epoch 42][V][78]   Loss: 7.9241e-01   Top-1:  87.75   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.281737774025162\n",
      "T Top-1\t63.979771562766864\n",
      "V Loss\t0.7924084987640381\n",
      "V Top-1\t87.75\n",
      "\n",
      "Best acc1 88.00\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 43/100][T][1170]   Loss: 1.2807e+00   Top-1:  62.96   LR: 0.0007037        poch 43/100][T][0]   Loss: 1.7653e+00   Top-1:  15.62   LR: 0.0007195        \n",
      "[Epoch 43][V][78]   Loss: 7.8317e-01   Top-1:  87.99   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.280744155129238\n",
      "T Top-1\t62.95567356959863\n",
      "V Loss\t0.7831705557823181\n",
      "V Top-1\t87.99\n",
      "\n",
      "Best acc1 88.00\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 44/100][T][1170]   Loss: 1.2623e+00   Top-1:  64.46   LR: 0.0006876        poch 44/100][T][0]   Loss: 2.0569e+00   Top-1:  33.59   LR: 0.0007037        \n",
      "[Epoch 44][V][78]   Loss: 7.6378e-01   Top-1:  89.14   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.262330673526842\n",
      "T Top-1\t64.46346605465415\n",
      "V Loss\t0.7637824493408203\n",
      "V Top-1\t89.14\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 89.14\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 45/100][T][1170]   Loss: 1.2568e+00   Top-1:  65.70   LR: 0.0006713        poch 45/100][T][0]   Loss: 1.4746e+00   Top-1:  72.66   LR: 0.0006876        \n",
      "[Epoch 45][V][78]   Loss: 7.8775e-01   Top-1:  87.48   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2568238488534493\n",
      "T Top-1\t65.69838812980359\n",
      "V Loss\t0.7877469869613647\n",
      "V Top-1\t87.48\n",
      "\n",
      "Best acc1 89.14\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 46/100][T][1170]   Loss: 1.2539e+00   Top-1:  64.99   LR: 0.0006549        poch 46/100][T][0]   Loss: 8.3843e-01   Top-1:  86.72   LR: 0.0006713        \n",
      "[Epoch 46][V][78]   Loss: 8.0919e-01   Top-1:  87.44   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2539017927331053\n",
      "T Top-1\t64.99386208368915\n",
      "V Loss\t0.8091871906280518\n",
      "V Top-1\t87.44\n",
      "\n",
      "Best acc1 89.14\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 47/100][T][1170]   Loss: 1.2480e+00   Top-1:  66.44   LR: 0.0006382        poch 47/100][T][0]   Loss: 8.5804e-01   Top-1:  89.06   LR: 0.0006548        \n",
      "[Epoch 47][V][78]   Loss: 7.8315e-01   Top-1:  88.20   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.247968460534191\n",
      "T Top-1\t66.43960824081981\n",
      "V Loss\t0.7831508092880249\n",
      "V Top-1\t88.2\n",
      "\n",
      "Best acc1 89.14\n",
      "********************************************************************************\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 48/100][T][1170]   Loss: 1.2440e+00   Top-1:  66.57   LR: 0.0006213        poch 48/100][T][0]   Loss: 9.2912e-01   Top-1:  82.81   LR: 0.0006382        \n",
      "[Epoch 48][V][78]   Loss: 7.5901e-01   Top-1:  88.89   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2440069053643388\n",
      "T Top-1\t66.56970538001708\n",
      "V Loss\t0.7590099177360534\n",
      "V Top-1\t88.89\n",
      "\n",
      "Best acc1 89.14\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 49/100][T][1170]   Loss: 1.2596e+00   Top-1:  65.92   LR: 0.0006044        poch 49/100][T][0]   Loss: 1.5829e+00   Top-1:  17.19   LR: 0.0006213        \n",
      "[Epoch 49][V][78]   Loss: 7.6014e-01   Top-1:  88.98   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2595880653591465\n",
      "T Top-1\t65.92122117847993\n",
      "V Loss\t0.7601448273658753\n",
      "V Top-1\t88.98\n",
      "\n",
      "Best acc1 89.14\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 50/100][T][1170]   Loss: 1.2096e+00   Top-1:  67.61   LR: 0.0005872        poch 50/100][T][0]   Loss: 1.5098e+00   Top-1:  74.22   LR: 0.0006043        \n",
      "[Epoch 50][V][78]   Loss: 7.5723e-01   Top-1:  89.35   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2095817547899144\n",
      "T Top-1\t67.60981532877882\n",
      "V Loss\t0.75723314037323\n",
      "V Top-1\t89.35\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 89.35\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 51/100][T][1170]   Loss: 1.2278e+00   Top-1:  66.90   LR: 0.0005700        poch 51/100][T][0]   Loss: 9.6840e-01   Top-1:  79.69   LR: 0.0005872        \n",
      "[Epoch 51][V][78]   Loss: 7.5900e-01   Top-1:  89.40   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2277923812731832\n",
      "T Top-1\t66.90061912894961\n",
      "V Loss\t0.7589991432189941\n",
      "V Top-1\t89.4\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 89.40\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 52/100][T][1170]   Loss: 1.2320e+00   Top-1:  66.77   LR: 0.0005527        poch 52/100][T][0]   Loss: 1.4142e+00   Top-1:  71.88   LR: 0.0005700        \n",
      "[Epoch 52][V][78]   Loss: 7.6532e-01   Top-1:  89.37   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2319967195989738\n",
      "T Top-1\t66.76852049530316\n",
      "V Loss\t0.7653245414733887\n",
      "V Top-1\t89.37\n",
      "\n",
      "Best acc1 89.40\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 53/100][T][1170]   Loss: 1.2312e+00   Top-1:  67.29   LR: 0.0005353        poch 53/100][T][0]   Loss: 8.3316e-01   Top-1:  85.94   LR: 0.0005527        \n",
      "[Epoch 53][V][78]   Loss: 7.4613e-01   Top-1:  89.76   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2311534011699927\n",
      "T Top-1\t67.28557322801025\n",
      "V Loss\t0.7461328682899475\n",
      "V Top-1\t89.76\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 89.76\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 54/100][T][1170]   Loss: 1.2188e+00   Top-1:  67.84   LR: 0.0005179        poch 54/100][T][0]   Loss: 1.0777e+00   Top-1:  11.72   LR: 0.0005353        \n",
      "[Epoch 54][V][78]   Loss: 7.4832e-01   Top-1:  89.31   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2187907496748778\n",
      "T Top-1\t67.84265584970112\n",
      "V Loss\t0.7483176441192627\n",
      "V Top-1\t89.31\n",
      "\n",
      "Best acc1 89.76\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 55/100][T][1170]   Loss: 1.2054e+00   Top-1:  68.64   LR: 0.0005005        poch 55/100][T][0]   Loss: 1.0470e+00   Top-1:  80.47   LR: 0.0005179        \n",
      "[Epoch 55][V][78]   Loss: 7.5075e-01   Top-1:  89.28   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2053577856158315\n",
      "T Top-1\t68.63991780529462\n",
      "V Loss\t0.7507454526901245\n",
      "V Top-1\t89.28\n",
      "\n",
      "Best acc1 89.76\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 56/100][T][1170]   Loss: 1.1900e+00   Top-1:  68.85   LR: 0.0004831        poch 56/100][T][0]   Loss: 7.7047e-01   Top-1:  89.06   LR: 0.0005005        \n",
      "[Epoch 56][V][78]   Loss: 7.6595e-01   Top-1:  89.52   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1899622633455147\n",
      "T Top-1\t68.84740606319386\n",
      "V Loss\t0.7659471921920776\n",
      "V Top-1\t89.52\n",
      "\n",
      "Best acc1 89.76\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 57/100][T][1170]   Loss: 1.1934e+00   Top-1:  68.48   LR: 0.0004657        poch 57/100][T][0]   Loss: 8.9244e-01   Top-1:  82.81   LR: 0.0004831        \n",
      "[Epoch 57][V][78]   Loss: 7.3212e-01   Top-1:  90.60   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1934016740393374\n",
      "T Top-1\t68.47979824935952\n",
      "V Loss\t0.7321245965957641\n",
      "V Top-1\t90.6\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 90.60\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 58/100][T][1170]   Loss: 1.2043e+00   Top-1:  68.27   LR: 0.0004483        poch 58/100][T][0]   Loss: 1.5165e+00   Top-1:  74.22   LR: 0.0004656        \n",
      "[Epoch 58][V][78]   Loss: 7.4598e-01   Top-1:  89.81   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2042813680296165\n",
      "T Top-1\t68.27431148590948\n",
      "V Loss\t0.7459787824630737\n",
      "V Top-1\t89.81\n",
      "\n",
      "Best acc1 90.60\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 59/100][T][1170]   Loss: 1.2190e+00   Top-1:  66.89   LR: 0.0004310        poch 59/100][T][0]   Loss: 2.0069e+00   Top-1:  49.22   LR: 0.0004483        \n",
      "[Epoch 59][V][78]   Loss: 7.3107e-01   Top-1:  90.34   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2189970063817084\n",
      "T Top-1\t66.8859415029889\n",
      "V Loss\t0.7310652528762818\n",
      "V Top-1\t90.34\n",
      "\n",
      "Best acc1 90.60\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 60/100][T][1170]   Loss: 1.1875e+00   Top-1:  68.20   LR: 0.0004138        poch 60/100][T][0]   Loss: 8.4698e-01   Top-1:  85.16   LR: 0.0004310        \n",
      "[Epoch 60][V][78]   Loss: 7.2946e-01   Top-1:  90.57   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1874954727447309\n",
      "T Top-1\t68.1995890264731\n",
      "V Loss\t0.7294556265830994\n",
      "V Top-1\t90.57\n",
      "\n",
      "Best acc1 90.60\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 61/100][T][1170]   Loss: 1.1846e+00   Top-1:  69.20   LR: 0.0003966        poch 61/100][T][0]   Loss: 1.9585e+00   Top-1:  43.75   LR: 0.0004137        \n",
      "[Epoch 61][V][78]   Loss: 7.3556e-01   Top-1:  90.91   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1846309472209464\n",
      "T Top-1\t69.20233774551666\n",
      "V Loss\t0.735557534790039\n",
      "V Top-1\t90.91\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 90.91\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 62/100][T][1170]   Loss: 1.1640e+00   Top-1:  69.63   LR: 0.0003797        poch 62/100][T][0]   Loss: 8.8425e-01   Top-1:  81.25   LR: 0.0003966        \n",
      "[Epoch 62][V][78]   Loss: 7.1002e-01   Top-1:  91.02   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1639700946290712\n",
      "T Top-1\t69.63265905209222\n",
      "V Loss\t0.7100160099029541\n",
      "V Top-1\t91.02\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 91.02\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 63/100][T][1170]   Loss: 1.1799e+00   Top-1:  68.89   LR: 0.0003628        poch 63/100][T][0]   Loss: 1.8944e+00   Top-1:  47.66   LR: 0.0003796        \n",
      "[Epoch 63][V][78]   Loss: 7.1116e-01   Top-1:  91.46   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1799114207545476\n",
      "T Top-1\t68.8927732707088\n",
      "V Loss\t0.7111618320465087\n",
      "V Top-1\t91.46\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 91.46\n",
      "********************************************************************************\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 64/100][T][1170]   Loss: 1.1682e+00   Top-1:  69.78   LR: 0.0003461        poch 64/100][T][0]   Loss: 1.1722e+00   Top-1:  81.25   LR: 0.0003628        \n",
      "[Epoch 64][V][78]   Loss: 7.1537e-01   Top-1:  91.12   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1682172626333294\n",
      "T Top-1\t69.77876814688301\n",
      "V Loss\t0.7153668125152588\n",
      "V Top-1\t91.12\n",
      "\n",
      "Best acc1 91.46\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 65/100][T][1170]   Loss: 1.1736e+00   Top-1:  69.03   LR: 0.0003297        poch 65/100][T][0]   Loss: 7.7219e-01   Top-1:  87.50   LR: 0.0003461        \n",
      "[Epoch 65][V][78]   Loss: 7.0879e-01   Top-1:  91.11   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.173578021721795\n",
      "T Top-1\t69.02754056362083\n",
      "V Loss\t0.7087940578460693\n",
      "V Top-1\t91.11\n",
      "\n",
      "Best acc1 91.46\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 66/100][T][1170]   Loss: 1.1760e+00   Top-1:  68.36   LR: 0.0003134        poch 66/100][T][0]   Loss: 1.7212e+00   Top-1:  17.19   LR: 0.0003296        \n",
      "[Epoch 66][V][78]   Loss: 7.1435e-01   Top-1:  91.29   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1760065152439472\n",
      "T Top-1\t68.3597085824082\n",
      "V Loss\t0.7143499619483947\n",
      "V Top-1\t91.29\n",
      "\n",
      "Best acc1 91.46\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 67/100][T][1170]   Loss: 1.1698e+00   Top-1:  69.03   LR: 0.0002973        poch 67/100][T][0]   Loss: 8.0434e-01   Top-1:  85.94   LR: 0.0003134        \n",
      "[Epoch 67][V][78]   Loss: 7.0115e-01   Top-1:  91.37   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.169773139116807\n",
      "T Top-1\t69.0348793766012\n",
      "V Loss\t0.7011495370864869\n",
      "V Top-1\t91.37\n",
      "\n",
      "Best acc1 91.46\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 68/100][T][1170]   Loss: 1.1589e+00   Top-1:  69.98   LR: 0.0002815        poch 68/100][T][0]   Loss: 8.3099e-01   Top-1:  85.16   LR: 0.0002973        \n",
      "[Epoch 68][V][78]   Loss: 7.1190e-01   Top-1:  91.13   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1588970911899863\n",
      "T Top-1\t69.97958475661828\n",
      "V Loss\t0.7118972373008728\n",
      "V Top-1\t91.13\n",
      "\n",
      "Best acc1 91.46\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 69/100][T][1170]   Loss: 1.1404e+00   Top-1:  70.36   LR: 0.0002660        poch 69/100][T][0]   Loss: 8.7300e-01   Top-1:  84.38   LR: 0.0002815        \n",
      "[Epoch 69][V][78]   Loss: 7.0183e-01   Top-1:  91.70   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1403988590615106\n",
      "T Top-1\t70.35920153714774\n",
      "V Loss\t0.7018335468292236\n",
      "V Top-1\t91.7\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 91.70\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 70/100][T][1170]   Loss: 1.1614e+00   Top-1:  69.92   LR: 0.0002508        poch 70/100][T][0]   Loss: 1.7063e+00   Top-1:  65.62   LR: 0.0002660        \n",
      "[Epoch 70][V][78]   Loss: 7.0828e-01   Top-1:  91.34   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.161435936069407\n",
      "T Top-1\t69.91620409906064\n",
      "V Loss\t0.70828028383255\n",
      "V Top-1\t91.34\n",
      "\n",
      "Best acc1 91.70\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 71/100][T][1170]   Loss: 1.1384e+00   Top-1:  70.54   LR: 0.0002358        poch 71/100][T][0]   Loss: 1.7494e+00   Top-1:  60.16   LR: 0.0002507        \n",
      "[Epoch 71][V][78]   Loss: 6.8830e-01   Top-1:  91.92   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1384201950236938\n",
      "T Top-1\t70.54000320239112\n",
      "V Loss\t0.6882957527160645\n",
      "V Top-1\t91.92\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 91.92\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 72/100][T][1170]   Loss: 1.1345e+00   Top-1:  71.32   LR: 0.0002212        poch 72/100][T][0]   Loss: 8.2778e-01   Top-1:  86.72   LR: 0.0002358        \n",
      "[Epoch 72][V][78]   Loss: 6.9295e-01   Top-1:  92.22   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1344596292604043\n",
      "T Top-1\t71.31591588385994\n",
      "V Loss\t0.6929477680206299\n",
      "V Top-1\t92.22\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 92.22\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 73/100][T][1170]   Loss: 1.1344e+00   Top-1:  70.62   LR: 0.0002069        poch 73/100][T][0]   Loss: 8.3367e-01   Top-1:  87.50   LR: 0.0002212        \n",
      "[Epoch 73][V][78]   Loss: 6.9402e-01   Top-1:  92.07   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1344164214920123\n",
      "T Top-1\t70.62339880444065\n",
      "V Loss\t0.6940245988845826\n",
      "V Top-1\t92.07\n",
      "\n",
      "Best acc1 92.22\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 74/100][T][1170]   Loss: 1.1392e+00   Top-1:  70.63   LR: 0.0001930        poch 74/100][T][0]   Loss: 1.7873e+00   Top-1:  58.59   LR: 0.0002069        \n",
      "[Epoch 74][V][78]   Loss: 6.9281e-01   Top-1:  92.14   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1392063953548694\n",
      "T Top-1\t70.6327391118702\n",
      "V Loss\t0.6928105125427246\n",
      "V Top-1\t92.14\n",
      "\n",
      "Best acc1 92.22\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 75/100][T][1170]   Loss: 1.1242e+00   Top-1:  71.32   LR: 0.0001794        poch 75/100][T][0]   Loss: 7.2629e-01   Top-1:  89.06   LR: 0.0001930        \n",
      "[Epoch 75][V][78]   Loss: 6.9629e-01   Top-1:  91.85   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1242493229681971\n",
      "T Top-1\t71.31925170794193\n",
      "V Loss\t0.6962851062774659\n",
      "V Top-1\t91.85\n",
      "\n",
      "Best acc1 92.22\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 76/100][T][1170]   Loss: 1.1040e+00   Top-1:  72.03   LR: 0.0001663        poch 76/100][T][0]   Loss: 1.2174e+00   Top-1:  86.72   LR: 0.0001794        \n",
      "[Epoch 76][V][78]   Loss: 6.9262e-01   Top-1:  92.19   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1040472522645197\n",
      "T Top-1\t72.0344523911187\n",
      "V Loss\t0.6926183251380921\n",
      "V Top-1\t92.19\n",
      "\n",
      "Best acc1 92.22\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 77/100][T][1170]   Loss: 1.0930e+00   Top-1:  72.61   LR: 0.0001535        poch 77/100][T][0]   Loss: 1.3021e+00   Top-1:  85.94   LR: 0.0001663        \n",
      "[Epoch 77][V][78]   Loss: 6.8402e-01   Top-1:  92.31   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.093037446781064\n",
      "T Top-1\t72.61355145175064\n",
      "V Loss\t0.6840190418243408\n",
      "V Top-1\t92.31\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 92.31\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 78/100][T][1170]   Loss: 1.1130e+00   Top-1:  71.83   LR: 0.0001412        poch 78/100][T][0]   Loss: 6.9717e-01   Top-1:  89.84   LR: 0.0001535        \n",
      "[Epoch 78][V][78]   Loss: 6.8342e-01   Top-1:  92.45   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.113011804734612\n",
      "T Top-1\t71.83430294619983\n",
      "V Loss\t0.6834234382629395\n",
      "V Top-1\t92.45\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 92.45\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 79/100][T][1170]   Loss: 1.0856e+00   Top-1:  72.68   LR: 0.0001293        poch 79/100][T][0]   Loss: 1.2694e+00   Top-1:  85.94   LR: 0.0001412        \n",
      "[Epoch 79][V][78]   Loss: 6.7555e-01   Top-1:  92.84   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0855964417135888\n",
      "T Top-1\t72.68160226302305\n",
      "V Loss\t0.6755492974281311\n",
      "V Top-1\t92.84\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 92.84\n",
      "********************************************************************************\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 80/100][T][1170]   Loss: 1.1033e+00   Top-1:  71.93   LR: 0.0001179        poch 80/100][T][0]   Loss: 1.7941e+00   Top-1:  57.81   LR: 0.0001293        \n",
      "[Epoch 80][V][78]   Loss: 6.8131e-01   Top-1:  92.38   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1033114566953643\n",
      "T Top-1\t71.92637169086251\n",
      "V Loss\t0.6813146965026855\n",
      "V Top-1\t92.38\n",
      "\n",
      "Best acc1 92.84\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 81/100][T][1170]   Loss: 1.0999e+00   Top-1:  73.76   LR: 0.0001069        poch 81/100][T][0]   Loss: 7.1270e-01   Top-1:  90.62   LR: 0.0001179        \n",
      "[Epoch 81][V][78]   Loss: 6.8115e-01   Top-1:  92.48   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0999137750046564\n",
      "T Top-1\t73.76174210076857\n",
      "V Loss\t0.6811531359672547\n",
      "V Top-1\t92.48\n",
      "\n",
      "Best acc1 92.84\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 82/100][T][1170]   Loss: 1.0975e+00   Top-1:  73.01   LR: 0.0000964        poch 82/100][T][0]   Loss: 7.1522e-01   Top-1:  90.62   LR: 0.0001069        \n",
      "[Epoch 82][V][78]   Loss: 6.8248e-01   Top-1:  92.61   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0975111171183272\n",
      "T Top-1\t73.00984735269\n",
      "V Loss\t0.6824793556213379\n",
      "V Top-1\t92.61\n",
      "\n",
      "Best acc1 92.84\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 83/100][T][1170]   Loss: 1.0747e+00   Top-1:  72.85   LR: 0.0000864        poch 83/100][T][0]   Loss: 7.4818e-01   Top-1:  92.19   LR: 0.0000964        \n",
      "[Epoch 83][V][78]   Loss: 6.7563e-01   Top-1:  92.81   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0747093757841955\n",
      "T Top-1\t72.84505764304014\n",
      "V Loss\t0.6756311515808106\n",
      "V Top-1\t92.81\n",
      "\n",
      "Best acc1 92.84\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 84/100][T][1170]   Loss: 1.0761e+00   Top-1:  73.96   LR: 0.0000769        poch 84/100][T][0]   Loss: 7.4622e-01   Top-1:  91.41   LR: 0.0000864        \n",
      "[Epoch 84][V][78]   Loss: 6.7240e-01   Top-1:  92.67   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.076058827079737\n",
      "T Top-1\t73.96456020495303\n",
      "V Loss\t0.6724027172088624\n",
      "V Top-1\t92.67\n",
      "\n",
      "Best acc1 92.84\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 85/100][T][1170]   Loss: 1.0773e+00   Top-1:  72.96   LR: 0.0000679        poch 85/100][T][0]   Loss: 6.7299e-01   Top-1:  92.19   LR: 0.0000769        \n",
      "[Epoch 85][V][78]   Loss: 6.7434e-01   Top-1:  92.80   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0773252743009214\n",
      "T Top-1\t72.96114432109309\n",
      "V Loss\t0.6743396240234375\n",
      "V Top-1\t92.8\n",
      "\n",
      "Best acc1 92.84\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 86/100][T][1170]   Loss: 1.0847e+00   Top-1:  74.10   LR: 0.0000595        poch 86/100][T][0]   Loss: 8.6901e-01   Top-1:  92.97   LR: 0.0000679        \n",
      "[Epoch 86][V][78]   Loss: 6.7333e-01   Top-1:  92.75   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0846502837414012\n",
      "T Top-1\t74.10132899231427\n",
      "V Loss\t0.6733250589370727\n",
      "V Top-1\t92.75\n",
      "\n",
      "Best acc1 92.84\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 87/100][T][1170]   Loss: 1.0408e+00   Top-1:  75.41   LR: 0.0000516        poch 87/100][T][0]   Loss: 8.8641e-01   Top-1:  10.16   LR: 0.0000595        \n",
      "[Epoch 87][V][78]   Loss: 6.6560e-01   Top-1:  93.10   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0408322228314426\n",
      "T Top-1\t75.41430935098207\n",
      "V Loss\t0.6656015089035034\n",
      "V Top-1\t93.1\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 93.10\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 88/100][T][1170]   Loss: 1.0756e+00   Top-1:  74.35   LR: 0.0000442        poch 88/100][T][0]   Loss: 7.3389e-01   Top-1:  91.41   LR: 0.0000515        \n",
      "[Epoch 88][V][78]   Loss: 6.6744e-01   Top-1:  93.10   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0755903355389147\n",
      "T Top-1\t74.35084863364645\n",
      "V Loss\t0.6674402723312378\n",
      "V Top-1\t93.1\n",
      "\n",
      "Best acc1 93.10\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 89/100][T][1170]   Loss: 1.0420e+00   Top-1:  74.83   LR: 0.0000374        poch 89/100][T][0]   Loss: 6.7761e-01   Top-1:  93.75   LR: 0.0000442        \n",
      "[Epoch 89][V][78]   Loss: 6.6867e-01   Top-1:  93.08   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0419628567211043\n",
      "T Top-1\t74.83254163108454\n",
      "V Loss\t0.6686670578956604\n",
      "V Top-1\t93.08\n",
      "\n",
      "Best acc1 93.10\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 90/100][T][1170]   Loss: 1.0631e+00   Top-1:  73.65   LR: 0.0000311        poch 90/100][T][0]   Loss: 7.3084e-01   Top-1:  91.41   LR: 0.0000374        \n",
      "[Epoch 90][V][78]   Loss: 6.7054e-01   Top-1:  93.18   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.063107757055281\n",
      "T Top-1\t73.65299423569599\n",
      "V Loss\t0.6705371213912964\n",
      "V Top-1\t93.18\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 93.18\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 91/100][T][1170]   Loss: 1.0571e+00   Top-1:  74.58   LR: 0.0000254        poch 91/100][T][0]   Loss: 7.3495e-01   Top-1:  91.41   LR: 0.0000311        \n",
      "[Epoch 91][V][78]   Loss: 6.6611e-01   Top-1:  93.32   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0570744549383169\n",
      "T Top-1\t74.57568317677199\n",
      "V Loss\t0.6661076057434082\n",
      "V Top-1\t93.32\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 93.32\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 92/100][T][1170]   Loss: 1.0768e+00   Top-1:  74.31   LR: 0.0000203        poch 92/100][T][0]   Loss: 9.6026e-01   Top-1:  85.16   LR: 0.0000254        \n",
      "[Epoch 92][V][78]   Loss: 6.6682e-01   Top-1:  93.42   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0767521722310633\n",
      "T Top-1\t74.30548142613151\n",
      "V Loss\t0.6668171492576599\n",
      "V Top-1\t93.42\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 93.42\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 93/100][T][1170]   Loss: 1.0570e+00   Top-1:  74.16   LR: 0.0000158        poch 93/100][T][0]   Loss: 1.7041e+00   Top-1:  57.81   LR: 0.0000203        \n",
      "[Epoch 93][V][78]   Loss: 6.6827e-01   Top-1:  93.19   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0569767351846833\n",
      "T Top-1\t74.16204099060631\n",
      "V Loss\t0.6682666176795959\n",
      "V Top-1\t93.19\n",
      "\n",
      "Best acc1 93.42\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 94/100][T][1170]   Loss: 1.0519e+00   Top-1:  75.07   LR: 0.0000119        poch 94/100][T][0]   Loss: 6.5035e-01   Top-1:  94.53   LR: 0.0000158        \n",
      "[Epoch 94][V][78]   Loss: 6.6708e-01   Top-1:  93.08   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0519158861557711\n",
      "T Top-1\t75.06871797608882\n",
      "V Loss\t0.6670836767196655\n",
      "V Top-1\t93.08\n",
      "\n",
      "Best acc1 93.42\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 95/100][T][1170]   Loss: 1.0415e+00   Top-1:  75.57   LR: 0.0000086        poch 95/100][T][0]   Loss: 1.7399e+00   Top-1:  35.16   LR: 0.0000119        \n",
      "[Epoch 95][V][78]   Loss: 6.6597e-01   Top-1:  93.20   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0415430087293955\n",
      "T Top-1\t75.5677572587532\n",
      "V Loss\t0.665969372177124\n",
      "V Top-1\t93.2\n",
      "\n",
      "Best acc1 93.42\n",
      "********************************************************************************\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 96/100][T][1170]   Loss: 1.0532e+00   Top-1:  74.84   LR: 0.0000059        poch 96/100][T][0]   Loss: 6.7345e-01   Top-1:  90.62   LR: 0.0000086        \n",
      "[Epoch 96][V][78]   Loss: 6.6590e-01   Top-1:  93.27   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0532306816514794\n",
      "T Top-1\t74.84321626814689\n",
      "V Loss\t0.6659048017501831\n",
      "V Top-1\t93.27\n",
      "\n",
      "Best acc1 93.42\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 97/100][T][1170]   Loss: 1.0539e+00   Top-1:  75.07   LR: 0.0000037        poch 97/100][T][0]   Loss: 1.8232e+00   Top-1:  53.91   LR: 0.0000059        \n",
      "[Epoch 97][V][78]   Loss: 6.6527e-01   Top-1:  93.29   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0539067281707548\n",
      "T Top-1\t75.070719470538\n",
      "V Loss\t0.6652707708358765\n",
      "V Top-1\t93.29\n",
      "\n",
      "Best acc1 93.42\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 98/100][T][1170]   Loss: 1.0685e+00   Top-1:  74.54   LR: 0.0000022        poch 98/100][T][0]   Loss: 1.7799e+00   Top-1:  31.25   LR: 0.0000037        \n",
      "[Epoch 98][V][78]   Loss: 6.6576e-01   Top-1:  93.27   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0685379485306223\n",
      "T Top-1\t74.54499359521776\n",
      "V Loss\t0.6657615642547607\n",
      "V Top-1\t93.27\n",
      "\n",
      "Best acc1 93.42\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 99/100][T][1056]   Loss: 1.0550e+00   Top-1:  75.03   LR: 0.0000014        poch 99/100][T][0]   Loss: 6.6268e-01   Top-1:  92.19   LR: 0.0000022        \r"
     ]
    }
   ],
   "source": [
    "!python main.py --model vit --dataset CIFAR10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd041596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m********************************************************************************\n",
      "CIFAR100\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[32m********************************************************************************\n",
      "Creating model: vit-Base--CIFAR100-LR[0.001]-Seed0\n",
      "Number of params: 2,709,796\n",
      "Initial learning rate: 0.001000\n",
      "Start training for 100 epochs\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "label smoothing used\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Stochastic depth(0.1) used \n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Cutmix used\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Mixup used\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Repeated Aug(3) used\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Autoaugmentation used\n",
      "CIFAR Policy\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Random erasing(0.25) used \n",
      "********************************************************************************\u001b[0m\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Rearrange-1               [-1, 64, 48]               0\n",
      "            Linear-2              [-1, 64, 192]           9,408\n",
      "           Dropout-3              [-1, 65, 192]               0\n",
      "         LayerNorm-4              [-1, 65, 192]             384\n",
      "            Linear-5              [-1, 65, 576]         110,592\n",
      "           Softmax-6           [-1, 12, 65, 65]               0\n",
      "            Linear-7              [-1, 65, 192]          37,056\n",
      "           Dropout-8              [-1, 65, 192]               0\n",
      "         Attention-9              [-1, 65, 192]               0\n",
      "          PreNorm-10              [-1, 65, 192]               0\n",
      "         DropPath-11              [-1, 65, 192]               0\n",
      "        LayerNorm-12              [-1, 65, 192]             384\n",
      "           Linear-13              [-1, 65, 384]          74,112\n",
      "             GELU-14              [-1, 65, 384]               0\n",
      "          Dropout-15              [-1, 65, 384]               0\n",
      "           Linear-16              [-1, 65, 192]          73,920\n",
      "          Dropout-17              [-1, 65, 192]               0\n",
      "      FeedForward-18              [-1, 65, 192]               0\n",
      "          PreNorm-19              [-1, 65, 192]               0\n",
      "         DropPath-20              [-1, 65, 192]               0\n",
      "        LayerNorm-21              [-1, 65, 192]             384\n",
      "           Linear-22              [-1, 65, 576]         110,592\n",
      "          Softmax-23           [-1, 12, 65, 65]               0\n",
      "           Linear-24              [-1, 65, 192]          37,056\n",
      "          Dropout-25              [-1, 65, 192]               0\n",
      "        Attention-26              [-1, 65, 192]               0\n",
      "          PreNorm-27              [-1, 65, 192]               0\n",
      "         DropPath-28              [-1, 65, 192]               0\n",
      "        LayerNorm-29              [-1, 65, 192]             384\n",
      "           Linear-30              [-1, 65, 384]          74,112\n",
      "             GELU-31              [-1, 65, 384]               0\n",
      "          Dropout-32              [-1, 65, 384]               0\n",
      "           Linear-33              [-1, 65, 192]          73,920\n",
      "          Dropout-34              [-1, 65, 192]               0\n",
      "      FeedForward-35              [-1, 65, 192]               0\n",
      "          PreNorm-36              [-1, 65, 192]               0\n",
      "         DropPath-37              [-1, 65, 192]               0\n",
      "        LayerNorm-38              [-1, 65, 192]             384\n",
      "           Linear-39              [-1, 65, 576]         110,592\n",
      "          Softmax-40           [-1, 12, 65, 65]               0\n",
      "           Linear-41              [-1, 65, 192]          37,056\n",
      "          Dropout-42              [-1, 65, 192]               0\n",
      "        Attention-43              [-1, 65, 192]               0\n",
      "          PreNorm-44              [-1, 65, 192]               0\n",
      "         DropPath-45              [-1, 65, 192]               0\n",
      "        LayerNorm-46              [-1, 65, 192]             384\n",
      "           Linear-47              [-1, 65, 384]          74,112\n",
      "             GELU-48              [-1, 65, 384]               0\n",
      "          Dropout-49              [-1, 65, 384]               0\n",
      "           Linear-50              [-1, 65, 192]          73,920\n",
      "          Dropout-51              [-1, 65, 192]               0\n",
      "      FeedForward-52              [-1, 65, 192]               0\n",
      "          PreNorm-53              [-1, 65, 192]               0\n",
      "         DropPath-54              [-1, 65, 192]               0\n",
      "        LayerNorm-55              [-1, 65, 192]             384\n",
      "           Linear-56              [-1, 65, 576]         110,592\n",
      "          Softmax-57           [-1, 12, 65, 65]               0\n",
      "           Linear-58              [-1, 65, 192]          37,056\n",
      "          Dropout-59              [-1, 65, 192]               0\n",
      "        Attention-60              [-1, 65, 192]               0\n",
      "          PreNorm-61              [-1, 65, 192]               0\n",
      "         DropPath-62              [-1, 65, 192]               0\n",
      "        LayerNorm-63              [-1, 65, 192]             384\n",
      "           Linear-64              [-1, 65, 384]          74,112\n",
      "             GELU-65              [-1, 65, 384]               0\n",
      "          Dropout-66              [-1, 65, 384]               0\n",
      "           Linear-67              [-1, 65, 192]          73,920\n",
      "          Dropout-68              [-1, 65, 192]               0\n",
      "      FeedForward-69              [-1, 65, 192]               0\n",
      "          PreNorm-70              [-1, 65, 192]               0\n",
      "         DropPath-71              [-1, 65, 192]               0\n",
      "        LayerNorm-72              [-1, 65, 192]             384\n",
      "           Linear-73              [-1, 65, 576]         110,592\n",
      "          Softmax-74           [-1, 12, 65, 65]               0\n",
      "           Linear-75              [-1, 65, 192]          37,056\n",
      "          Dropout-76              [-1, 65, 192]               0\n",
      "        Attention-77              [-1, 65, 192]               0\n",
      "          PreNorm-78              [-1, 65, 192]               0\n",
      "         DropPath-79              [-1, 65, 192]               0\n",
      "        LayerNorm-80              [-1, 65, 192]             384\n",
      "           Linear-81              [-1, 65, 384]          74,112\n",
      "             GELU-82              [-1, 65, 384]               0\n",
      "          Dropout-83              [-1, 65, 384]               0\n",
      "           Linear-84              [-1, 65, 192]          73,920\n",
      "          Dropout-85              [-1, 65, 192]               0\n",
      "      FeedForward-86              [-1, 65, 192]               0\n",
      "          PreNorm-87              [-1, 65, 192]               0\n",
      "         DropPath-88              [-1, 65, 192]               0\n",
      "        LayerNorm-89              [-1, 65, 192]             384\n",
      "           Linear-90              [-1, 65, 576]         110,592\n",
      "          Softmax-91           [-1, 12, 65, 65]               0\n",
      "           Linear-92              [-1, 65, 192]          37,056\n",
      "          Dropout-93              [-1, 65, 192]               0\n",
      "        Attention-94              [-1, 65, 192]               0\n",
      "          PreNorm-95              [-1, 65, 192]               0\n",
      "         DropPath-96              [-1, 65, 192]               0\n",
      "        LayerNorm-97              [-1, 65, 192]             384\n",
      "           Linear-98              [-1, 65, 384]          74,112\n",
      "             GELU-99              [-1, 65, 384]               0\n",
      "         Dropout-100              [-1, 65, 384]               0\n",
      "          Linear-101              [-1, 65, 192]          73,920\n",
      "         Dropout-102              [-1, 65, 192]               0\n",
      "     FeedForward-103              [-1, 65, 192]               0\n",
      "         PreNorm-104              [-1, 65, 192]               0\n",
      "        DropPath-105              [-1, 65, 192]               0\n",
      "       LayerNorm-106              [-1, 65, 192]             384\n",
      "          Linear-107              [-1, 65, 576]         110,592\n",
      "         Softmax-108           [-1, 12, 65, 65]               0\n",
      "          Linear-109              [-1, 65, 192]          37,056\n",
      "         Dropout-110              [-1, 65, 192]               0\n",
      "       Attention-111              [-1, 65, 192]               0\n",
      "         PreNorm-112              [-1, 65, 192]               0\n",
      "        DropPath-113              [-1, 65, 192]               0\n",
      "       LayerNorm-114              [-1, 65, 192]             384\n",
      "          Linear-115              [-1, 65, 384]          74,112\n",
      "            GELU-116              [-1, 65, 384]               0\n",
      "         Dropout-117              [-1, 65, 384]               0\n",
      "          Linear-118              [-1, 65, 192]          73,920\n",
      "         Dropout-119              [-1, 65, 192]               0\n",
      "     FeedForward-120              [-1, 65, 192]               0\n",
      "         PreNorm-121              [-1, 65, 192]               0\n",
      "        DropPath-122              [-1, 65, 192]               0\n",
      "       LayerNorm-123              [-1, 65, 192]             384\n",
      "          Linear-124              [-1, 65, 576]         110,592\n",
      "         Softmax-125           [-1, 12, 65, 65]               0\n",
      "          Linear-126              [-1, 65, 192]          37,056\n",
      "         Dropout-127              [-1, 65, 192]               0\n",
      "       Attention-128              [-1, 65, 192]               0\n",
      "         PreNorm-129              [-1, 65, 192]               0\n",
      "        DropPath-130              [-1, 65, 192]               0\n",
      "       LayerNorm-131              [-1, 65, 192]             384\n",
      "          Linear-132              [-1, 65, 384]          74,112\n",
      "            GELU-133              [-1, 65, 384]               0\n",
      "         Dropout-134              [-1, 65, 384]               0\n",
      "          Linear-135              [-1, 65, 192]          73,920\n",
      "         Dropout-136              [-1, 65, 192]               0\n",
      "     FeedForward-137              [-1, 65, 192]               0\n",
      "         PreNorm-138              [-1, 65, 192]               0\n",
      "        DropPath-139              [-1, 65, 192]               0\n",
      "       LayerNorm-140              [-1, 65, 192]             384\n",
      "          Linear-141              [-1, 65, 576]         110,592\n",
      "         Softmax-142           [-1, 12, 65, 65]               0\n",
      "          Linear-143              [-1, 65, 192]          37,056\n",
      "         Dropout-144              [-1, 65, 192]               0\n",
      "       Attention-145              [-1, 65, 192]               0\n",
      "         PreNorm-146              [-1, 65, 192]               0\n",
      "        DropPath-147              [-1, 65, 192]               0\n",
      "       LayerNorm-148              [-1, 65, 192]             384\n",
      "          Linear-149              [-1, 65, 384]          74,112\n",
      "            GELU-150              [-1, 65, 384]               0\n",
      "         Dropout-151              [-1, 65, 384]               0\n",
      "          Linear-152              [-1, 65, 192]          73,920\n",
      "         Dropout-153              [-1, 65, 192]               0\n",
      "     FeedForward-154              [-1, 65, 192]               0\n",
      "         PreNorm-155              [-1, 65, 192]               0\n",
      "        DropPath-156              [-1, 65, 192]               0\n",
      "     Transformer-157              [-1, 65, 192]               0\n",
      "       LayerNorm-158                  [-1, 192]             384\n",
      "          Linear-159                  [-1, 100]          19,300\n",
      "================================================================\n",
      "Total params: 2,697,124\n",
      "Trainable params: 2,697,124\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 21.79\n",
      "Params size (MB): 10.29\n",
      "Estimated Total Size (MB): 32.09\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Beginning training\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/100][T][1170]   Loss: 4.5889e+00   Top-1:   2.51   LR: 0.0001009        Epoch 1/100][T][0]   Loss: 5.2911e+00   Top-1:   2.34   LR: 0.0000011        \n",
      "[Epoch 1][V][78]   Loss: 4.2317e+00   Top-1:   6.62   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t4.5888962000481195\n",
      "T Top-1\t2.5105412040990607\n",
      "V Loss\t4.23170299911499\n",
      "V Top-1\t6.62\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 6.62\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 2/100][T][1170]   Loss: 4.3321e+00   Top-1:   6.05   LR: 0.0002008        Epoch 2/100][T][0]   Loss: 4.5374e+00   Top-1:   0.00   LR: 0.0001010        \n",
      "[Epoch 2][V][78]   Loss: 3.7722e+00   Top-1:  16.22   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t4.332105946683354\n",
      "T Top-1\t6.046514730999146\n",
      "V Loss\t3.772185792541504\n",
      "V Top-1\t16.22\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 16.22\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 3/100][T][1170]   Loss: 4.1265e+00   Top-1:   9.75   LR: 0.0003007        Epoch 3/100][T][0]   Loss: 4.0889e+00   Top-1:   7.81   LR: 0.0002009        \n",
      "[Epoch 3][V][78]   Loss: 3.4887e+00   Top-1:  22.60   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t4.1264849991802475\n",
      "T Top-1\t9.749946626814689\n",
      "V Loss\t3.488727952957153\n",
      "V Top-1\t22.6\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 22.60\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 4/100][T][1170]   Loss: 4.0158e+00   Top-1:  12.15   LR: 0.0004006        Epoch 4/100][T][0]   Loss: 4.3365e+00   Top-1:   7.03   LR: 0.0003008        \n",
      "[Epoch 4][V][78]   Loss: 3.2811e+00   Top-1:  27.13   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t4.01583455873289\n",
      "T Top-1\t12.154408625106747\n",
      "V Loss\t3.2811316162109376\n",
      "V Top-1\t27.13\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 27.13\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 5/100][T][1170]   Loss: 3.9042e+00   Top-1:  14.31   LR: 0.0005005        Epoch 5/100][T][0]   Loss: 3.7889e+00   Top-1:  15.62   LR: 0.0004007        \n",
      "[Epoch 5][V][78]   Loss: 3.1263e+00   Top-1:  30.66   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.904152055971649\n",
      "T Top-1\t14.31468830059778\n",
      "V Loss\t3.126267800140381\n",
      "V Top-1\t30.66\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 30.66\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 6/100][T][1170]   Loss: 3.7965e+00   Top-1:  16.60   LR: 0.0006004        Epoch 6/100][T][0]   Loss: 3.5173e+00   Top-1:  21.09   LR: 0.0005006        \n",
      "[Epoch 6][V][78]   Loss: 2.9928e+00   Top-1:  35.21   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.7965068965768527\n",
      "T Top-1\t16.59839346712212\n",
      "V Loss\t2.9928246726989745\n",
      "V Top-1\t35.21\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 35.21\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 7/100][T][1170]   Loss: 3.6850e+00   Top-1:  19.35   LR: 0.0007003        Epoch 7/100][T][0]   Loss: 4.2442e+00   Top-1:  11.72   LR: 0.0006005        \n",
      "[Epoch 7][V][78]   Loss: 2.9525e+00   Top-1:  35.39   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.6850283532749373\n",
      "T Top-1\t19.347112510674638\n",
      "V Loss\t2.9524699668884278\n",
      "V Top-1\t35.39\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 35.39\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 8/100][T][1170]   Loss: 3.6111e+00   Top-1:  20.85   LR: 0.0008002        Epoch 8/100][T][0]   Loss: 3.4645e+00   Top-1:  25.00   LR: 0.0007004        \n",
      "[Epoch 8][V][78]   Loss: 2.8143e+00   Top-1:  39.28   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.6111114269439017\n",
      "T Top-1\t20.846231853116993\n",
      "V Loss\t2.8142765449523925\n",
      "V Top-1\t39.28\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 39.28\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 9/100][T][1170]   Loss: 3.5028e+00   Top-1:  23.40   LR: 0.0009001        Epoch 9/100][T][0]   Loss: 3.2132e+00   Top-1:  31.25   LR: 0.0008003        \n",
      "[Epoch 9][V][78]   Loss: 2.6321e+00   Top-1:  43.76   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.502751180191105\n",
      "T Top-1\t23.39880444064902\n",
      "V Loss\t2.6320724437713623\n",
      "V Top-1\t43.76\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 43.76\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 10/100][T][1170]   Loss: 3.4430e+00   Top-1:  24.58   LR: 0.0010000        poch 10/100][T][0]   Loss: 3.2453e+00   Top-1:  21.88   LR: 0.0009002        \n",
      "[Epoch 10][V][78]   Loss: 2.5796e+00   Top-1:  45.13   LR: 0.0010\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.4429978322616304\n",
      "T Top-1\t24.583689154568745\n",
      "V Loss\t2.5795584648132324\n",
      "V Top-1\t45.13\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 45.13\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 11/100][T][1170]   Loss: 3.3636e+00   Top-1:  26.66   LR: 0.0009997        poch 11/100][T][0]   Loss: 2.9636e+00   Top-1:  37.50   LR: 0.0010000        \n",
      "[Epoch 11][V][78]   Loss: 2.4708e+00   Top-1:  48.18   LR: 0.0010\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.363594968129589\n",
      "T Top-1\t26.66190755764304\n",
      "V Loss\t2.4707538536071776\n",
      "V Top-1\t48.18\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 48.18\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 12/100][T][1170]   Loss: 3.2659e+00   Top-1:  29.21   LR: 0.0009988        poch 12/100][T][0]   Loss: 2.9163e+00   Top-1:  37.50   LR: 0.0009997        \n",
      "[Epoch 12][V][78]   Loss: 2.3791e+00   Top-1:  50.45   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.2658747844875213\n",
      "T Top-1\t29.20980999146029\n",
      "V Loss\t2.3790909786224366\n",
      "V Top-1\t50.45\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 50.45\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 13/100][T][1170]   Loss: 3.2094e+00   Top-1:  30.12   LR: 0.0009973        poch 13/100][T][0]   Loss: 2.6798e+00   Top-1:  41.41   LR: 0.0009988        \n",
      "[Epoch 13][V][78]   Loss: 2.2943e+00   Top-1:  52.23   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.2094460561884253\n",
      "T Top-1\t30.123158625106747\n",
      "V Loss\t2.2943273164749147\n",
      "V Top-1\t52.23\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 52.23\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 14/100][T][1170]   Loss: 3.1295e+00   Top-1:  32.98   LR: 0.0009951        poch 14/100][T][0]   Loss: 2.9559e+00   Top-1:  35.94   LR: 0.0009973        \n",
      "[Epoch 14][V][78]   Loss: 2.2142e+00   Top-1:  54.92   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.1295485366215368\n",
      "T Top-1\t32.97929120409906\n",
      "V Loss\t2.2142121131896975\n",
      "V Top-1\t54.92\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 54.92\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 15/100][T][1170]   Loss: 3.1139e+00   Top-1:  32.67   LR: 0.0009924        poch 15/100][T][0]   Loss: 2.6601e+00   Top-1:  46.88   LR: 0.0009951        \n",
      "[Epoch 15][V][78]   Loss: 2.1924e+00   Top-1:  55.39   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.1138660205316584\n",
      "T Top-1\t32.671728223740395\n",
      "V Loss\t2.1924148723602297\n",
      "V Top-1\t55.39\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 55.39\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 16/100][T][1170]   Loss: 3.0656e+00   Top-1:  34.09   LR: 0.0009891        poch 16/100][T][0]   Loss: 4.1372e+00   Top-1:  10.94   LR: 0.0009924        \n",
      "[Epoch 16][V][78]   Loss: 2.1671e+00   Top-1:  56.75   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T Loss\t3.065576442788545\n",
      "T Top-1\t34.09412361229718\n",
      "V Loss\t2.1671287521362306\n",
      "V Top-1\t56.75\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 56.75\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 17/100][T][1170]   Loss: 3.0156e+00   Top-1:  35.32   LR: 0.0009852        poch 17/100][T][0]   Loss: 2.3748e+00   Top-1:  44.53   LR: 0.0009891        \n",
      "[Epoch 17][V][78]   Loss: 2.1479e+00   Top-1:  57.37   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.0156060808444\n",
      "T Top-1\t35.317036720751496\n",
      "V Loss\t2.147932426071167\n",
      "V Top-1\t57.37\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 57.37\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 18/100][T][1170]   Loss: 2.9613e+00   Top-1:  37.50   LR: 0.0009807        poch 18/100][T][0]   Loss: 3.6309e+00   Top-1:  38.28   LR: 0.0009852        \n",
      "[Epoch 18][V][78]   Loss: 2.1458e+00   Top-1:  57.39   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.9613375525307593\n",
      "T Top-1\t37.5006671648164\n",
      "V Loss\t2.145824893951416\n",
      "V Top-1\t57.39\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 57.39\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 19/100][T][1170]   Loss: 2.9464e+00   Top-1:  37.59   LR: 0.0009756        poch 19/100][T][0]   Loss: 4.0008e+00   Top-1:   7.03   LR: 0.0009806        \n",
      "[Epoch 19][V][78]   Loss: 2.1012e+00   Top-1:  58.23   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.9464147958258704\n",
      "T Top-1\t37.59407023911187\n",
      "V Loss\t2.1011523132324217\n",
      "V Top-1\t58.23\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 58.23\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 20/100][T][1170]   Loss: 2.9170e+00   Top-1:  37.46   LR: 0.0009699        poch 20/100][T][0]   Loss: 2.3348e+00   Top-1:  55.47   LR: 0.0009755        \n",
      "[Epoch 20][V][78]   Loss: 2.0786e+00   Top-1:  59.58   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.917048756634446\n",
      "T Top-1\t37.458635781383435\n",
      "V Loss\t2.078642138671875\n",
      "V Top-1\t59.58\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 59.58\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 21/100][T][1170]   Loss: 2.8677e+00   Top-1:  39.92   LR: 0.0009636        poch 21/100][T][0]   Loss: 4.0107e+00   Top-1:  10.94   LR: 0.0009699        \n",
      "[Epoch 21][V][78]   Loss: 2.0453e+00   Top-1:  60.96   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.8677373886312\n",
      "T Top-1\t39.91780529461998\n",
      "V Loss\t2.045287046813965\n",
      "V Top-1\t60.96\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 60.96\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 22/100][T][1170]   Loss: 2.8556e+00   Top-1:  40.53   LR: 0.0009568        poch 22/100][T][0]   Loss: 2.2781e+00   Top-1:  53.91   LR: 0.0009636        \n",
      "[Epoch 22][V][78]   Loss: 2.0507e+00   Top-1:  59.86   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.8555524713661202\n",
      "T Top-1\t40.533598420153716\n",
      "V Loss\t2.0507294439315795\n",
      "V Top-1\t59.86\n",
      "\n",
      "Best acc1 60.96\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 23/100][T][1170]   Loss: 2.8302e+00   Top-1:  40.66   LR: 0.0009494        poch 23/100][T][0]   Loss: 2.3165e+00   Top-1:  52.34   LR: 0.0009568        \n",
      "[Epoch 23][V][78]   Loss: 2.0353e+00   Top-1:  60.81   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.8301515920257896\n",
      "T Top-1\t40.663028394534585\n",
      "V Loss\t2.035257801818848\n",
      "V Top-1\t60.81\n",
      "\n",
      "Best acc1 60.96\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 24/100][T][1170]   Loss: 2.8465e+00   Top-1:  40.18   LR: 0.0009415        poch 24/100][T][0]   Loss: 2.3234e+00   Top-1:  52.34   LR: 0.0009494        \n",
      "[Epoch 24][V][78]   Loss: 1.9823e+00   Top-1:  61.58   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.846486680839532\n",
      "T Top-1\t40.184004056362085\n",
      "V Loss\t1.9823266748428345\n",
      "V Top-1\t61.58\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 61.58\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 25/100][T][1170]   Loss: 2.8145e+00   Top-1:  41.20   LR: 0.0009331        poch 25/100][T][0]   Loss: 3.5545e+00   Top-1:  38.28   LR: 0.0009415        \n",
      "[Epoch 25][V][78]   Loss: 1.9999e+00   Top-1:  61.10   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.814492372851612\n",
      "T Top-1\t41.20276473099915\n",
      "V Loss\t1.9999000238418578\n",
      "V Top-1\t61.1\n",
      "\n",
      "Best acc1 61.58\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 26/100][T][1170]   Loss: 2.7723e+00   Top-1:  42.16   LR: 0.0009241        poch 26/100][T][0]   Loss: 2.2066e+00   Top-1:  58.59   LR: 0.0009331        \n",
      "[Epoch 26][V][78]   Loss: 1.9918e+00   Top-1:  61.74   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.772295480096167\n",
      "T Top-1\t42.16481639624253\n",
      "V Loss\t1.9917809410095215\n",
      "V Top-1\t61.74\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 61.74\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 27/100][T][1170]   Loss: 2.7785e+00   Top-1:  41.84   LR: 0.0009146        poch 27/100][T][0]   Loss: 2.0284e+00   Top-1:  63.28   LR: 0.0009241        \n",
      "[Epoch 27][V][78]   Loss: 1.9547e+00   Top-1:  62.57   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.7785121318522323\n",
      "T Top-1\t41.84057429547396\n",
      "V Loss\t1.9547351356506348\n",
      "V Top-1\t62.57\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 62.57\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 28/100][T][1170]   Loss: 2.7542e+00   Top-1:  42.36   LR: 0.0009046        poch 28/100][T][0]   Loss: 3.6897e+00   Top-1:  38.28   LR: 0.0009146        \n",
      "[Epoch 28][V][78]   Loss: 1.9599e+00   Top-1:  62.50   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.754157652211535\n",
      "T Top-1\t42.35896135781383\n",
      "V Loss\t1.9599408971786498\n",
      "V Top-1\t62.5\n",
      "\n",
      "Best acc1 62.57\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 29/100][T][1170]   Loss: 2.7519e+00   Top-1:  42.63   LR: 0.0008941        poch 29/100][T][0]   Loss: 2.0384e+00   Top-1:  60.16   LR: 0.0009046        \n",
      "[Epoch 29][V][78]   Loss: 1.9684e+00   Top-1:  62.13   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.751877971228934\n",
      "T Top-1\t42.631831767719895\n",
      "V Loss\t1.9684473846435546\n",
      "V Top-1\t62.13\n",
      "\n",
      "Best acc1 62.57\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 30/100][T][1170]   Loss: 2.6835e+00   Top-1:  44.47   LR: 0.0008831        poch 30/100][T][0]   Loss: 3.5750e+00   Top-1:   7.03   LR: 0.0008941        \n",
      "[Epoch 30][V][78]   Loss: 1.9853e+00   Top-1:  62.21   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.6834923035054\n",
      "T Top-1\t44.47387382578992\n",
      "V Loss\t1.9852547214508056\n",
      "V Top-1\t62.21\n",
      "\n",
      "Best acc1 62.57\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 31/100][T][1170]   Loss: 2.6958e+00   Top-1:  44.57   LR: 0.0008717        poch 31/100][T][0]   Loss: 3.6783e+00   Top-1:  31.25   LR: 0.0008831        \n",
      "[Epoch 31][V][78]   Loss: 1.9553e+00   Top-1:  62.65   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.6958235013596936\n",
      "T Top-1\t44.57394854824936\n",
      "V Loss\t1.95530280418396\n",
      "V Top-1\t62.65\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 62.65\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 32/100][T][1170]   Loss: 2.7115e+00   Top-1:  43.79   LR: 0.0008598        poch 32/100][T][0]   Loss: 3.0781e+00   Top-1:  51.56   LR: 0.0008717        \n",
      "[Epoch 32][V][78]   Loss: 1.9702e+00   Top-1:  62.94   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.7114503084340735\n",
      "T Top-1\t43.789362724167376\n",
      "V Loss\t1.9702337661743163\n",
      "V Top-1\t62.94\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 62.94\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 33/100][T][1170]   Loss: 2.6690e+00   Top-1:  45.23   LR: 0.0008475        poch 33/100][T][0]   Loss: 3.9043e+00   Top-1:  27.34   LR: 0.0008598        \n",
      "[Epoch 33][V][78]   Loss: 1.9543e+00   Top-1:  63.29   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.6689984921610934\n",
      "T Top-1\t45.22910439795047\n",
      "V Loss\t1.9542584327697754\n",
      "V Top-1\t63.29\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 63.29\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 34/100][T][1170]   Loss: 2.6336e+00   Top-1:  46.47   LR: 0.0008347        poch 34/100][T][0]   Loss: 2.0147e+00   Top-1:  58.59   LR: 0.0008475        \n",
      "[Epoch 34][V][78]   Loss: 1.9194e+00   Top-1:  64.54   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.6335502920957023\n",
      "T Top-1\t46.47403394534586\n",
      "V Loss\t1.9194339321136475\n",
      "V Top-1\t64.54\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 64.54\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 35/100][T][1170]   Loss: 2.6058e+00   Top-1:  47.22   LR: 0.0008216        poch 35/100][T][0]   Loss: 3.8013e+00   Top-1:  25.78   LR: 0.0008347        \n",
      "[Epoch 35][V][78]   Loss: 1.9040e+00   Top-1:  64.34   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.6058410176041797\n",
      "T Top-1\t47.217255550811274\n",
      "V Loss\t1.9040484909057618\n",
      "V Top-1\t64.34\n",
      "\n",
      "Best acc1 64.54\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 36/100][T][1170]   Loss: 2.5783e+00   Top-1:  47.38   LR: 0.0008080        poch 36/100][T][0]   Loss: 2.1169e+00   Top-1:  51.56   LR: 0.0008216        \n",
      "[Epoch 36][V][78]   Loss: 1.8859e+00   Top-1:  65.02   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.5783413812505396\n",
      "T Top-1\t47.37537361229718\n",
      "V Loss\t1.8858600704193116\n",
      "V Top-1\t65.02\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 65.02\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 37/100][T][1170]   Loss: 2.5888e+00   Top-1:  47.62   LR: 0.0007941        poch 37/100][T][0]   Loss: 2.9403e+00   Top-1:   4.69   LR: 0.0008080        \n",
      "[Epoch 37][V][78]   Loss: 1.8997e+00   Top-1:  64.54   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.588752909973073\n",
      "T Top-1\t47.618221605465415\n",
      "V Loss\t1.89972760181427\n",
      "V Top-1\t64.54\n",
      "\n",
      "Best acc1 65.02\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 38/100][T][1170]   Loss: 2.6205e+00   Top-1:  46.71   LR: 0.0007798        poch 38/100][T][0]   Loss: 2.7052e+00   Top-1:  57.03   LR: 0.0007941        \n",
      "[Epoch 38][V][78]   Loss: 1.8642e+00   Top-1:  65.67   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.620527362233039\n",
      "T Top-1\t46.710877455166525\n",
      "V Loss\t1.8641843597412109\n",
      "V Top-1\t65.67\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 65.67\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 39/100][T][1170]   Loss: 2.5588e+00   Top-1:  47.93   LR: 0.0007652        poch 39/100][T][0]   Loss: 2.5354e+00   Top-1:  57.03   LR: 0.0007798        \n",
      "[Epoch 39][V][78]   Loss: 1.9043e+00   Top-1:  64.78   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.558755567499351\n",
      "T Top-1\t47.93379056362084\n",
      "V Loss\t1.9042747323989868\n",
      "V Top-1\t64.78\n",
      "\n",
      "Best acc1 65.67\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 40/100][T][1170]   Loss: 2.5637e+00   Top-1:  48.72   LR: 0.0007503        poch 40/100][T][0]   Loss: 3.8907e+00   Top-1:  13.28   LR: 0.0007652        \n",
      "[Epoch 40][V][78]   Loss: 1.8635e+00   Top-1:  65.87   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.5636977550822584\n",
      "T Top-1\t48.71770922288642\n",
      "V Loss\t1.863453862953186\n",
      "V Top-1\t65.87\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 65.87\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 41/100][T][1170]   Loss: 2.5936e+00   Top-1:  48.06   LR: 0.0007350        poch 41/100][T][0]   Loss: 4.0274e+00   Top-1:  11.72   LR: 0.0007502        \n",
      "[Epoch 41][V][78]   Loss: 1.8553e+00   Top-1:  65.49   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.593603985722914\n",
      "T Top-1\t48.06322053800171\n",
      "V Loss\t1.8553260522842407\n",
      "V Top-1\t65.49\n",
      "\n",
      "Best acc1 65.87\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 42/100][T][1170]   Loss: 2.5413e+00   Top-1:  48.46   LR: 0.0007195        poch 42/100][T][0]   Loss: 2.9465e+00   Top-1:   1.56   LR: 0.0007350        \n",
      "[Epoch 42][V][78]   Loss: 1.8531e+00   Top-1:  65.90   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.541323033283146\n",
      "T Top-1\t48.45684777967549\n",
      "V Loss\t1.853123091506958\n",
      "V Top-1\t65.9\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 65.90\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 43/100][T][1170]   Loss: 2.5331e+00   Top-1:  47.99   LR: 0.0007037        poch 43/100][T][0]   Loss: 3.3495e+00   Top-1:   7.81   LR: 0.0007195        \n",
      "[Epoch 43][V][78]   Loss: 1.8824e+00   Top-1:  65.44   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.533144039847116\n",
      "T Top-1\t47.98849807856533\n",
      "V Loss\t1.8824305927276612\n",
      "V Top-1\t65.44\n",
      "\n",
      "Best acc1 65.90\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 44/100][T][1170]   Loss: 2.4976e+00   Top-1:  49.37   LR: 0.0006876        poch 44/100][T][0]   Loss: 3.9448e+00   Top-1:  16.41   LR: 0.0007037        \n",
      "[Epoch 44][V][78]   Loss: 1.8458e+00   Top-1:  66.26   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.4975999015503514\n",
      "T Top-1\t49.36952924850555\n",
      "V Loss\t1.8458479261398315\n",
      "V Top-1\t66.26\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 66.26\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 45/100][T][1170]   Loss: 2.4887e+00   Top-1:  50.50   LR: 0.0006713        poch 45/100][T][0]   Loss: 2.8675e+00   Top-1:  59.38   LR: 0.0006876        \n",
      "[Epoch 45][V][78]   Loss: 1.8545e+00   Top-1:  66.33   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.4887279413583236\n",
      "T Top-1\t50.50370943637916\n",
      "V Loss\t1.8545484420776368\n",
      "V Top-1\t66.33\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 66.33\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 46/100][T][1170]   Loss: 2.4790e+00   Top-1:  50.16   LR: 0.0006549        poch 46/100][T][0]   Loss: 2.0172e+00   Top-1:  67.97   LR: 0.0006713        \n",
      "[Epoch 46][V][78]   Loss: 1.8418e+00   Top-1:  66.71   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.479022835280323\n",
      "T Top-1\t50.157450896669516\n",
      "V Loss\t1.8417839361190795\n",
      "V Top-1\t66.71\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 66.71\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 47/100][T][1170]   Loss: 2.4664e+00   Top-1:  51.66   LR: 0.0006382        poch 47/100][T][0]   Loss: 1.7206e+00   Top-1:  71.88   LR: 0.0006548        \n",
      "[Epoch 47][V][78]   Loss: 1.8449e+00   Top-1:  66.80   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.466441285600019\n",
      "T Top-1\t51.66257472245944\n",
      "V Loss\t1.8448880153656007\n",
      "V Top-1\t66.8\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 66.80\n",
      "********************************************************************************\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 48/100][T][1170]   Loss: 2.4556e+00   Top-1:  51.87   LR: 0.0006213        poch 48/100][T][0]   Loss: 1.6456e+00   Top-1:  75.00   LR: 0.0006382        \n",
      "[Epoch 48][V][78]   Loss: 1.8546e+00   Top-1:  65.89   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.455576064543883\n",
      "T Top-1\t51.86605999146029\n",
      "V Loss\t1.8545520345687867\n",
      "V Top-1\t65.89\n",
      "\n",
      "Best acc1 66.80\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 49/100][T][1170]   Loss: 2.4864e+00   Top-1:  51.21   LR: 0.0006044        poch 49/100][T][0]   Loss: 3.1316e+00   Top-1:   1.56   LR: 0.0006213        \n",
      "[Epoch 49][V][78]   Loss: 1.8185e+00   Top-1:  67.33   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.4864255202111782\n",
      "T Top-1\t51.21157130657558\n",
      "V Loss\t1.8184713500976561\n",
      "V Top-1\t67.33\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 67.33\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 50/100][T][1170]   Loss: 2.3802e+00   Top-1:  53.40   LR: 0.0005872        poch 50/100][T][0]   Loss: 3.0038e+00   Top-1:  53.91   LR: 0.0006043        \n",
      "[Epoch 50][V][78]   Loss: 1.8267e+00   Top-1:  67.07   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.3802102910057283\n",
      "T Top-1\t53.39787040990606\n",
      "V Loss\t1.8266635730743408\n",
      "V Top-1\t67.07\n",
      "\n",
      "Best acc1 67.33\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 51/100][T][1170]   Loss: 2.4138e+00   Top-1:  52.74   LR: 0.0005700        poch 51/100][T][0]   Loss: 1.9604e+00   Top-1:  65.62   LR: 0.0005872        \n",
      "[Epoch 51][V][78]   Loss: 1.8264e+00   Top-1:  66.98   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.4137774622409798\n",
      "T Top-1\t52.74004590093937\n",
      "V Loss\t1.8263730655670165\n",
      "V Top-1\t66.98\n",
      "\n",
      "Best acc1 67.33\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 52/100][T][1170]   Loss: 2.4187e+00   Top-1:  52.79   LR: 0.0005527        poch 52/100][T][0]   Loss: 2.8436e+00   Top-1:  46.09   LR: 0.0005700        \n",
      "[Epoch 52][V][78]   Loss: 1.8393e+00   Top-1:  66.90   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.4186518505839176\n",
      "T Top-1\t52.78874893253629\n",
      "V Loss\t1.839289028930664\n",
      "V Top-1\t66.9\n",
      "\n",
      "Best acc1 67.33\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 53/100][T][1170]   Loss: 2.4179e+00   Top-1:  53.20   LR: 0.0005353        poch 53/100][T][0]   Loss: 1.7594e+00   Top-1:  71.09   LR: 0.0005527        \n",
      "[Epoch 53][V][78]   Loss: 1.8135e+00   Top-1:  67.13   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.417873731441319\n",
      "T Top-1\t53.20172395388557\n",
      "V Loss\t1.813541178894043\n",
      "V Top-1\t67.13\n",
      "\n",
      "Best acc1 67.33\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 54/100][T][1170]   Loss: 2.3966e+00   Top-1:  53.73   LR: 0.0005179        poch 54/100][T][0]   Loss: 2.0688e+00   Top-1:   0.00   LR: 0.0005353        \n",
      "[Epoch 54][V][78]   Loss: 1.8107e+00   Top-1:  67.26   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.396569581952083\n",
      "T Top-1\t53.73011848847139\n",
      "V Loss\t1.810676505279541\n",
      "V Top-1\t67.26\n",
      "\n",
      "Best acc1 67.33\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 55/100][T][1170]   Loss: 2.3663e+00   Top-1:  55.00   LR: 0.0005005        poch 55/100][T][0]   Loss: 2.1337e+00   Top-1:  68.75   LR: 0.0005179        \n",
      "[Epoch 55][V][78]   Loss: 1.8057e+00   Top-1:  67.86   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.3662667528978276\n",
      "T Top-1\t55.004403287788215\n",
      "V Loss\t1.8056861890792846\n",
      "V Top-1\t67.86\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 67.86\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 56/100][T][1170]   Loss: 2.3256e+00   Top-1:  55.61   LR: 0.0004831        poch 56/100][T][0]   Loss: 1.6518e+00   Top-1:  78.12   LR: 0.0005005        \n",
      "[Epoch 56][V][78]   Loss: 1.7996e+00   Top-1:  68.37   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.325637971000932\n",
      "T Top-1\t55.61285760034159\n",
      "V Loss\t1.7995590324401856\n",
      "V Top-1\t68.37\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 68.37\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 57/100][T][1170]   Loss: 2.3278e+00   Top-1:  55.42   LR: 0.0004657        poch 57/100][T][0]   Loss: 1.6433e+00   Top-1:  74.22   LR: 0.0004831        \n",
      "[Epoch 57][V][78]   Loss: 1.7942e+00   Top-1:  68.19   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.3277555504586327\n",
      "T Top-1\t55.42471712211785\n",
      "V Loss\t1.7942074480056762\n",
      "V Top-1\t68.19\n",
      "\n",
      "Best acc1 68.37\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 58/100][T][1170]   Loss: 2.3564e+00   Top-1:  55.05   LR: 0.0004483        poch 58/100][T][0]   Loss: 3.1373e+00   Top-1:  50.00   LR: 0.0004656        \n",
      "[Epoch 58][V][78]   Loss: 1.7735e+00   Top-1:  69.27   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.3564285848203834\n",
      "T Top-1\t55.051771989752346\n",
      "V Loss\t1.7735160209655763\n",
      "V Top-1\t69.27\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 69.27\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 59/100][T][1170]   Loss: 2.3772e+00   Top-1:  54.03   LR: 0.0004310        poch 59/100][T][0]   Loss: 3.8485e+00   Top-1:  18.75   LR: 0.0004483        \n",
      "[Epoch 59][V][78]   Loss: 1.7858e+00   Top-1:  68.41   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.3772490972336495\n",
      "T Top-1\t54.02900832621691\n",
      "V Loss\t1.7858205253601074\n",
      "V Top-1\t68.41\n",
      "\n",
      "Best acc1 69.27\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 60/100][T][1170]   Loss: 2.3188e+00   Top-1:  55.33   LR: 0.0004138        poch 60/100][T][0]   Loss: 1.5770e+00   Top-1:  78.12   LR: 0.0004310        \n",
      "[Epoch 60][V][78]   Loss: 1.7874e+00   Top-1:  68.36   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.3188226369585774\n",
      "T Top-1\t55.325976729291206\n",
      "V Loss\t1.7874233585357666\n",
      "V Top-1\t68.36\n",
      "\n",
      "Best acc1 69.27\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 61/100][T][1170]   Loss: 2.3018e+00   Top-1:  56.67   LR: 0.0003966        poch 61/100][T][0]   Loss: 3.6601e+00   Top-1:  24.22   LR: 0.0004137        \n",
      "[Epoch 61][V][78]   Loss: 1.7796e+00   Top-1:  68.68   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.3018126450065464\n",
      "T Top-1\t56.666310845431255\n",
      "V Loss\t1.7796487678527833\n",
      "V Top-1\t68.68\n",
      "\n",
      "Best acc1 69.27\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 62/100][T][1170]   Loss: 2.2616e+00   Top-1:  57.35   LR: 0.0003797        poch 62/100][T][0]   Loss: 1.5448e+00   Top-1:  76.56   LR: 0.0003966        \n",
      "[Epoch 62][V][78]   Loss: 1.7749e+00   Top-1:  69.21   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.2616328635244263\n",
      "T Top-1\t57.348153287788215\n",
      "V Loss\t1.7748622978210449\n",
      "V Top-1\t69.21\n",
      "\n",
      "Best acc1 69.27\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 63/100][T][1170]   Loss: 2.2863e+00   Top-1:  56.68   LR: 0.0003628        poch 63/100][T][0]   Loss: 3.7703e+00   Top-1:  20.31   LR: 0.0003796        \n",
      "[Epoch 63][V][78]   Loss: 1.7770e+00   Top-1:  68.93   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.286339913037575\n",
      "T Top-1\t56.67698548249359\n",
      "V Loss\t1.77702963886261\n",
      "V Top-1\t68.93\n",
      "\n",
      "Best acc1 69.27\n",
      "********************************************************************************\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 64/100][T][1170]   Loss: 2.2682e+00   Top-1:  57.62   LR: 0.0003461        poch 64/100][T][0]   Loss: 2.2371e+00   Top-1:  70.31   LR: 0.0003628        \n",
      "[Epoch 64][V][78]   Loss: 1.7606e+00   Top-1:  69.54   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.26817872328803\n",
      "T Top-1\t57.624359521776256\n",
      "V Loss\t1.7605621646881104\n",
      "V Top-1\t69.54\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 69.54\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 65/100][T][1170]   Loss: 2.2775e+00   Top-1:  57.02   LR: 0.0003297        poch 65/100][T][0]   Loss: 1.7187e+00   Top-1:  72.66   LR: 0.0003461        \n",
      "[Epoch 65][V][78]   Loss: 1.7433e+00   Top-1:  69.83   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.2775159854343263\n",
      "T Top-1\t57.015905209222886\n",
      "V Loss\t1.7432943355560302\n",
      "V Top-1\t69.83\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 69.83\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 66/100][T][1170]   Loss: 2.2844e+00   Top-1:  56.14   LR: 0.0003134        poch 66/100][T][0]   Loss: 3.3489e+00   Top-1:   7.03   LR: 0.0003296        \n",
      "[Epoch 66][V][78]   Loss: 1.7458e+00   Top-1:  69.65   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.284351808611498\n",
      "T Top-1\t56.136581981212636\n",
      "V Loss\t1.7457761386871338\n",
      "V Top-1\t69.65\n",
      "\n",
      "Best acc1 69.83\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 67/100][T][1170]   Loss: 2.2658e+00   Top-1:  57.14   LR: 0.0002973        poch 67/100][T][0]   Loss: 1.3937e+00   Top-1:  82.81   LR: 0.0003134        \n",
      "[Epoch 67][V][78]   Loss: 1.7557e+00   Top-1:  69.82   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.2658100408127324\n",
      "T Top-1\t57.144668018787364\n",
      "V Loss\t1.7557165596008302\n",
      "V Top-1\t69.82\n",
      "\n",
      "Best acc1 69.83\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 68/100][T][1170]   Loss: 2.2452e+00   Top-1:  58.13   LR: 0.0002815        poch 68/100][T][0]   Loss: 1.4414e+00   Top-1:  78.91   LR: 0.0002973        \n",
      "[Epoch 68][V][78]   Loss: 1.7601e+00   Top-1:  69.36   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.2451507096408676\n",
      "T Top-1\t58.129403287788215\n",
      "V Loss\t1.7601223628997802\n",
      "V Top-1\t69.36\n",
      "\n",
      "Best acc1 69.83\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 69/100][T][1170]   Loss: 2.1956e+00   Top-1:  59.20   LR: 0.0002660        poch 69/100][T][0]   Loss: 1.3787e+00   Top-1:  84.38   LR: 0.0002815        \n",
      "[Epoch 69][V][78]   Loss: 1.7305e+00   Top-1:  70.37   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.1956205016827606\n",
      "T Top-1\t59.20220431255337\n",
      "V Loss\t1.7304897621154784\n",
      "V Top-1\t70.37\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 70.37\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 70/100][T][1170]   Loss: 2.2471e+00   Top-1:  58.28   LR: 0.0002508        poch 70/100][T][0]   Loss: 3.3823e+00   Top-1:  39.06   LR: 0.0002660        \n",
      "[Epoch 70][V][78]   Loss: 1.7514e+00   Top-1:  69.41   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.247097534974552\n",
      "T Top-1\t58.27617954739539\n",
      "V Loss\t1.7514368831634521\n",
      "V Top-1\t69.41\n",
      "\n",
      "Best acc1 70.37\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 71/100][T][1170]   Loss: 2.1963e+00   Top-1:  59.38   LR: 0.0002358        poch 71/100][T][0]   Loss: 3.2560e+00   Top-1:  53.91   LR: 0.0002507        \n",
      "[Epoch 71][V][78]   Loss: 1.7482e+00   Top-1:  70.06   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.1962785421529456\n",
      "T Top-1\t59.37766865926559\n",
      "V Loss\t1.7482058101654052\n",
      "V Top-1\t70.06\n",
      "\n",
      "Best acc1 70.37\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 72/100][T][1170]   Loss: 2.1845e+00   Top-1:  60.17   LR: 0.0002212        poch 72/100][T][0]   Loss: 1.3387e+00   Top-1:  84.38   LR: 0.0002358        \n",
      "[Epoch 72][V][78]   Loss: 1.7307e+00   Top-1:  70.49   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.184470228310022\n",
      "T Top-1\t60.168926131511526\n",
      "V Loss\t1.730721672439575\n",
      "V Top-1\t70.49\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 70.49\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 73/100][T][1170]   Loss: 2.1891e+00   Top-1:  59.55   LR: 0.0002069        poch 73/100][T][0]   Loss: 1.4716e+00   Top-1:  78.12   LR: 0.0002212        \n",
      "[Epoch 73][V][78]   Loss: 1.7592e+00   Top-1:  70.01   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.1890717762133485\n",
      "T Top-1\t59.54579419299744\n",
      "V Loss\t1.759159327697754\n",
      "V Top-1\t70.01\n",
      "\n",
      "Best acc1 70.49\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 74/100][T][1170]   Loss: 2.1929e+00   Top-1:  59.56   LR: 0.0001930        poch 74/100][T][0]   Loss: 3.6541e+00   Top-1:  33.59   LR: 0.0002069        \n",
      "[Epoch 74][V][78]   Loss: 1.7329e+00   Top-1:  70.42   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.1929391869920427\n",
      "T Top-1\t59.55847032450897\n",
      "V Loss\t1.7328916694641112\n",
      "V Top-1\t70.42\n",
      "\n",
      "Best acc1 70.49\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 75/100][T][1170]   Loss: 2.1607e+00   Top-1:  60.47   LR: 0.0001794        poch 75/100][T][0]   Loss: 1.2818e+00   Top-1:  87.50   LR: 0.0001930        \n",
      "[Epoch 75][V][78]   Loss: 1.7392e+00   Top-1:  70.26   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.160653631265707\n",
      "T Top-1\t60.474487617421005\n",
      "V Loss\t1.7392048023223876\n",
      "V Top-1\t70.26\n",
      "\n",
      "Best acc1 70.49\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 76/100][T][1170]   Loss: 2.1136e+00   Top-1:  61.66   LR: 0.0001663        poch 76/100][T][0]   Loss: 2.3059e+00   Top-1:  71.09   LR: 0.0001794        \n",
      "[Epoch 76][V][78]   Loss: 1.7297e+00   Top-1:  70.78   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.113632635007446\n",
      "T Top-1\t61.66404248505551\n",
      "V Loss\t1.7296578504562379\n",
      "V Top-1\t70.78\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 70.78\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 77/100][T][1170]   Loss: 2.0915e+00   Top-1:  62.51   LR: 0.0001535        poch 77/100][T][0]   Loss: 2.6450e+00   Top-1:  68.75   LR: 0.0001663        \n",
      "[Epoch 77][V][78]   Loss: 1.7268e+00   Top-1:  70.80   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.0915195655863474\n",
      "T Top-1\t62.50733881298036\n",
      "V Loss\t1.7267784637451171\n",
      "V Top-1\t70.8\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 70.80\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 78/100][T][1170]   Loss: 2.1360e+00   Top-1:  61.50   LR: 0.0001412        poch 78/100][T][0]   Loss: 1.2397e+00   Top-1:  86.72   LR: 0.0001535        \n",
      "[Epoch 78][V][78]   Loss: 1.7140e+00   Top-1:  71.11   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.1360451984772\n",
      "T Top-1\t61.49858561058924\n",
      "V Loss\t1.71401088848114\n",
      "V Top-1\t71.11\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 71.11\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 79/100][T][1170]   Loss: 2.0750e+00   Top-1:  62.70   LR: 0.0001293        poch 79/100][T][0]   Loss: 2.2946e+00   Top-1:  77.34   LR: 0.0001412        \n",
      "[Epoch 79][V][78]   Loss: 1.7225e+00   Top-1:  71.03   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.075034582645439\n",
      "T Top-1\t62.70215093936806\n",
      "V Loss\t1.7224885528564453\n",
      "V Top-1\t71.03\n",
      "\n",
      "Best acc1 71.11\n",
      "********************************************************************************\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 80/100][T][1170]   Loss: 2.1111e+00   Top-1:  61.73   LR: 0.0001179        poch 80/100][T][0]   Loss: 3.4805e+00   Top-1:  37.50   LR: 0.0001293        \n",
      "[Epoch 80][V][78]   Loss: 1.6988e+00   Top-1:  71.62   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.111117482999774\n",
      "T Top-1\t61.734094790777114\n",
      "V Loss\t1.6988196144104004\n",
      "V Top-1\t71.62\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 71.62\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 81/100][T][1170]   Loss: 2.0983e+00   Top-1:  63.88   LR: 0.0001069        poch 81/100][T][0]   Loss: 1.2825e+00   Top-1:  87.50   LR: 0.0001179        \n",
      "[Epoch 81][V][78]   Loss: 1.7259e+00   Top-1:  71.00   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.098317473286955\n",
      "T Top-1\t63.88036400512382\n",
      "V Loss\t1.725871608352661\n",
      "V Top-1\t71.0\n",
      "\n",
      "Best acc1 71.62\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 82/100][T][1170]   Loss: 2.0976e+00   Top-1:  63.07   LR: 0.0000964        poch 82/100][T][0]   Loss: 1.3832e+00   Top-1:  82.03   LR: 0.0001069        \n",
      "[Epoch 82][V][78]   Loss: 1.6986e+00   Top-1:  71.67   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.097649749377973\n",
      "T Top-1\t63.067757258753204\n",
      "V Loss\t1.6986172292709352\n",
      "V Top-1\t71.67\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 71.67\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 83/100][T][1170]   Loss: 2.0501e+00   Top-1:  63.37   LR: 0.0000864        poch 83/100][T][0]   Loss: 1.3562e+00   Top-1:  84.38   LR: 0.0000964        \n",
      "[Epoch 83][V][78]   Loss: 1.6967e+00   Top-1:  71.83   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.0501020947243798\n",
      "T Top-1\t63.36531276686593\n",
      "V Loss\t1.6966890924453735\n",
      "V Top-1\t71.83\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 71.83\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 84/100][T][1170]   Loss: 2.0477e+00   Top-1:  64.47   LR: 0.0000769        poch 84/100][T][0]   Loss: 1.2576e+00   Top-1:  85.16   LR: 0.0000864        \n",
      "[Epoch 84][V][78]   Loss: 1.7048e+00   Top-1:  71.66   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.047718374063376\n",
      "T Top-1\t64.4701377028181\n",
      "V Loss\t1.7047626235961915\n",
      "V Top-1\t71.66\n",
      "\n",
      "Best acc1 71.83\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 85/100][T][1170]   Loss: 2.0573e+00   Top-1:  63.31   LR: 0.0000679        poch 85/100][T][0]   Loss: 1.3205e+00   Top-1:  85.94   LR: 0.0000769        \n",
      "[Epoch 85][V][78]   Loss: 1.7021e+00   Top-1:  71.59   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.0572730796546836\n",
      "T Top-1\t63.30526793339026\n",
      "V Loss\t1.7020828491210938\n",
      "V Top-1\t71.59\n",
      "\n",
      "Best acc1 71.83\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 86/100][T][1170]   Loss: 2.0665e+00   Top-1:  64.56   LR: 0.0000595        poch 86/100][T][0]   Loss: 1.6029e+00   Top-1:  84.38   LR: 0.0000679        \n",
      "[Epoch 86][V][78]   Loss: 1.7030e+00   Top-1:  71.64   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.0665334312453623\n",
      "T Top-1\t64.55820345858241\n",
      "V Loss\t1.702989605140686\n",
      "V Top-1\t71.64\n",
      "\n",
      "Best acc1 71.83\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 87/100][T][1170]   Loss: 1.9782e+00   Top-1:  66.26   LR: 0.0000516        poch 87/100][T][0]   Loss: 1.4845e+00   Top-1:   0.78   LR: 0.0000595        \n",
      "[Epoch 87][V][78]   Loss: 1.6914e+00   Top-1:  72.29   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.978249415284441\n",
      "T Top-1\t66.26280956447481\n",
      "V Loss\t1.6913602056503296\n",
      "V Top-1\t72.29\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 72.29\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 88/100][T][1170]   Loss: 2.0432e+00   Top-1:  64.97   LR: 0.0000442        poch 88/100][T][0]   Loss: 1.3115e+00   Top-1:  83.59   LR: 0.0000515        \n",
      "[Epoch 88][V][78]   Loss: 1.6891e+00   Top-1:  72.15   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.0431843810769656\n",
      "T Top-1\t64.97184564474807\n",
      "V Loss\t1.6890541957855225\n",
      "V Top-1\t72.15\n",
      "\n",
      "Best acc1 72.29\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 89/100][T][1170]   Loss: 1.9794e+00   Top-1:  65.83   LR: 0.0000374        poch 89/100][T][0]   Loss: 1.4653e+00   Top-1:  82.81   LR: 0.0000442        \n",
      "[Epoch 89][V][78]   Loss: 1.6870e+00   Top-1:  72.09   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.979446708087123\n",
      "T Top-1\t65.82715093936807\n",
      "V Loss\t1.6869580799102784\n",
      "V Top-1\t72.09\n",
      "\n",
      "Best acc1 72.29\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 90/100][T][1170]   Loss: 2.0250e+00   Top-1:  64.17   LR: 0.0000311        poch 90/100][T][0]   Loss: 1.3321e+00   Top-1:  85.94   LR: 0.0000374        \n",
      "[Epoch 90][V][78]   Loss: 1.6909e+00   Top-1:  72.36   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.025025487455112\n",
      "T Top-1\t64.17124786507259\n",
      "V Loss\t1.6908863334655762\n",
      "V Top-1\t72.36\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 72.36\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 91/100][T][1170]   Loss: 2.0119e+00   Top-1:  65.49   LR: 0.0000254        poch 91/100][T][0]   Loss: 1.1443e+00   Top-1:  90.62   LR: 0.0000311        \n",
      "[Epoch 91][V][78]   Loss: 1.6813e+00   Top-1:  72.42   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.0118781900324443\n",
      "T Top-1\t65.48689688300598\n",
      "V Loss\t1.6813098844528198\n",
      "V Top-1\t72.42\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 72.42\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 92/100][T][1170]   Loss: 2.0529e+00   Top-1:  65.09   LR: 0.0000203        poch 92/100][T][0]   Loss: 1.8211e+00   Top-1:  76.56   LR: 0.0000254        \n",
      "[Epoch 92][V][78]   Loss: 1.6827e+00   Top-1:  72.61   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.0529085739772612\n",
      "T Top-1\t65.08659799316823\n",
      "V Loss\t1.6826891803741455\n",
      "V Top-1\t72.61\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 72.61\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 93/100][T][1170]   Loss: 2.0168e+00   Top-1:  64.64   LR: 0.0000158        poch 93/100][T][0]   Loss: 3.3171e+00   Top-1:  52.34   LR: 0.0000203        \n",
      "[Epoch 93][V][78]   Loss: 1.6853e+00   Top-1:  72.31   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.016803407648707\n",
      "T Top-1\t64.64493488471392\n",
      "V Loss\t1.6853257173538208\n",
      "V Top-1\t72.31\n",
      "\n",
      "Best acc1 72.61\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 94/100][T][1170]   Loss: 2.0024e+00   Top-1:  65.97   LR: 0.0000119        poch 94/100][T][0]   Loss: 1.1530e+00   Top-1:  89.84   LR: 0.0000158        \n",
      "[Epoch 94][V][78]   Loss: 1.6849e+00   Top-1:  72.35   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.002418342866417\n",
      "T Top-1\t65.96992421007685\n",
      "V Loss\t1.6848697528839112\n",
      "V Top-1\t72.35\n",
      "\n",
      "Best acc1 72.61\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 95/100][T][1170]   Loss: 1.9725e+00   Top-1:  66.59   LR: 0.0000086        poch 95/100][T][0]   Loss: 3.7977e+00   Top-1:  10.16   LR: 0.0000119        \n",
      "[Epoch 95][V][78]   Loss: 1.6861e+00   Top-1:  72.41   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.9724669619363968\n",
      "T Top-1\t66.59038748932537\n",
      "V Loss\t1.686086279296875\n",
      "V Top-1\t72.41\n",
      "\n",
      "Best acc1 72.61\n",
      "********************************************************************************\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 96/100][T][1170]   Loss: 1.9985e+00   Top-1:  65.89   LR: 0.0000059        poch 96/100][T][0]   Loss: 1.4472e+00   Top-1:  78.91   LR: 0.0000086        \n",
      "[Epoch 96][V][78]   Loss: 1.6841e+00   Top-1:  72.41   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.998485168160586\n",
      "T Top-1\t65.88786293766012\n",
      "V Loss\t1.6840811668395996\n",
      "V Top-1\t72.41\n",
      "\n",
      "Best acc1 72.61\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 97/100][T][1170]   Loss: 2.0058e+00   Top-1:  65.79   LR: 0.0000037        poch 97/100][T][0]   Loss: 3.4609e+00   Top-1:  38.28   LR: 0.0000059        \n",
      "[Epoch 97][V][78]   Loss: 1.6837e+00   Top-1:  72.40   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.005768992471247\n",
      "T Top-1\t65.79379269854824\n",
      "V Loss\t1.6836650527954102\n",
      "V Top-1\t72.4\n",
      "\n",
      "Best acc1 72.61\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 98/100][T][1170]   Loss: 2.0307e+00   Top-1:  65.33   LR: 0.0000022        poch 98/100][T][0]   Loss: 3.4514e+00   Top-1:  13.28   LR: 0.0000037        \n",
      "[Epoch 98][V][78]   Loss: 1.6847e+00   Top-1:  72.31   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.0306976058358925\n",
      "T Top-1\t65.33078031596926\n",
      "V Loss\t1.684655841445923\n",
      "V Top-1\t72.31\n",
      "\n",
      "Best acc1 72.61\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 99/100][T][1170]   Loss: 1.9985e+00   Top-1:  66.15   LR: 0.0000013        poch 99/100][T][0]   Loss: 1.1623e+00   Top-1:  89.06   LR: 0.0000022        \n",
      "[Epoch 99][V][78]   Loss: 1.6832e+00   Top-1:  72.45   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.99852383034948\n",
      "T Top-1\t66.15406169940222\n",
      "V Loss\t1.683194107055664\n",
      "V Top-1\t72.45\n",
      "\n",
      "Best acc1 72.61\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 100/100][T][1170]   Loss: 2.0294e+00   Top-1:  65.24   LR: 0.0000010        och 100/100][T][0]   Loss: 1.2397e+00   Top-1:  88.28   LR: 0.0000013        \n",
      "[Epoch 100][V][78]   Loss: 1.6840e+00   Top-1:  72.39   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.029447937194961\n",
      "T Top-1\t65.2353757472246\n",
      "V Loss\t1.6839570249557496\n",
      "V Top-1\t72.39\n",
      "\n",
      "Best acc1 72.61\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "100%|| 100/100 [3:33:26<00:00, 128.07s/it]\n",
      "\u001b[31m********************************************************************************\n",
      "best top-1: 72.61, final top-1: 72.39\n",
      "********************************************************************************\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python main.py --model vit --dataset CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b79f1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m********************************************************************************\n",
      "CIFAR10\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[32m********************************************************************************\n",
      "Creating model: swin-Base--CIFAR10-LR[0.001]-Seed0\n",
      "Number of params: 7,073,188\n",
      "Initial learning rate: 0.001000\n",
      "Start training for 100 epochs\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "label smoothing used\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Stochastic depth(0.1) used \n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Cutmix used\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Mixup used\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Repeated Aug(3) used\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Autoaugmentation used\n",
      "CIFAR Policy\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Random erasing(0.25) used \n",
      "********************************************************************************\u001b[0m\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 16, 16]           1,248\n",
      "         LayerNorm-2              [-1, 256, 96]             192\n",
      "        PatchEmbed-3              [-1, 256, 96]               0\n",
      "           Dropout-4              [-1, 256, 96]               0\n",
      "         LayerNorm-5              [-1, 256, 96]             192\n",
      "            Linear-6              [-1, 16, 288]          27,936\n",
      "           Softmax-7            [-1, 3, 16, 16]               0\n",
      "           Dropout-8            [-1, 3, 16, 16]               0\n",
      "            Linear-9               [-1, 16, 96]           9,312\n",
      "          Dropout-10               [-1, 16, 96]               0\n",
      "  WindowAttention-11               [-1, 16, 96]               0\n",
      "         Identity-12              [-1, 256, 96]               0\n",
      "        LayerNorm-13              [-1, 256, 96]             192\n",
      "           Linear-14             [-1, 256, 192]          18,624\n",
      "             GELU-15             [-1, 256, 192]               0\n",
      "          Dropout-16             [-1, 256, 192]               0\n",
      "           Linear-17              [-1, 256, 96]          18,528\n",
      "          Dropout-18              [-1, 256, 96]               0\n",
      "              Mlp-19              [-1, 256, 96]               0\n",
      "         Identity-20              [-1, 256, 96]               0\n",
      "SwinTransformerBlock-21              [-1, 256, 96]               0\n",
      "        LayerNorm-22              [-1, 256, 96]             192\n",
      "           Linear-23              [-1, 16, 288]          27,936\n",
      "          Softmax-24            [-1, 3, 16, 16]               0\n",
      "          Dropout-25            [-1, 3, 16, 16]               0\n",
      "           Linear-26               [-1, 16, 96]           9,312\n",
      "          Dropout-27               [-1, 16, 96]               0\n",
      "  WindowAttention-28               [-1, 16, 96]               0\n",
      "         DropPath-29              [-1, 256, 96]               0\n",
      "        LayerNorm-30              [-1, 256, 96]             192\n",
      "           Linear-31             [-1, 256, 192]          18,624\n",
      "             GELU-32             [-1, 256, 192]               0\n",
      "          Dropout-33             [-1, 256, 192]               0\n",
      "           Linear-34              [-1, 256, 96]          18,528\n",
      "          Dropout-35              [-1, 256, 96]               0\n",
      "              Mlp-36              [-1, 256, 96]               0\n",
      "         DropPath-37              [-1, 256, 96]               0\n",
      "SwinTransformerBlock-38              [-1, 256, 96]               0\n",
      "        LayerNorm-39              [-1, 64, 384]             768\n",
      "           Linear-40              [-1, 64, 192]          73,728\n",
      "     PatchMerging-41              [-1, 64, 192]               0\n",
      "       BasicLayer-42              [-1, 64, 192]               0\n",
      "        LayerNorm-43              [-1, 64, 192]             384\n",
      "           Linear-44              [-1, 16, 576]         111,168\n",
      "          Softmax-45            [-1, 6, 16, 16]               0\n",
      "          Dropout-46            [-1, 6, 16, 16]               0\n",
      "           Linear-47              [-1, 16, 192]          37,056\n",
      "          Dropout-48              [-1, 16, 192]               0\n",
      "  WindowAttention-49              [-1, 16, 192]               0\n",
      "         DropPath-50              [-1, 64, 192]               0\n",
      "        LayerNorm-51              [-1, 64, 192]             384\n",
      "           Linear-52              [-1, 64, 384]          74,112\n",
      "             GELU-53              [-1, 64, 384]               0\n",
      "          Dropout-54              [-1, 64, 384]               0\n",
      "           Linear-55              [-1, 64, 192]          73,920\n",
      "          Dropout-56              [-1, 64, 192]               0\n",
      "              Mlp-57              [-1, 64, 192]               0\n",
      "         DropPath-58              [-1, 64, 192]               0\n",
      "SwinTransformerBlock-59              [-1, 64, 192]               0\n",
      "        LayerNorm-60              [-1, 64, 192]             384\n",
      "           Linear-61              [-1, 16, 576]         111,168\n",
      "          Softmax-62            [-1, 6, 16, 16]               0\n",
      "          Dropout-63            [-1, 6, 16, 16]               0\n",
      "           Linear-64              [-1, 16, 192]          37,056\n",
      "          Dropout-65              [-1, 16, 192]               0\n",
      "  WindowAttention-66              [-1, 16, 192]               0\n",
      "         DropPath-67              [-1, 64, 192]               0\n",
      "        LayerNorm-68              [-1, 64, 192]             384\n",
      "           Linear-69              [-1, 64, 384]          74,112\n",
      "             GELU-70              [-1, 64, 384]               0\n",
      "          Dropout-71              [-1, 64, 384]               0\n",
      "           Linear-72              [-1, 64, 192]          73,920\n",
      "          Dropout-73              [-1, 64, 192]               0\n",
      "              Mlp-74              [-1, 64, 192]               0\n",
      "         DropPath-75              [-1, 64, 192]               0\n",
      "SwinTransformerBlock-76              [-1, 64, 192]               0\n",
      "        LayerNorm-77              [-1, 64, 192]             384\n",
      "           Linear-78              [-1, 16, 576]         111,168\n",
      "          Softmax-79            [-1, 6, 16, 16]               0\n",
      "          Dropout-80            [-1, 6, 16, 16]               0\n",
      "           Linear-81              [-1, 16, 192]          37,056\n",
      "          Dropout-82              [-1, 16, 192]               0\n",
      "  WindowAttention-83              [-1, 16, 192]               0\n",
      "         DropPath-84              [-1, 64, 192]               0\n",
      "        LayerNorm-85              [-1, 64, 192]             384\n",
      "           Linear-86              [-1, 64, 384]          74,112\n",
      "             GELU-87              [-1, 64, 384]               0\n",
      "          Dropout-88              [-1, 64, 384]               0\n",
      "           Linear-89              [-1, 64, 192]          73,920\n",
      "          Dropout-90              [-1, 64, 192]               0\n",
      "              Mlp-91              [-1, 64, 192]               0\n",
      "         DropPath-92              [-1, 64, 192]               0\n",
      "SwinTransformerBlock-93              [-1, 64, 192]               0\n",
      "        LayerNorm-94              [-1, 64, 192]             384\n",
      "           Linear-95              [-1, 16, 576]         111,168\n",
      "          Softmax-96            [-1, 6, 16, 16]               0\n",
      "          Dropout-97            [-1, 6, 16, 16]               0\n",
      "           Linear-98              [-1, 16, 192]          37,056\n",
      "          Dropout-99              [-1, 16, 192]               0\n",
      " WindowAttention-100              [-1, 16, 192]               0\n",
      "        DropPath-101              [-1, 64, 192]               0\n",
      "       LayerNorm-102              [-1, 64, 192]             384\n",
      "          Linear-103              [-1, 64, 384]          74,112\n",
      "            GELU-104              [-1, 64, 384]               0\n",
      "         Dropout-105              [-1, 64, 384]               0\n",
      "          Linear-106              [-1, 64, 192]          73,920\n",
      "         Dropout-107              [-1, 64, 192]               0\n",
      "             Mlp-108              [-1, 64, 192]               0\n",
      "        DropPath-109              [-1, 64, 192]               0\n",
      "SwinTransformerBlock-110              [-1, 64, 192]               0\n",
      "       LayerNorm-111              [-1, 64, 192]             384\n",
      "          Linear-112              [-1, 16, 576]         111,168\n",
      "         Softmax-113            [-1, 6, 16, 16]               0\n",
      "         Dropout-114            [-1, 6, 16, 16]               0\n",
      "          Linear-115              [-1, 16, 192]          37,056\n",
      "         Dropout-116              [-1, 16, 192]               0\n",
      " WindowAttention-117              [-1, 16, 192]               0\n",
      "        DropPath-118              [-1, 64, 192]               0\n",
      "       LayerNorm-119              [-1, 64, 192]             384\n",
      "          Linear-120              [-1, 64, 384]          74,112\n",
      "            GELU-121              [-1, 64, 384]               0\n",
      "         Dropout-122              [-1, 64, 384]               0\n",
      "          Linear-123              [-1, 64, 192]          73,920\n",
      "         Dropout-124              [-1, 64, 192]               0\n",
      "             Mlp-125              [-1, 64, 192]               0\n",
      "        DropPath-126              [-1, 64, 192]               0\n",
      "SwinTransformerBlock-127              [-1, 64, 192]               0\n",
      "       LayerNorm-128              [-1, 64, 192]             384\n",
      "          Linear-129              [-1, 16, 576]         111,168\n",
      "         Softmax-130            [-1, 6, 16, 16]               0\n",
      "         Dropout-131            [-1, 6, 16, 16]               0\n",
      "          Linear-132              [-1, 16, 192]          37,056\n",
      "         Dropout-133              [-1, 16, 192]               0\n",
      " WindowAttention-134              [-1, 16, 192]               0\n",
      "        DropPath-135              [-1, 64, 192]               0\n",
      "       LayerNorm-136              [-1, 64, 192]             384\n",
      "          Linear-137              [-1, 64, 384]          74,112\n",
      "            GELU-138              [-1, 64, 384]               0\n",
      "         Dropout-139              [-1, 64, 384]               0\n",
      "          Linear-140              [-1, 64, 192]          73,920\n",
      "         Dropout-141              [-1, 64, 192]               0\n",
      "             Mlp-142              [-1, 64, 192]               0\n",
      "        DropPath-143              [-1, 64, 192]               0\n",
      "SwinTransformerBlock-144              [-1, 64, 192]               0\n",
      "       LayerNorm-145              [-1, 16, 768]           1,536\n",
      "          Linear-146              [-1, 16, 384]         294,912\n",
      "    PatchMerging-147              [-1, 16, 384]               0\n",
      "      BasicLayer-148              [-1, 16, 384]               0\n",
      "       LayerNorm-149              [-1, 16, 384]             768\n",
      "          Linear-150             [-1, 16, 1152]         443,520\n",
      "         Softmax-151           [-1, 12, 16, 16]               0\n",
      "         Dropout-152           [-1, 12, 16, 16]               0\n",
      "          Linear-153              [-1, 16, 384]         147,840\n",
      "         Dropout-154              [-1, 16, 384]               0\n",
      " WindowAttention-155              [-1, 16, 384]               0\n",
      "        DropPath-156              [-1, 16, 384]               0\n",
      "       LayerNorm-157              [-1, 16, 384]             768\n",
      "          Linear-158              [-1, 16, 768]         295,680\n",
      "            GELU-159              [-1, 16, 768]               0\n",
      "         Dropout-160              [-1, 16, 768]               0\n",
      "          Linear-161              [-1, 16, 384]         295,296\n",
      "         Dropout-162              [-1, 16, 384]               0\n",
      "             Mlp-163              [-1, 16, 384]               0\n",
      "        DropPath-164              [-1, 16, 384]               0\n",
      "SwinTransformerBlock-165              [-1, 16, 384]               0\n",
      "       LayerNorm-166              [-1, 16, 384]             768\n",
      "          Linear-167             [-1, 16, 1152]         443,520\n",
      "         Softmax-168           [-1, 12, 16, 16]               0\n",
      "         Dropout-169           [-1, 12, 16, 16]               0\n",
      "          Linear-170              [-1, 16, 384]         147,840\n",
      "         Dropout-171              [-1, 16, 384]               0\n",
      " WindowAttention-172              [-1, 16, 384]               0\n",
      "        DropPath-173              [-1, 16, 384]               0\n",
      "       LayerNorm-174              [-1, 16, 384]             768\n",
      "          Linear-175              [-1, 16, 768]         295,680\n",
      "            GELU-176              [-1, 16, 768]               0\n",
      "         Dropout-177              [-1, 16, 768]               0\n",
      "          Linear-178              [-1, 16, 384]         295,296\n",
      "         Dropout-179              [-1, 16, 384]               0\n",
      "             Mlp-180              [-1, 16, 384]               0\n",
      "        DropPath-181              [-1, 16, 384]               0\n",
      "SwinTransformerBlock-182              [-1, 16, 384]               0\n",
      "       LayerNorm-183              [-1, 16, 384]             768\n",
      "          Linear-184             [-1, 16, 1152]         443,520\n",
      "         Softmax-185           [-1, 12, 16, 16]               0\n",
      "         Dropout-186           [-1, 12, 16, 16]               0\n",
      "          Linear-187              [-1, 16, 384]         147,840\n",
      "         Dropout-188              [-1, 16, 384]               0\n",
      " WindowAttention-189              [-1, 16, 384]               0\n",
      "        DropPath-190              [-1, 16, 384]               0\n",
      "       LayerNorm-191              [-1, 16, 384]             768\n",
      "          Linear-192              [-1, 16, 768]         295,680\n",
      "            GELU-193              [-1, 16, 768]               0\n",
      "         Dropout-194              [-1, 16, 768]               0\n",
      "          Linear-195              [-1, 16, 384]         295,296\n",
      "         Dropout-196              [-1, 16, 384]               0\n",
      "             Mlp-197              [-1, 16, 384]               0\n",
      "        DropPath-198              [-1, 16, 384]               0\n",
      "SwinTransformerBlock-199              [-1, 16, 384]               0\n",
      "       LayerNorm-200              [-1, 16, 384]             768\n",
      "          Linear-201             [-1, 16, 1152]         443,520\n",
      "         Softmax-202           [-1, 12, 16, 16]               0\n",
      "         Dropout-203           [-1, 12, 16, 16]               0\n",
      "          Linear-204              [-1, 16, 384]         147,840\n",
      "         Dropout-205              [-1, 16, 384]               0\n",
      " WindowAttention-206              [-1, 16, 384]               0\n",
      "        DropPath-207              [-1, 16, 384]               0\n",
      "       LayerNorm-208              [-1, 16, 384]             768\n",
      "          Linear-209              [-1, 16, 768]         295,680\n",
      "            GELU-210              [-1, 16, 768]               0\n",
      "         Dropout-211              [-1, 16, 768]               0\n",
      "          Linear-212              [-1, 16, 384]         295,296\n",
      "         Dropout-213              [-1, 16, 384]               0\n",
      "             Mlp-214              [-1, 16, 384]               0\n",
      "        DropPath-215              [-1, 16, 384]               0\n",
      "SwinTransformerBlock-216              [-1, 16, 384]               0\n",
      "      BasicLayer-217              [-1, 16, 384]               0\n",
      "       LayerNorm-218              [-1, 16, 384]             768\n",
      "AdaptiveAvgPool1d-219               [-1, 384, 1]               0\n",
      "          Linear-220                   [-1, 10]           3,850\n",
      "================================================================\n",
      "Total params: 7,044,202\n",
      "Trainable params: 7,044,202\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 19.76\n",
      "Params size (MB): 26.87\n",
      "Estimated Total Size (MB): 46.64\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Beginning training\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/100][T][1170]   Loss: 2.1534e+00   Top-1:  21.48   LR: 0.0001009        Epoch 1/100][T][0]   Loss: 2.5238e+00   Top-1:   8.59   LR: 0.0000011        \n",
      "[Epoch 1][V][78]   Loss: 1.7641e+00   Top-1:  41.10   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.1533856999660332\n",
      "T Top-1\t21.481372758326216\n",
      "V Loss\t1.7640729778289794\n",
      "V Top-1\t41.1\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 41.10\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 2/100][T][1170]   Loss: 1.9240e+00   Top-1:  33.68   LR: 0.0002008        Epoch 2/100][T][0]   Loss: 2.2613e+00   Top-1:  13.28   LR: 0.0001010        \n",
      "[Epoch 2][V][78]   Loss: 1.5443e+00   Top-1:  52.88   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.923975668739397\n",
      "T Top-1\t33.67914709649872\n",
      "V Loss\t1.544281760787964\n",
      "V Top-1\t52.88\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 52.88\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 3/100][T][1170]   Loss: 1.7771e+00   Top-1:  40.69   LR: 0.0003007        Epoch 3/100][T][0]   Loss: 1.7530e+00   Top-1:  48.44   LR: 0.0002009        \n",
      "[Epoch 3][V][78]   Loss: 1.3435e+00   Top-1:  62.70   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.7770581984499598\n",
      "T Top-1\t40.685044833475665\n",
      "V Loss\t1.3435212074279785\n",
      "V Top-1\t62.7\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 62.70\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 4/100][T][1170]   Loss: 1.7263e+00   Top-1:  44.12   LR: 0.0004006        Epoch 4/100][T][0]   Loss: 2.0327e+00   Top-1:  41.41   LR: 0.0003008        \n",
      "[Epoch 4][V][78]   Loss: 1.2928e+00   Top-1:  63.66   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.7263251600053757\n",
      "T Top-1\t44.11960930828352\n",
      "V Loss\t1.2928398475646972\n",
      "V Top-1\t63.66\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 63.66\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 5/100][T][1170]   Loss: 1.6669e+00   Top-1:  46.13   LR: 0.0005005        Epoch 5/100][T][0]   Loss: 1.5554e+00   Top-1:  48.44   LR: 0.0004007        \n",
      "[Epoch 5][V][78]   Loss: 1.1974e+00   Top-1:  68.95   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.6669068351757008\n",
      "T Top-1\t46.132445559350984\n",
      "V Loss\t1.1974444885253905\n",
      "V Top-1\t68.95\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 68.95\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 6/100][T][1170]   Loss: 1.6203e+00   Top-1:  48.87   LR: 0.0006004        Epoch 6/100][T][0]   Loss: 1.4273e+00   Top-1:  59.38   LR: 0.0005006        \n",
      "[Epoch 6][V][78]   Loss: 1.1974e+00   Top-1:  69.81   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.6203403635782425\n",
      "T Top-1\t48.87449295473954\n",
      "V Loss\t1.1973824571609497\n",
      "V Top-1\t69.81\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 69.81\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 7/100][T][1170]   Loss: 1.5824e+00   Top-1:  51.14   LR: 0.0007003        Epoch 7/100][T][0]   Loss: 1.8959e+00   Top-1:  43.75   LR: 0.0006005        \n",
      "[Epoch 7][V][78]   Loss: 1.1125e+00   Top-1:  73.43   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.582441699392871\n",
      "T Top-1\t51.13885034158839\n",
      "V Loss\t1.1124594955444336\n",
      "V Top-1\t73.43\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 73.43\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 8/100][T][1170]   Loss: 1.5741e+00   Top-1:  51.33   LR: 0.0008002        Epoch 8/100][T][0]   Loss: 1.4163e+00   Top-1:  64.06   LR: 0.0007004        \n",
      "[Epoch 8][V][78]   Loss: 1.1122e+00   Top-1:  74.87   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.574051129929944\n",
      "T Top-1\t51.32965947907771\n",
      "V Loss\t1.1122209117889403\n",
      "V Top-1\t74.87\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 74.87\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 9/100][T][1170]   Loss: 1.5325e+00   Top-1:  53.32   LR: 0.0009001        Epoch 9/100][T][0]   Loss: 1.2914e+00   Top-1:  67.97   LR: 0.0008003        \n",
      "[Epoch 9][V][78]   Loss: 1.0947e+00   Top-1:  73.60   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.532522727341656\n",
      "T Top-1\t53.315809137489325\n",
      "V Loss\t1.094680093574524\n",
      "V Top-1\t73.6\n",
      "\n",
      "Best acc1 74.87\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 10/100][T][1170]   Loss: 1.5328e+00   Top-1:  52.96   LR: 0.0010000        poch 10/100][T][0]   Loss: 1.2895e+00   Top-1:  64.06   LR: 0.0009002        \n",
      "[Epoch 10][V][78]   Loss: 1.0775e+00   Top-1:  76.15   LR: 0.0010\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.5327879151760464\n",
      "T Top-1\t52.955540136635356\n",
      "V Loss\t1.0774776035308837\n",
      "V Top-1\t76.15\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 76.15\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 11/100][T][1170]   Loss: 1.5052e+00   Top-1:  54.45   LR: 0.0009997        poch 11/100][T][0]   Loss: 1.2488e+00   Top-1:  71.88   LR: 0.0010000        \n",
      "[Epoch 11][V][78]   Loss: 1.0147e+00   Top-1:  77.79   LR: 0.0010\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.5052367315447097\n",
      "T Top-1\t54.445319171648165\n",
      "V Loss\t1.0147493152618408\n",
      "V Top-1\t77.79\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 77.79\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 12/100][T][1170]   Loss: 1.4615e+00   Top-1:  56.85   LR: 0.0009988        poch 12/100][T][0]   Loss: 1.2741e+00   Top-1:  71.88   LR: 0.0009997        \n",
      "[Epoch 12][V][78]   Loss: 9.5780e-01   Top-1:  80.34   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.4615227420751504\n",
      "T Top-1\t56.84777967549103\n",
      "V Loss\t0.9578007252693176\n",
      "V Top-1\t80.34\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 80.34\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 13/100][T][1170]   Loss: 1.4484e+00   Top-1:  56.47   LR: 0.0009973        poch 13/100][T][0]   Loss: 1.1457e+00   Top-1:  73.44   LR: 0.0009988        \n",
      "[Epoch 13][V][78]   Loss: 9.3379e-01   Top-1:  81.53   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.448432885765724\n",
      "T Top-1\t56.47350021349274\n",
      "V Loss\t0.9337876601219177\n",
      "V Top-1\t81.53\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 81.53\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 14/100][T][1170]   Loss: 1.4126e+00   Top-1:  59.31   LR: 0.0009951        poch 14/100][T][0]   Loss: 1.0385e+00   Top-1:  79.69   LR: 0.0009973        \n",
      "[Epoch 14][V][78]   Loss: 9.2652e-01   Top-1:  81.30   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.41255544664308\n",
      "T Top-1\t59.306949188727586\n",
      "V Loss\t0.9265222612380981\n",
      "V Top-1\t81.3\n",
      "\n",
      "Best acc1 81.53\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 15/100][T][1170]   Loss: 1.4079e+00   Top-1:  58.67   LR: 0.0009924        poch 15/100][T][0]   Loss: 9.7761e-01   Top-1:  81.25   LR: 0.0009951        \n",
      "[Epoch 15][V][78]   Loss: 9.1171e-01   Top-1:  81.76   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.4079209906641588\n",
      "T Top-1\t58.67180828351836\n",
      "V Loss\t0.9117094141960144\n",
      "V Top-1\t81.76\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 81.76\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 16/100][T][1170]   Loss: 1.3913e+00   Top-1:  59.66   LR: 0.0009891        poch 16/100][T][0]   Loss: 1.9423e+00   Top-1:  44.53   LR: 0.0009924        \n",
      "[Epoch 16][V][78]   Loss: 8.8675e-01   Top-1:  83.26   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T Loss\t1.391290201731567\n",
      "T Top-1\t59.66254803586678\n",
      "V Loss\t0.886749603176117\n",
      "V Top-1\t83.26\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 83.26\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 17/100][T][1170]   Loss: 1.3662e+00   Top-1:  60.61   LR: 0.0009852        poch 17/100][T][0]   Loss: 9.6775e-01   Top-1:  76.56   LR: 0.0009891        \n",
      "[Epoch 17][V][78]   Loss: 8.7090e-01   Top-1:  84.29   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3661859355147938\n",
      "T Top-1\t60.60591908625107\n",
      "V Loss\t0.8709032775878907\n",
      "V Top-1\t84.29\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 84.29\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 18/100][T][1170]   Loss: 1.3446e+00   Top-1:  62.64   LR: 0.0009807        poch 18/100][T][0]   Loss: 1.6901e+00   Top-1:  65.62   LR: 0.0009852        \n",
      "[Epoch 18][V][78]   Loss: 8.7041e-01   Top-1:  84.95   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3446418989122066\n",
      "T Top-1\t62.641438941076004\n",
      "V Loss\t0.8704122774124146\n",
      "V Top-1\t84.95\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 84.95\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 19/100][T][1170]   Loss: 1.3383e+00   Top-1:  62.51   LR: 0.0009756        poch 19/100][T][0]   Loss: 1.9125e+00   Top-1:  29.69   LR: 0.0009806        \n",
      "[Epoch 19][V][78]   Loss: 8.5406e-01   Top-1:  84.42   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3383351991972692\n",
      "T Top-1\t62.512676131511526\n",
      "V Loss\t0.8540600225448608\n",
      "V Top-1\t84.42\n",
      "\n",
      "Best acc1 84.95\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 20/100][T][1170]   Loss: 1.3280e+00   Top-1:  61.89   LR: 0.0009699        poch 20/100][T][0]   Loss: 1.0859e+00   Top-1:  73.44   LR: 0.0009755        \n",
      "[Epoch 20][V][78]   Loss: 8.4391e-01   Top-1:  85.11   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3280023915150754\n",
      "T Top-1\t61.886208368915455\n",
      "V Loss\t0.8439066040992736\n",
      "V Top-1\t85.11\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 85.11\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 21/100][T][1170]   Loss: 1.3034e+00   Top-1:  64.38   LR: 0.0009636        poch 21/100][T][0]   Loss: 1.8928e+00   Top-1:  31.25   LR: 0.0009699        \n",
      "[Epoch 21][V][78]   Loss: 8.2345e-01   Top-1:  86.52   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3034216732575687\n",
      "T Top-1\t64.380737617421\n",
      "V Loss\t0.8234487041473388\n",
      "V Top-1\t86.52\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 86.52\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 22/100][T][1170]   Loss: 1.3006e+00   Top-1:  64.74   LR: 0.0009568        poch 22/100][T][0]   Loss: 9.8185e-01   Top-1:  80.47   LR: 0.0009636        \n",
      "[Epoch 22][V][78]   Loss: 8.1303e-01   Top-1:  86.68   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.30062915548924\n",
      "T Top-1\t64.74300811272417\n",
      "V Loss\t0.8130291460037231\n",
      "V Top-1\t86.68\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 86.68\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 23/100][T][1170]   Loss: 1.2899e+00   Top-1:  64.54   LR: 0.0009494        poch 23/100][T][0]   Loss: 9.4357e-01   Top-1:  81.25   LR: 0.0009568        \n",
      "[Epoch 23][V][78]   Loss: 7.9751e-01   Top-1:  87.66   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2898872069168255\n",
      "T Top-1\t64.5421915029889\n",
      "V Loss\t0.7975149267196655\n",
      "V Top-1\t87.66\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 87.66\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 24/100][T][1170]   Loss: 1.2994e+00   Top-1:  63.75   LR: 0.0009415        poch 24/100][T][0]   Loss: 1.0177e+00   Top-1:  78.91   LR: 0.0009494        \n",
      "[Epoch 24][V][78]   Loss: 8.0593e-01   Top-1:  87.18   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.299442879671118\n",
      "T Top-1\t63.746263877028184\n",
      "V Loss\t0.8059304512023926\n",
      "V Top-1\t87.18\n",
      "\n",
      "Best acc1 87.66\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 25/100][T][1170]   Loss: 1.2843e+00   Top-1:  64.59   LR: 0.0009331        poch 25/100][T][0]   Loss: 1.6798e+00   Top-1:  60.16   LR: 0.0009415        \n",
      "[Epoch 25][V][78]   Loss: 7.8481e-01   Top-1:  87.47   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.284323776969535\n",
      "T Top-1\t64.58755871050384\n",
      "V Loss\t0.7848126854896545\n",
      "V Top-1\t87.47\n",
      "\n",
      "Best acc1 87.66\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 26/100][T][1170]   Loss: 1.2670e+00   Top-1:  65.20   LR: 0.0009241        poch 26/100][T][0]   Loss: 9.6787e-01   Top-1:  78.91   LR: 0.0009331        \n",
      "[Epoch 26][V][78]   Loss: 7.9266e-01   Top-1:  87.59   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2670052527974143\n",
      "T Top-1\t65.20068317677199\n",
      "V Loss\t0.7926631653785705\n",
      "V Top-1\t87.59\n",
      "\n",
      "Best acc1 87.66\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 27/100][T][1170]   Loss: 1.2727e+00   Top-1:  64.75   LR: 0.0009146        poch 27/100][T][0]   Loss: 9.5418e-01   Top-1:  82.03   LR: 0.0009241        \n",
      "[Epoch 27][V][78]   Loss: 7.8531e-01   Top-1:  87.38   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.272657748015898\n",
      "T Top-1\t64.74634393680614\n",
      "V Loss\t0.7853127954483032\n",
      "V Top-1\t87.38\n",
      "\n",
      "Best acc1 87.66\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 28/100][T][1170]   Loss: 1.2614e+00   Top-1:  64.89   LR: 0.0009046        poch 28/100][T][0]   Loss: 1.9320e+00   Top-1:  51.56   LR: 0.0009146        \n",
      "[Epoch 28][V][78]   Loss: 7.7200e-01   Top-1:  88.29   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2614051217100337\n",
      "T Top-1\t64.88978437233133\n",
      "V Loss\t0.7719965847969055\n",
      "V Top-1\t88.29\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 88.29\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 29/100][T][1170]   Loss: 1.2567e+00   Top-1:  65.40   LR: 0.0008941        poch 29/100][T][0]   Loss: 9.1189e-01   Top-1:  80.47   LR: 0.0009046        \n",
      "[Epoch 29][V][78]   Loss: 7.6095e-01   Top-1:  88.65   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2567313221354042\n",
      "T Top-1\t65.39883112724168\n",
      "V Loss\t0.7609533508300781\n",
      "V Top-1\t88.65\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 88.65\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 30/100][T][1170]   Loss: 1.2328e+00   Top-1:  66.53   LR: 0.0008831        poch 30/100][T][0]   Loss: 1.7925e+00   Top-1:  18.75   LR: 0.0008941        \n",
      "[Epoch 30][V][78]   Loss: 7.8466e-01   Top-1:  88.37   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2327817771090697\n",
      "T Top-1\t66.52767399658411\n",
      "V Loss\t0.7846625300407409\n",
      "V Top-1\t88.37\n",
      "\n",
      "Best acc1 88.65\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 31/100][T][1170]   Loss: 1.2326e+00   Top-1:  66.96   LR: 0.0008717        poch 31/100][T][0]   Loss: 1.6302e+00   Top-1:  60.16   LR: 0.0008831        \n",
      "[Epoch 31][V][78]   Loss: 7.6670e-01   Top-1:  88.54   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2326082946600616\n",
      "T Top-1\t66.95799530315969\n",
      "V Loss\t0.7667008519172669\n",
      "V Top-1\t88.54\n",
      "\n",
      "Best acc1 88.65\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 32/100][T][1170]   Loss: 1.2503e+00   Top-1:  65.58   LR: 0.0008598        poch 32/100][T][0]   Loss: 1.4392e+00   Top-1:  79.69   LR: 0.0008717        \n",
      "[Epoch 32][V][78]   Loss: 7.6196e-01   Top-1:  88.83   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2502635744878312\n",
      "T Top-1\t65.57562980358668\n",
      "V Loss\t0.7619569159507752\n",
      "V Top-1\t88.83\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 88.83\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 33/100][T][1170]   Loss: 1.2243e+00   Top-1:  67.13   LR: 0.0008475        poch 33/100][T][0]   Loss: 1.8526e+00   Top-1:  50.00   LR: 0.0008598        \n",
      "[Epoch 33][V][78]   Loss: 7.6117e-01   Top-1:  89.30   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2243081817863133\n",
      "T Top-1\t67.12612083689154\n",
      "V Loss\t0.7611673805236816\n",
      "V Top-1\t89.3\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 89.30\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 34/100][T][1170]   Loss: 1.2134e+00   Top-1:  68.04   LR: 0.0008347        poch 34/100][T][0]   Loss: 8.1592e-01   Top-1:  86.72   LR: 0.0008475        \n",
      "[Epoch 34][V][78]   Loss: 7.5937e-01   Top-1:  89.10   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2134389855941068\n",
      "T Top-1\t68.039469470538\n",
      "V Loss\t0.7593656177520752\n",
      "V Top-1\t89.1\n",
      "\n",
      "Best acc1 89.30\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 35/100][T][1170]   Loss: 1.1962e+00   Top-1:  68.69   LR: 0.0008216        poch 35/100][T][0]   Loss: 1.7978e+00   Top-1:  58.59   LR: 0.0008347        \n",
      "[Epoch 35][V][78]   Loss: 7.4293e-01   Top-1:  89.57   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.196242637353306\n",
      "T Top-1\t68.68595217762596\n",
      "V Loss\t0.742933009815216\n",
      "V Top-1\t89.57\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 89.57\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 36/100][T][1170]   Loss: 1.1859e+00   Top-1:  68.50   LR: 0.0008080        poch 36/100][T][0]   Loss: 8.4181e-01   Top-1:  82.81   LR: 0.0008216        \n",
      "[Epoch 36][V][78]   Loss: 7.3276e-01   Top-1:  90.12   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1858849275529537\n",
      "T Top-1\t68.5004803586678\n",
      "V Loss\t0.732762518119812\n",
      "V Top-1\t90.12\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 90.12\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 37/100][T][1170]   Loss: 1.1936e+00   Top-1:  68.75   LR: 0.0007941        poch 37/100][T][0]   Loss: 1.5048e+00   Top-1:  10.94   LR: 0.0008080        \n",
      "[Epoch 37][V][78]   Loss: 7.4667e-01   Top-1:  89.81   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1935873075390757\n",
      "T Top-1\t68.74599701110162\n",
      "V Loss\t0.7466747923851014\n",
      "V Top-1\t89.81\n",
      "\n",
      "Best acc1 90.12\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 38/100][T][1170]   Loss: 1.2127e+00   Top-1:  67.42   LR: 0.0007798        poch 38/100][T][0]   Loss: 1.2487e+00   Top-1:  81.25   LR: 0.0007941        \n",
      "[Epoch 38][V][78]   Loss: 7.3653e-01   Top-1:  90.27   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2127477363273693\n",
      "T Top-1\t67.42234201537148\n",
      "V Loss\t0.7365286375045776\n",
      "V Top-1\t90.27\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 90.27\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 39/100][T][1170]   Loss: 1.1857e+00   Top-1:  68.31   LR: 0.0007652        poch 39/100][T][0]   Loss: 1.1166e+00   Top-1:  81.25   LR: 0.0007798        \n",
      "[Epoch 39][V][78]   Loss: 7.5880e-01   Top-1:  90.05   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.185732570649415\n",
      "T Top-1\t68.31367421007685\n",
      "V Loss\t0.7588037963867188\n",
      "V Top-1\t90.05\n",
      "\n",
      "Best acc1 90.27\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 40/100][T][1170]   Loss: 1.1831e+00   Top-1:  69.49   LR: 0.0007503        poch 40/100][T][0]   Loss: 1.8039e+00   Top-1:  42.97   LR: 0.0007652        \n",
      "[Epoch 40][V][78]   Loss: 7.2009e-01   Top-1:  90.62   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1831491755482277\n",
      "T Top-1\t69.48788428693425\n",
      "V Loss\t0.7200871059417725\n",
      "V Top-1\t90.62\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 90.62\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 41/100][T][1170]   Loss: 1.2050e+00   Top-1:  68.34   LR: 0.0007350        poch 41/100][T][0]   Loss: 1.8545e+00   Top-1:  36.72   LR: 0.0007502        \n",
      "[Epoch 41][V][78]   Loss: 7.1422e-01   Top-1:  90.55   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2049536182776563\n",
      "T Top-1\t68.3430294619983\n",
      "V Loss\t0.7142231392860413\n",
      "V Top-1\t90.55\n",
      "\n",
      "Best acc1 90.62\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 42/100][T][1170]   Loss: 1.1770e+00   Top-1:  68.58   LR: 0.0007195        poch 42/100][T][0]   Loss: 1.3673e+00   Top-1:  11.72   LR: 0.0007350        \n",
      "[Epoch 42][V][78]   Loss: 7.1714e-01   Top-1:  90.87   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1769813890644467\n",
      "T Top-1\t68.58187446626815\n",
      "V Loss\t0.7171434944152832\n",
      "V Top-1\t90.87\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 90.87\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 43/100][T][1170]   Loss: 1.1784e+00   Top-1:  67.49   LR: 0.0007037        poch 43/100][T][0]   Loss: 1.6718e+00   Top-1:  17.97   LR: 0.0007195        \n",
      "[Epoch 43][V][78]   Loss: 7.1509e-01   Top-1:  90.75   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1783583864098832\n",
      "T Top-1\t67.49372865072587\n",
      "V Loss\t0.7150897673606873\n",
      "V Top-1\t90.75\n",
      "\n",
      "Best acc1 90.87\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 44/100][T][1170]   Loss: 1.1594e+00   Top-1:  68.88   LR: 0.0006876        poch 44/100][T][0]   Loss: 1.8605e+00   Top-1:  45.31   LR: 0.0007037        \n",
      "[Epoch 44][V][78]   Loss: 7.0092e-01   Top-1:  91.36   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.159367278587299\n",
      "T Top-1\t68.88343296327925\n",
      "V Loss\t0.7009216871261597\n",
      "V Top-1\t91.36\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 91.36\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 45/100][T][1170]   Loss: 1.1576e+00   Top-1:  70.09   LR: 0.0006713        poch 45/100][T][0]   Loss: 1.4316e+00   Top-1:  79.69   LR: 0.0006876        \n",
      "[Epoch 45][V][78]   Loss: 7.0649e-01   Top-1:  91.42   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1575937242613807\n",
      "T Top-1\t70.08699829205807\n",
      "V Loss\t0.7064872114181519\n",
      "V Top-1\t91.42\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 91.42\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 46/100][T][1170]   Loss: 1.1514e+00   Top-1:  69.58   LR: 0.0006549        poch 46/100][T][0]   Loss: 8.2071e-01   Top-1:  86.72   LR: 0.0006713        \n",
      "[Epoch 46][V][78]   Loss: 7.2002e-01   Top-1:  90.99   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1513550311047027\n",
      "T Top-1\t69.57795153714774\n",
      "V Loss\t0.7200191529273987\n",
      "V Top-1\t90.99\n",
      "\n",
      "Best acc1 91.42\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 47/100][T][1170]   Loss: 1.1483e+00   Top-1:  70.83   LR: 0.0006382        poch 47/100][T][0]   Loss: 8.2889e-01   Top-1:  85.94   LR: 0.0006548        \n",
      "[Epoch 47][V][78]   Loss: 6.9614e-01   Top-1:  91.93   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1483038258695073\n",
      "T Top-1\t70.82554974380871\n",
      "V Loss\t0.6961369102478028\n",
      "V Top-1\t91.93\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 91.93\n",
      "********************************************************************************\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 48/100][T][1170]   Loss: 1.1449e+00   Top-1:  71.02   LR: 0.0006213        poch 48/100][T][0]   Loss: 8.2403e-01   Top-1:  85.94   LR: 0.0006382        \n",
      "[Epoch 48][V][78]   Loss: 6.8752e-01   Top-1:  91.95   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1448719657760313\n",
      "T Top-1\t71.02436485909479\n",
      "V Loss\t0.6875212334632873\n",
      "V Top-1\t91.95\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 91.95\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 49/100][T][1170]   Loss: 1.1628e+00   Top-1:  70.22   LR: 0.0006044        poch 49/100][T][0]   Loss: 1.5951e+00   Top-1:  13.28   LR: 0.0006213        \n",
      "[Epoch 49][V][78]   Loss: 6.8358e-01   Top-1:  92.04   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1628199068528786\n",
      "T Top-1\t70.21976409052093\n",
      "V Loss\t0.6835779211044312\n",
      "V Top-1\t92.04\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 92.04\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 50/100][T][1170]   Loss: 1.1128e+00   Top-1:  71.89   LR: 0.0005872        poch 50/100][T][0]   Loss: 1.4106e+00   Top-1:  77.34   LR: 0.0006043        \n",
      "[Epoch 50][V][78]   Loss: 6.8833e-01   Top-1:  91.76   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1127675144617422\n",
      "T Top-1\t71.88634180187874\n",
      "V Loss\t0.6883339077949524\n",
      "V Top-1\t91.76\n",
      "\n",
      "Best acc1 92.04\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 51/100][T][1170]   Loss: 1.1315e+00   Top-1:  71.12   LR: 0.0005700        poch 51/100][T][0]   Loss: 8.1177e-01   Top-1:  86.72   LR: 0.0005872        \n",
      "[Epoch 51][V][78]   Loss: 6.8303e-01   Top-1:  92.27   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1314897110684115\n",
      "T Top-1\t71.12177092228865\n",
      "V Loss\t0.683029987335205\n",
      "V Top-1\t92.27\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 92.27\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 52/100][T][1170]   Loss: 1.1340e+00   Top-1:  71.04   LR: 0.0005527        poch 52/100][T][0]   Loss: 1.3945e+00   Top-1:  70.31   LR: 0.0005700        \n",
      "[Epoch 52][V][78]   Loss: 7.0223e-01   Top-1:  91.70   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1340312956236653\n",
      "T Top-1\t71.04104397950469\n",
      "V Loss\t0.7022324844360351\n",
      "V Top-1\t91.7\n",
      "\n",
      "Best acc1 92.27\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 53/100][T][1170]   Loss: 1.1356e+00   Top-1:  71.33   LR: 0.0005353        poch 53/100][T][0]   Loss: 7.5571e-01   Top-1:  88.28   LR: 0.0005527        \n",
      "[Epoch 53][V][78]   Loss: 6.9543e-01   Top-1:  92.07   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1356207119105313\n",
      "T Top-1\t71.32592335610589\n",
      "V Loss\t0.6954293918609619\n",
      "V Top-1\t92.07\n",
      "\n",
      "Best acc1 92.27\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 54/100][T][1170]   Loss: 1.1197e+00   Top-1:  72.21   LR: 0.0005179        poch 54/100][T][0]   Loss: 1.0039e+00   Top-1:  13.28   LR: 0.0005353        \n",
      "[Epoch 54][V][78]   Loss: 6.7343e-01   Top-1:  92.52   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.119714398194744\n",
      "T Top-1\t72.2125853970965\n",
      "V Loss\t0.6734289573669434\n",
      "V Top-1\t92.52\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 92.52\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 55/100][T][1170]   Loss: 1.1118e+00   Top-1:  72.69   LR: 0.0005005        poch 55/100][T][0]   Loss: 9.3151e-01   Top-1:  92.19   LR: 0.0005179        \n",
      "[Epoch 55][V][78]   Loss: 6.8565e-01   Top-1:  92.29   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1118343590061244\n",
      "T Top-1\t72.68760674637062\n",
      "V Loss\t0.6856524095535278\n",
      "V Top-1\t92.29\n",
      "\n",
      "Best acc1 92.52\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 56/100][T][1170]   Loss: 1.0952e+00   Top-1:  72.87   LR: 0.0004831        poch 56/100][T][0]   Loss: 7.7897e-01   Top-1:  88.28   LR: 0.0005005        \n",
      "[Epoch 56][V][78]   Loss: 6.8663e-01   Top-1:  92.26   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.095223058705448\n",
      "T Top-1\t72.87241140051238\n",
      "V Loss\t0.6866304617881774\n",
      "V Top-1\t92.26\n",
      "\n",
      "Best acc1 92.52\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 57/100][T][1170]   Loss: 1.0967e+00   Top-1:  72.60   LR: 0.0004657        poch 57/100][T][0]   Loss: 8.1183e-01   Top-1:  86.72   LR: 0.0004831        \n",
      "[Epoch 57][V][78]   Loss: 6.7183e-01   Top-1:  92.52   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0967255746269715\n",
      "T Top-1\t72.59753949615713\n",
      "V Loss\t0.6718261092185974\n",
      "V Top-1\t92.52\n",
      "\n",
      "Best acc1 92.52\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 58/100][T][1170]   Loss: 1.1067e+00   Top-1:  72.27   LR: 0.0004483        poch 58/100][T][0]   Loss: 1.5029e+00   Top-1:  73.44   LR: 0.0004656        \n",
      "[Epoch 58][V][78]   Loss: 6.8528e-01   Top-1:  92.27   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1067003470827228\n",
      "T Top-1\t72.2652914175918\n",
      "V Loss\t0.685278539276123\n",
      "V Top-1\t92.27\n",
      "\n",
      "Best acc1 92.52\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 59/100][T][1170]   Loss: 1.1196e+00   Top-1:  71.15   LR: 0.0004310        poch 59/100][T][0]   Loss: 1.7264e+00   Top-1:  49.22   LR: 0.0004483        \n",
      "[Epoch 59][V][78]   Loss: 6.7539e-01   Top-1:  92.50   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.119558450556738\n",
      "T Top-1\t71.15312766865927\n",
      "V Loss\t0.6753894060134887\n",
      "V Top-1\t92.5\n",
      "\n",
      "Best acc1 92.52\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 60/100][T][1170]   Loss: 1.0938e+00   Top-1:  72.03   LR: 0.0004138        poch 60/100][T][0]   Loss: 6.9392e-01   Top-1:  94.53   LR: 0.0004310        \n",
      "[Epoch 60][V][78]   Loss: 6.6483e-01   Top-1:  92.99   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0937523943150176\n",
      "T Top-1\t72.0337852263023\n",
      "V Loss\t0.6648328086853027\n",
      "V Top-1\t92.99\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 92.99\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 61/100][T][1170]   Loss: 1.0889e+00   Top-1:  73.21   LR: 0.0003966        poch 61/100][T][0]   Loss: 1.6948e+00   Top-1:  50.00   LR: 0.0004137        \n",
      "[Epoch 61][V][78]   Loss: 6.7940e-01   Top-1:  92.71   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0888679835350468\n",
      "T Top-1\t73.20866246797608\n",
      "V Loss\t0.6793997400283813\n",
      "V Top-1\t92.71\n",
      "\n",
      "Best acc1 92.99\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 62/100][T][1170]   Loss: 1.0687e+00   Top-1:  73.57   LR: 0.0003797        poch 62/100][T][0]   Loss: 7.2048e-01   Top-1:  91.41   LR: 0.0003966        \n",
      "[Epoch 62][V][78]   Loss: 6.5982e-01   Top-1:  93.13   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0687463701942046\n",
      "T Top-1\t73.57026579846286\n",
      "V Loss\t0.6598202046394348\n",
      "V Top-1\t93.13\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 93.13\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 63/100][T][1170]   Loss: 1.0810e+00   Top-1:  72.98   LR: 0.0003628        poch 63/100][T][0]   Loss: 1.8205e+00   Top-1:  48.44   LR: 0.0003796        \n",
      "[Epoch 63][V][78]   Loss: 6.8638e-01   Top-1:  92.71   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0810379574280535\n",
      "T Top-1\t72.9758219470538\n",
      "V Loss\t0.6863777857780456\n",
      "V Top-1\t92.71\n",
      "\n",
      "Best acc1 93.13\n",
      "********************************************************************************\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 64/100][T][1170]   Loss: 1.0765e+00   Top-1:  73.55   LR: 0.0003461        poch 64/100][T][0]   Loss: 1.1203e+00   Top-1:  85.16   LR: 0.0003628        \n",
      "[Epoch 64][V][78]   Loss: 6.5975e-01   Top-1:  93.15   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0765475370657942\n",
      "T Top-1\t73.54891652433817\n",
      "V Loss\t0.6597539915084839\n",
      "V Top-1\t93.15\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 93.15\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 65/100][T][1170]   Loss: 1.0774e+00   Top-1:  73.05   LR: 0.0003297        poch 65/100][T][0]   Loss: 7.6259e-01   Top-1:  89.84   LR: 0.0003461        \n",
      "[Epoch 65][V][78]   Loss: 6.6178e-01   Top-1:  92.97   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0774152546637492\n",
      "T Top-1\t73.05321306575577\n",
      "V Loss\t0.6617825212478637\n",
      "V Top-1\t92.97\n",
      "\n",
      "Best acc1 93.15\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 66/100][T][1170]   Loss: 1.0794e+00   Top-1:  72.25   LR: 0.0003134        poch 66/100][T][0]   Loss: 1.5913e+00   Top-1:  23.44   LR: 0.0003296        \n",
      "[Epoch 66][V][78]   Loss: 6.6355e-01   Top-1:  93.09   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.079389830515183\n",
      "T Top-1\t72.25128095644749\n",
      "V Loss\t0.6635520773887634\n",
      "V Top-1\t93.09\n",
      "\n",
      "Best acc1 93.15\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 67/100][T][1170]   Loss: 1.0763e+00   Top-1:  72.80   LR: 0.0002973        poch 67/100][T][0]   Loss: 7.5652e-01   Top-1:  89.84   LR: 0.0003134        \n",
      "[Epoch 67][V][78]   Loss: 6.5086e-01   Top-1:  93.54   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.076252027722529\n",
      "T Top-1\t72.79568744662681\n",
      "V Loss\t0.6508607320785522\n",
      "V Top-1\t93.54\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 93.54\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 68/100][T][1170]   Loss: 1.0630e+00   Top-1:  73.87   LR: 0.0002815        poch 68/100][T][0]   Loss: 6.6651e-01   Top-1:  91.41   LR: 0.0002973        \n",
      "[Epoch 68][V][78]   Loss: 6.6302e-01   Top-1:  93.16   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0630450207488746\n",
      "T Top-1\t73.86715414175919\n",
      "V Loss\t0.6630220825195312\n",
      "V Top-1\t93.16\n",
      "\n",
      "Best acc1 93.54\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 69/100][T][1170]   Loss: 1.0473e+00   Top-1:  74.19   LR: 0.0002660        poch 69/100][T][0]   Loss: 8.1944e-01   Top-1:  86.72   LR: 0.0002815        \n",
      "[Epoch 69][V][78]   Loss: 6.4929e-01   Top-1:  93.91   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.047290097791654\n",
      "T Top-1\t74.19206340734415\n",
      "V Loss\t0.6492873295783996\n",
      "V Top-1\t93.91\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 93.91\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 70/100][T][1170]   Loss: 1.0680e+00   Top-1:  73.79   LR: 0.0002508        poch 70/100][T][0]   Loss: 1.5564e+00   Top-1:  73.44   LR: 0.0002660        \n",
      "[Epoch 70][V][78]   Loss: 6.4380e-01   Top-1:  93.85   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0680035005564572\n",
      "T Top-1\t73.78976302305722\n",
      "V Loss\t0.6437951675415039\n",
      "V Top-1\t93.85\n",
      "\n",
      "Best acc1 93.91\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 71/100][T][1170]   Loss: 1.0498e+00   Top-1:  74.13   LR: 0.0002358        poch 71/100][T][0]   Loss: 1.6373e+00   Top-1:  68.75   LR: 0.0002507        \n",
      "[Epoch 71][V][78]   Loss: 6.4814e-01   Top-1:  93.55   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0498061345032808\n",
      "T Top-1\t74.12734842015371\n",
      "V Loss\t0.648136148071289\n",
      "V Top-1\t93.55\n",
      "\n",
      "Best acc1 93.91\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 72/100][T][1170]   Loss: 1.0406e+00   Top-1:  75.19   LR: 0.0002212        poch 72/100][T][0]   Loss: 6.9418e-01   Top-1:  92.19   LR: 0.0002358        \n",
      "[Epoch 72][V][78]   Loss: 6.5183e-01   Top-1:  93.52   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0406310361842637\n",
      "T Top-1\t75.18880764304014\n",
      "V Loss\t0.6518327923774719\n",
      "V Top-1\t93.52\n",
      "\n",
      "Best acc1 93.91\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 73/100][T][1170]   Loss: 1.0408e+00   Top-1:  74.39   LR: 0.0002069        poch 73/100][T][0]   Loss: 7.0596e-01   Top-1:  92.19   LR: 0.0002212        \n",
      "[Epoch 73][V][78]   Loss: 6.4038e-01   Top-1:  93.96   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.040822504373008\n",
      "T Top-1\t74.38887702818104\n",
      "V Loss\t0.6403841725349426\n",
      "V Top-1\t93.96\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 93.96\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 74/100][T][1170]   Loss: 1.0442e+00   Top-1:  74.38   LR: 0.0001930        poch 74/100][T][0]   Loss: 1.6891e+00   Top-1:  60.16   LR: 0.0002069        \n",
      "[Epoch 74][V][78]   Loss: 6.4882e-01   Top-1:  94.24   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0441529023453886\n",
      "T Top-1\t74.3788695559351\n",
      "V Loss\t0.6488169393539429\n",
      "V Top-1\t94.24\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 94.24\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 75/100][T][1170]   Loss: 1.0324e+00   Top-1:  74.96   LR: 0.0001794        poch 75/100][T][0]   Loss: 7.4474e-01   Top-1:  89.84   LR: 0.0001930        \n",
      "[Epoch 75][V][78]   Loss: 6.4107e-01   Top-1:  94.20   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.032367569681521\n",
      "T Top-1\t74.964640264731\n",
      "V Loss\t0.6410692000389099\n",
      "V Top-1\t94.2\n",
      "\n",
      "Best acc1 94.24\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 76/100][T][1170]   Loss: 1.0098e+00   Top-1:  75.88   LR: 0.0001663        poch 76/100][T][0]   Loss: 1.1794e+00   Top-1:  85.94   LR: 0.0001794        \n",
      "[Epoch 76][V][78]   Loss: 6.4352e-01   Top-1:  94.06   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0098138896447793\n",
      "T Top-1\t75.87999039282664\n",
      "V Loss\t0.643520058631897\n",
      "V Top-1\t94.06\n",
      "\n",
      "Best acc1 94.24\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 77/100][T][1170]   Loss: 1.0027e+00   Top-1:  76.27   LR: 0.0001535        poch 77/100][T][0]   Loss: 1.3503e+00   Top-1:  85.16   LR: 0.0001663        \n",
      "[Epoch 77][V][78]   Loss: 6.3226e-01   Top-1:  94.40   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0026670659229757\n",
      "T Top-1\t76.27495196413322\n",
      "V Loss\t0.6322560514450073\n",
      "V Top-1\t94.4\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 94.40\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 78/100][T][1170]   Loss: 1.0213e+00   Top-1:  75.47   LR: 0.0001412        poch 78/100][T][0]   Loss: 6.8879e-01   Top-1:  92.97   LR: 0.0001535        \n",
      "[Epoch 78][V][78]   Loss: 6.3648e-01   Top-1:  94.41   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0212816515099177\n",
      "T Top-1\t75.46701537147737\n",
      "V Loss\t0.6364847236633301\n",
      "V Top-1\t94.41\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 94.41\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 79/100][T][1170]   Loss: 9.9679e-01   Top-1:  76.29   LR: 0.0001293        poch 79/100][T][0]   Loss: 1.2223e+00   Top-1:  86.72   LR: 0.0001412        \n",
      "[Epoch 79][V][78]   Loss: 6.3347e-01   Top-1:  94.55   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t0.9967857763158471\n",
      "T Top-1\t76.29429974380871\n",
      "V Loss\t0.6334748167037964\n",
      "V Top-1\t94.55\n",
      "\n",
      "* Best model upate *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best acc1 94.55\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 80/100][T][1170]   Loss: 1.0123e+00   Top-1:  75.52   LR: 0.0001179        poch 80/100][T][0]   Loss: 1.6738e+00   Top-1:  66.41   LR: 0.0001293        \n",
      "[Epoch 80][V][78]   Loss: 6.3682e-01   Top-1:  94.57   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0123046051733113\n",
      "T Top-1\t75.52105572160546\n",
      "V Loss\t0.6368175977706909\n",
      "V Top-1\t94.57\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 94.57\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 81/100][T][1170]   Loss: 1.0060e+00   Top-1:  77.65   LR: 0.0001069        poch 81/100][T][0]   Loss: 6.8670e-01   Top-1:  89.84   LR: 0.0001179        \n",
      "[Epoch 81][V][78]   Loss: 6.3011e-01   Top-1:  94.64   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0059680618555702\n",
      "T Top-1\t77.6453084970111\n",
      "V Loss\t0.6301060976982117\n",
      "V Top-1\t94.64\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 94.64\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 82/100][T][1170]   Loss: 1.0048e+00   Top-1:  76.52   LR: 0.0000964        poch 82/100][T][0]   Loss: 6.3518e-01   Top-1:  94.53   LR: 0.0001069        \n",
      "[Epoch 82][V][78]   Loss: 6.3172e-01   Top-1:  94.60   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0047772087468538\n",
      "T Top-1\t76.52447160546541\n",
      "V Loss\t0.6317241509437561\n",
      "V Top-1\t94.6\n",
      "\n",
      "Best acc1 94.64\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 83/100][T][1170]   Loss: 9.8578e-01   Top-1:  76.17   LR: 0.0000864        poch 83/100][T][0]   Loss: 6.8821e-01   Top-1:  89.84   LR: 0.0000964        \n",
      "[Epoch 83][V][78]   Loss: 6.3517e-01   Top-1:  94.54   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t0.9857767114875461\n",
      "T Top-1\t76.16887275832622\n",
      "V Loss\t0.6351669141769409\n",
      "V Top-1\t94.54\n",
      "\n",
      "Best acc1 94.64\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 84/100][T][1170]   Loss: 9.8428e-01   Top-1:  77.63   LR: 0.0000769        poch 84/100][T][0]   Loss: 6.4208e-01   Top-1:  93.75   LR: 0.0000864        \n",
      "[Epoch 84][V][78]   Loss: 6.3097e-01   Top-1:  94.43   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t0.9842817238718907\n",
      "T Top-1\t77.63196520068318\n",
      "V Loss\t0.6309701494216919\n",
      "V Top-1\t94.43\n",
      "\n",
      "Best acc1 94.64\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 85/100][T][1170]   Loss: 9.8981e-01   Top-1:  76.26   LR: 0.0000679        poch 85/100][T][0]   Loss: 6.1420e-01   Top-1:  96.09   LR: 0.0000769        \n",
      "[Epoch 85][V][78]   Loss: 6.2898e-01   Top-1:  94.77   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t0.9898051580845241\n",
      "T Top-1\t76.26361016225448\n",
      "V Loss\t0.6289761922836303\n",
      "V Top-1\t94.77\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 94.77\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 86/100][T][1170]   Loss: 9.9364e-01   Top-1:  77.42   LR: 0.0000595        poch 86/100][T][0]   Loss: 8.0883e-01   Top-1:  93.75   LR: 0.0000679        \n",
      "[Epoch 86][V][78]   Loss: 6.2513e-01   Top-1:  94.81   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t0.9936442473930339\n",
      "T Top-1\t77.41580380017079\n",
      "V Loss\t0.6251295412063599\n",
      "V Top-1\t94.81\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 94.81\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 87/100][T][1170]   Loss: 9.5379e-01   Top-1:  78.77   LR: 0.0000516        poch 87/100][T][0]   Loss: 8.2296e-01   Top-1:   3.12   LR: 0.0000595        \n",
      "[Epoch 87][V][78]   Loss: 6.2387e-01   Top-1:  94.93   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t0.9537918930579006\n",
      "T Top-1\t78.76547822374039\n",
      "V Loss\t0.6238661584854126\n",
      "V Top-1\t94.93\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 94.93\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 88/100][T][1170]   Loss: 9.8071e-01   Top-1:  78.07   LR: 0.0000442        poch 88/100][T][0]   Loss: 6.6529e-01   Top-1:  92.19   LR: 0.0000515        \n",
      "[Epoch 88][V][78]   Loss: 6.2391e-01   Top-1:  95.06   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t0.9807117645502295\n",
      "T Top-1\t78.0716268146883\n",
      "V Loss\t0.6239083820343018\n",
      "V Top-1\t95.06\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 95.06\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 89/100][T][1170]   Loss: 9.5490e-01   Top-1:  78.30   LR: 0.0000374        poch 89/100][T][0]   Loss: 6.0786e-01   Top-1:  96.09   LR: 0.0000442        \n",
      "[Epoch 89][V][78]   Loss: 6.2391e-01   Top-1:  94.79   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t0.9549016890415872\n",
      "T Top-1\t78.29979718189581\n",
      "V Loss\t0.623906598854065\n",
      "V Top-1\t94.79\n",
      "\n",
      "Best acc1 95.06\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 90/100][T][1018]   Loss: 9.7345e-01   Top-1:  76.76   LR: 0.0000319        poch 90/100][T][0]   Loss: 6.2326e-01   Top-1:  95.31   LR: 0.0000374        \r"
     ]
    }
   ],
   "source": [
    "!python main.py --model swin --dataset CIFAR10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ba9360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m********************************************************************************\n",
      "CIFAR10\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[32m********************************************************************************\n",
      "Creating model: resnet-Base--CIFAR10-LR[0.001]-Seed0\n",
      "Number of params: 58,240,010\n",
      "Initial learning rate: 0.001000\n",
      "Start training for 100 epochs\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "label smoothing used\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Stochastic depth(0.1) used \n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Cutmix used\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Mixup used\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Repeated Aug(3) used\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Autoaugmentation used\n",
      "CIFAR Policy\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Random erasing(0.25) used \n",
      "********************************************************************************\u001b[0m\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,472\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]           4,160\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,928\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "           Conv2d-11            [-1, 256, 8, 8]          16,640\n",
      "      BatchNorm2d-12            [-1, 256, 8, 8]             512\n",
      "           Conv2d-13            [-1, 256, 8, 8]          16,640\n",
      "      BatchNorm2d-14            [-1, 256, 8, 8]             512\n",
      "             ReLU-15            [-1, 256, 8, 8]               0\n",
      "            Block-16            [-1, 256, 8, 8]               0\n",
      "           Conv2d-17             [-1, 64, 8, 8]          16,448\n",
      "      BatchNorm2d-18             [-1, 64, 8, 8]             128\n",
      "             ReLU-19             [-1, 64, 8, 8]               0\n",
      "           Conv2d-20             [-1, 64, 8, 8]          36,928\n",
      "      BatchNorm2d-21             [-1, 64, 8, 8]             128\n",
      "             ReLU-22             [-1, 64, 8, 8]               0\n",
      "           Conv2d-23            [-1, 256, 8, 8]          16,640\n",
      "      BatchNorm2d-24            [-1, 256, 8, 8]             512\n",
      "             ReLU-25            [-1, 256, 8, 8]               0\n",
      "            Block-26            [-1, 256, 8, 8]               0\n",
      "           Conv2d-27             [-1, 64, 8, 8]          16,448\n",
      "      BatchNorm2d-28             [-1, 64, 8, 8]             128\n",
      "             ReLU-29             [-1, 64, 8, 8]               0\n",
      "           Conv2d-30             [-1, 64, 8, 8]          36,928\n",
      "      BatchNorm2d-31             [-1, 64, 8, 8]             128\n",
      "             ReLU-32             [-1, 64, 8, 8]               0\n",
      "           Conv2d-33            [-1, 256, 8, 8]          16,640\n",
      "      BatchNorm2d-34            [-1, 256, 8, 8]             512\n",
      "             ReLU-35            [-1, 256, 8, 8]               0\n",
      "            Block-36            [-1, 256, 8, 8]               0\n",
      "           Conv2d-37            [-1, 128, 8, 8]          32,896\n",
      "      BatchNorm2d-38            [-1, 128, 8, 8]             256\n",
      "             ReLU-39            [-1, 128, 8, 8]               0\n",
      "           Conv2d-40            [-1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-41            [-1, 128, 4, 4]             256\n",
      "             ReLU-42            [-1, 128, 4, 4]               0\n",
      "           Conv2d-43            [-1, 512, 4, 4]          66,048\n",
      "      BatchNorm2d-44            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-45            [-1, 512, 4, 4]         131,584\n",
      "      BatchNorm2d-46            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-47            [-1, 512, 4, 4]               0\n",
      "            Block-48            [-1, 512, 4, 4]               0\n",
      "           Conv2d-49            [-1, 128, 4, 4]          65,664\n",
      "      BatchNorm2d-50            [-1, 128, 4, 4]             256\n",
      "             ReLU-51            [-1, 128, 4, 4]               0\n",
      "           Conv2d-52            [-1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-53            [-1, 128, 4, 4]             256\n",
      "             ReLU-54            [-1, 128, 4, 4]               0\n",
      "           Conv2d-55            [-1, 512, 4, 4]          66,048\n",
      "      BatchNorm2d-56            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-57            [-1, 512, 4, 4]               0\n",
      "            Block-58            [-1, 512, 4, 4]               0\n",
      "           Conv2d-59            [-1, 128, 4, 4]          65,664\n",
      "      BatchNorm2d-60            [-1, 128, 4, 4]             256\n",
      "             ReLU-61            [-1, 128, 4, 4]               0\n",
      "           Conv2d-62            [-1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-63            [-1, 128, 4, 4]             256\n",
      "             ReLU-64            [-1, 128, 4, 4]               0\n",
      "           Conv2d-65            [-1, 512, 4, 4]          66,048\n",
      "      BatchNorm2d-66            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-67            [-1, 512, 4, 4]               0\n",
      "            Block-68            [-1, 512, 4, 4]               0\n",
      "           Conv2d-69            [-1, 128, 4, 4]          65,664\n",
      "      BatchNorm2d-70            [-1, 128, 4, 4]             256\n",
      "             ReLU-71            [-1, 128, 4, 4]               0\n",
      "           Conv2d-72            [-1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-73            [-1, 128, 4, 4]             256\n",
      "             ReLU-74            [-1, 128, 4, 4]               0\n",
      "           Conv2d-75            [-1, 512, 4, 4]          66,048\n",
      "      BatchNorm2d-76            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-77            [-1, 512, 4, 4]               0\n",
      "            Block-78            [-1, 512, 4, 4]               0\n",
      "           Conv2d-79            [-1, 128, 4, 4]          65,664\n",
      "      BatchNorm2d-80            [-1, 128, 4, 4]             256\n",
      "             ReLU-81            [-1, 128, 4, 4]               0\n",
      "           Conv2d-82            [-1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-83            [-1, 128, 4, 4]             256\n",
      "             ReLU-84            [-1, 128, 4, 4]               0\n",
      "           Conv2d-85            [-1, 512, 4, 4]          66,048\n",
      "      BatchNorm2d-86            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-87            [-1, 512, 4, 4]               0\n",
      "            Block-88            [-1, 512, 4, 4]               0\n",
      "           Conv2d-89            [-1, 128, 4, 4]          65,664\n",
      "      BatchNorm2d-90            [-1, 128, 4, 4]             256\n",
      "             ReLU-91            [-1, 128, 4, 4]               0\n",
      "           Conv2d-92            [-1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-93            [-1, 128, 4, 4]             256\n",
      "             ReLU-94            [-1, 128, 4, 4]               0\n",
      "           Conv2d-95            [-1, 512, 4, 4]          66,048\n",
      "      BatchNorm2d-96            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-97            [-1, 512, 4, 4]               0\n",
      "            Block-98            [-1, 512, 4, 4]               0\n",
      "           Conv2d-99            [-1, 128, 4, 4]          65,664\n",
      "     BatchNorm2d-100            [-1, 128, 4, 4]             256\n",
      "            ReLU-101            [-1, 128, 4, 4]               0\n",
      "          Conv2d-102            [-1, 128, 4, 4]         147,584\n",
      "     BatchNorm2d-103            [-1, 128, 4, 4]             256\n",
      "            ReLU-104            [-1, 128, 4, 4]               0\n",
      "          Conv2d-105            [-1, 512, 4, 4]          66,048\n",
      "     BatchNorm2d-106            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-107            [-1, 512, 4, 4]               0\n",
      "           Block-108            [-1, 512, 4, 4]               0\n",
      "          Conv2d-109            [-1, 128, 4, 4]          65,664\n",
      "     BatchNorm2d-110            [-1, 128, 4, 4]             256\n",
      "            ReLU-111            [-1, 128, 4, 4]               0\n",
      "          Conv2d-112            [-1, 128, 4, 4]         147,584\n",
      "     BatchNorm2d-113            [-1, 128, 4, 4]             256\n",
      "            ReLU-114            [-1, 128, 4, 4]               0\n",
      "          Conv2d-115            [-1, 512, 4, 4]          66,048\n",
      "     BatchNorm2d-116            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-117            [-1, 512, 4, 4]               0\n",
      "           Block-118            [-1, 512, 4, 4]               0\n",
      "          Conv2d-119            [-1, 256, 4, 4]         131,328\n",
      "     BatchNorm2d-120            [-1, 256, 4, 4]             512\n",
      "            ReLU-121            [-1, 256, 4, 4]               0\n",
      "          Conv2d-122            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-123            [-1, 256, 2, 2]             512\n",
      "            ReLU-124            [-1, 256, 2, 2]               0\n",
      "          Conv2d-125           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-126           [-1, 1024, 2, 2]           2,048\n",
      "          Conv2d-127           [-1, 1024, 2, 2]         525,312\n",
      "     BatchNorm2d-128           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-129           [-1, 1024, 2, 2]               0\n",
      "           Block-130           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-131            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-132            [-1, 256, 2, 2]             512\n",
      "            ReLU-133            [-1, 256, 2, 2]               0\n",
      "          Conv2d-134            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-135            [-1, 256, 2, 2]             512\n",
      "            ReLU-136            [-1, 256, 2, 2]               0\n",
      "          Conv2d-137           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-138           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-139           [-1, 1024, 2, 2]               0\n",
      "           Block-140           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-141            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-142            [-1, 256, 2, 2]             512\n",
      "            ReLU-143            [-1, 256, 2, 2]               0\n",
      "          Conv2d-144            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-145            [-1, 256, 2, 2]             512\n",
      "            ReLU-146            [-1, 256, 2, 2]               0\n",
      "          Conv2d-147           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-148           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-149           [-1, 1024, 2, 2]               0\n",
      "           Block-150           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-151            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-152            [-1, 256, 2, 2]             512\n",
      "            ReLU-153            [-1, 256, 2, 2]               0\n",
      "          Conv2d-154            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-155            [-1, 256, 2, 2]             512\n",
      "            ReLU-156            [-1, 256, 2, 2]               0\n",
      "          Conv2d-157           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-158           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-159           [-1, 1024, 2, 2]               0\n",
      "           Block-160           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-161            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-162            [-1, 256, 2, 2]             512\n",
      "            ReLU-163            [-1, 256, 2, 2]               0\n",
      "          Conv2d-164            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-165            [-1, 256, 2, 2]             512\n",
      "            ReLU-166            [-1, 256, 2, 2]               0\n",
      "          Conv2d-167           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-168           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-169           [-1, 1024, 2, 2]               0\n",
      "           Block-170           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-171            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-172            [-1, 256, 2, 2]             512\n",
      "            ReLU-173            [-1, 256, 2, 2]               0\n",
      "          Conv2d-174            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-175            [-1, 256, 2, 2]             512\n",
      "            ReLU-176            [-1, 256, 2, 2]               0\n",
      "          Conv2d-177           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-178           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-179           [-1, 1024, 2, 2]               0\n",
      "           Block-180           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-181            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-182            [-1, 256, 2, 2]             512\n",
      "            ReLU-183            [-1, 256, 2, 2]               0\n",
      "          Conv2d-184            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-185            [-1, 256, 2, 2]             512\n",
      "            ReLU-186            [-1, 256, 2, 2]               0\n",
      "          Conv2d-187           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-188           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-189           [-1, 1024, 2, 2]               0\n",
      "           Block-190           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-191            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-192            [-1, 256, 2, 2]             512\n",
      "            ReLU-193            [-1, 256, 2, 2]               0\n",
      "          Conv2d-194            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-195            [-1, 256, 2, 2]             512\n",
      "            ReLU-196            [-1, 256, 2, 2]               0\n",
      "          Conv2d-197           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-198           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-199           [-1, 1024, 2, 2]               0\n",
      "           Block-200           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-201            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-202            [-1, 256, 2, 2]             512\n",
      "            ReLU-203            [-1, 256, 2, 2]               0\n",
      "          Conv2d-204            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-205            [-1, 256, 2, 2]             512\n",
      "            ReLU-206            [-1, 256, 2, 2]               0\n",
      "          Conv2d-207           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-208           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-209           [-1, 1024, 2, 2]               0\n",
      "           Block-210           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-211            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-212            [-1, 256, 2, 2]             512\n",
      "            ReLU-213            [-1, 256, 2, 2]               0\n",
      "          Conv2d-214            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-215            [-1, 256, 2, 2]             512\n",
      "            ReLU-216            [-1, 256, 2, 2]               0\n",
      "          Conv2d-217           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-218           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-219           [-1, 1024, 2, 2]               0\n",
      "           Block-220           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-221            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-222            [-1, 256, 2, 2]             512\n",
      "            ReLU-223            [-1, 256, 2, 2]               0\n",
      "          Conv2d-224            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-225            [-1, 256, 2, 2]             512\n",
      "            ReLU-226            [-1, 256, 2, 2]               0\n",
      "          Conv2d-227           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-228           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-229           [-1, 1024, 2, 2]               0\n",
      "           Block-230           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-231            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-232            [-1, 256, 2, 2]             512\n",
      "            ReLU-233            [-1, 256, 2, 2]               0\n",
      "          Conv2d-234            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-235            [-1, 256, 2, 2]             512\n",
      "            ReLU-236            [-1, 256, 2, 2]               0\n",
      "          Conv2d-237           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-238           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-239           [-1, 1024, 2, 2]               0\n",
      "           Block-240           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-241            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-242            [-1, 256, 2, 2]             512\n",
      "            ReLU-243            [-1, 256, 2, 2]               0\n",
      "          Conv2d-244            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-245            [-1, 256, 2, 2]             512\n",
      "            ReLU-246            [-1, 256, 2, 2]               0\n",
      "          Conv2d-247           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-248           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-249           [-1, 1024, 2, 2]               0\n",
      "           Block-250           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-251            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-252            [-1, 256, 2, 2]             512\n",
      "            ReLU-253            [-1, 256, 2, 2]               0\n",
      "          Conv2d-254            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-255            [-1, 256, 2, 2]             512\n",
      "            ReLU-256            [-1, 256, 2, 2]               0\n",
      "          Conv2d-257           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-258           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-259           [-1, 1024, 2, 2]               0\n",
      "           Block-260           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-261            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-262            [-1, 256, 2, 2]             512\n",
      "            ReLU-263            [-1, 256, 2, 2]               0\n",
      "          Conv2d-264            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-265            [-1, 256, 2, 2]             512\n",
      "            ReLU-266            [-1, 256, 2, 2]               0\n",
      "          Conv2d-267           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-268           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-269           [-1, 1024, 2, 2]               0\n",
      "           Block-270           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-271            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-272            [-1, 256, 2, 2]             512\n",
      "            ReLU-273            [-1, 256, 2, 2]               0\n",
      "          Conv2d-274            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-275            [-1, 256, 2, 2]             512\n",
      "            ReLU-276            [-1, 256, 2, 2]               0\n",
      "          Conv2d-277           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-278           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-279           [-1, 1024, 2, 2]               0\n",
      "           Block-280           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-281            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-282            [-1, 256, 2, 2]             512\n",
      "            ReLU-283            [-1, 256, 2, 2]               0\n",
      "          Conv2d-284            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-285            [-1, 256, 2, 2]             512\n",
      "            ReLU-286            [-1, 256, 2, 2]               0\n",
      "          Conv2d-287           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-288           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-289           [-1, 1024, 2, 2]               0\n",
      "           Block-290           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-291            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-292            [-1, 256, 2, 2]             512\n",
      "            ReLU-293            [-1, 256, 2, 2]               0\n",
      "          Conv2d-294            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-295            [-1, 256, 2, 2]             512\n",
      "            ReLU-296            [-1, 256, 2, 2]               0\n",
      "          Conv2d-297           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-298           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-299           [-1, 1024, 2, 2]               0\n",
      "           Block-300           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-301            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-302            [-1, 256, 2, 2]             512\n",
      "            ReLU-303            [-1, 256, 2, 2]               0\n",
      "          Conv2d-304            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-305            [-1, 256, 2, 2]             512\n",
      "            ReLU-306            [-1, 256, 2, 2]               0\n",
      "          Conv2d-307           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-308           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-309           [-1, 1024, 2, 2]               0\n",
      "           Block-310           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-311            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-312            [-1, 256, 2, 2]             512\n",
      "            ReLU-313            [-1, 256, 2, 2]               0\n",
      "          Conv2d-314            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-315            [-1, 256, 2, 2]             512\n",
      "            ReLU-316            [-1, 256, 2, 2]               0\n",
      "          Conv2d-317           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-318           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-319           [-1, 1024, 2, 2]               0\n",
      "           Block-320           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-321            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-322            [-1, 256, 2, 2]             512\n",
      "            ReLU-323            [-1, 256, 2, 2]               0\n",
      "          Conv2d-324            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-325            [-1, 256, 2, 2]             512\n",
      "            ReLU-326            [-1, 256, 2, 2]               0\n",
      "          Conv2d-327           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-328           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-329           [-1, 1024, 2, 2]               0\n",
      "           Block-330           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-331            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-332            [-1, 256, 2, 2]             512\n",
      "            ReLU-333            [-1, 256, 2, 2]               0\n",
      "          Conv2d-334            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-335            [-1, 256, 2, 2]             512\n",
      "            ReLU-336            [-1, 256, 2, 2]               0\n",
      "          Conv2d-337           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-338           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-339           [-1, 1024, 2, 2]               0\n",
      "           Block-340           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-341            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-342            [-1, 256, 2, 2]             512\n",
      "            ReLU-343            [-1, 256, 2, 2]               0\n",
      "          Conv2d-344            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-345            [-1, 256, 2, 2]             512\n",
      "            ReLU-346            [-1, 256, 2, 2]               0\n",
      "          Conv2d-347           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-348           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-349           [-1, 1024, 2, 2]               0\n",
      "           Block-350           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-351            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-352            [-1, 256, 2, 2]             512\n",
      "            ReLU-353            [-1, 256, 2, 2]               0\n",
      "          Conv2d-354            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-355            [-1, 256, 2, 2]             512\n",
      "            ReLU-356            [-1, 256, 2, 2]               0\n",
      "          Conv2d-357           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-358           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-359           [-1, 1024, 2, 2]               0\n",
      "           Block-360           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-361            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-362            [-1, 256, 2, 2]             512\n",
      "            ReLU-363            [-1, 256, 2, 2]               0\n",
      "          Conv2d-364            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-365            [-1, 256, 2, 2]             512\n",
      "            ReLU-366            [-1, 256, 2, 2]               0\n",
      "          Conv2d-367           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-368           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-369           [-1, 1024, 2, 2]               0\n",
      "           Block-370           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-371            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-372            [-1, 256, 2, 2]             512\n",
      "            ReLU-373            [-1, 256, 2, 2]               0\n",
      "          Conv2d-374            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-375            [-1, 256, 2, 2]             512\n",
      "            ReLU-376            [-1, 256, 2, 2]               0\n",
      "          Conv2d-377           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-378           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-379           [-1, 1024, 2, 2]               0\n",
      "           Block-380           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-381            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-382            [-1, 256, 2, 2]             512\n",
      "            ReLU-383            [-1, 256, 2, 2]               0\n",
      "          Conv2d-384            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-385            [-1, 256, 2, 2]             512\n",
      "            ReLU-386            [-1, 256, 2, 2]               0\n",
      "          Conv2d-387           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-388           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-389           [-1, 1024, 2, 2]               0\n",
      "           Block-390           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-391            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-392            [-1, 256, 2, 2]             512\n",
      "            ReLU-393            [-1, 256, 2, 2]               0\n",
      "          Conv2d-394            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-395            [-1, 256, 2, 2]             512\n",
      "            ReLU-396            [-1, 256, 2, 2]               0\n",
      "          Conv2d-397           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-398           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-399           [-1, 1024, 2, 2]               0\n",
      "           Block-400           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-401            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-402            [-1, 256, 2, 2]             512\n",
      "            ReLU-403            [-1, 256, 2, 2]               0\n",
      "          Conv2d-404            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-405            [-1, 256, 2, 2]             512\n",
      "            ReLU-406            [-1, 256, 2, 2]               0\n",
      "          Conv2d-407           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-408           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-409           [-1, 1024, 2, 2]               0\n",
      "           Block-410           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-411            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-412            [-1, 256, 2, 2]             512\n",
      "            ReLU-413            [-1, 256, 2, 2]               0\n",
      "          Conv2d-414            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-415            [-1, 256, 2, 2]             512\n",
      "            ReLU-416            [-1, 256, 2, 2]               0\n",
      "          Conv2d-417           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-418           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-419           [-1, 1024, 2, 2]               0\n",
      "           Block-420           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-421            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-422            [-1, 256, 2, 2]             512\n",
      "            ReLU-423            [-1, 256, 2, 2]               0\n",
      "          Conv2d-424            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-425            [-1, 256, 2, 2]             512\n",
      "            ReLU-426            [-1, 256, 2, 2]               0\n",
      "          Conv2d-427           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-428           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-429           [-1, 1024, 2, 2]               0\n",
      "           Block-430           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-431            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-432            [-1, 256, 2, 2]             512\n",
      "            ReLU-433            [-1, 256, 2, 2]               0\n",
      "          Conv2d-434            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-435            [-1, 256, 2, 2]             512\n",
      "            ReLU-436            [-1, 256, 2, 2]               0\n",
      "          Conv2d-437           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-438           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-439           [-1, 1024, 2, 2]               0\n",
      "           Block-440           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-441            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-442            [-1, 256, 2, 2]             512\n",
      "            ReLU-443            [-1, 256, 2, 2]               0\n",
      "          Conv2d-444            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-445            [-1, 256, 2, 2]             512\n",
      "            ReLU-446            [-1, 256, 2, 2]               0\n",
      "          Conv2d-447           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-448           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-449           [-1, 1024, 2, 2]               0\n",
      "           Block-450           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-451            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-452            [-1, 256, 2, 2]             512\n",
      "            ReLU-453            [-1, 256, 2, 2]               0\n",
      "          Conv2d-454            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-455            [-1, 256, 2, 2]             512\n",
      "            ReLU-456            [-1, 256, 2, 2]               0\n",
      "          Conv2d-457           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-458           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-459           [-1, 1024, 2, 2]               0\n",
      "           Block-460           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-461            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-462            [-1, 256, 2, 2]             512\n",
      "            ReLU-463            [-1, 256, 2, 2]               0\n",
      "          Conv2d-464            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-465            [-1, 256, 2, 2]             512\n",
      "            ReLU-466            [-1, 256, 2, 2]               0\n",
      "          Conv2d-467           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-468           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-469           [-1, 1024, 2, 2]               0\n",
      "           Block-470           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-471            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-472            [-1, 256, 2, 2]             512\n",
      "            ReLU-473            [-1, 256, 2, 2]               0\n",
      "          Conv2d-474            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-475            [-1, 256, 2, 2]             512\n",
      "            ReLU-476            [-1, 256, 2, 2]               0\n",
      "          Conv2d-477           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-478           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-479           [-1, 1024, 2, 2]               0\n",
      "           Block-480           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-481            [-1, 512, 2, 2]         524,800\n",
      "     BatchNorm2d-482            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-483            [-1, 512, 2, 2]               0\n",
      "          Conv2d-484            [-1, 512, 1, 1]       2,359,808\n",
      "     BatchNorm2d-485            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-486            [-1, 512, 1, 1]               0\n",
      "          Conv2d-487           [-1, 2048, 1, 1]       1,050,624\n",
      "     BatchNorm2d-488           [-1, 2048, 1, 1]           4,096\n",
      "          Conv2d-489           [-1, 2048, 1, 1]       2,099,200\n",
      "     BatchNorm2d-490           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-491           [-1, 2048, 1, 1]               0\n",
      "           Block-492           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-493            [-1, 512, 1, 1]       1,049,088\n",
      "     BatchNorm2d-494            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-495            [-1, 512, 1, 1]               0\n",
      "          Conv2d-496            [-1, 512, 1, 1]       2,359,808\n",
      "     BatchNorm2d-497            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-498            [-1, 512, 1, 1]               0\n",
      "          Conv2d-499           [-1, 2048, 1, 1]       1,050,624\n",
      "     BatchNorm2d-500           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-501           [-1, 2048, 1, 1]               0\n",
      "           Block-502           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-503            [-1, 512, 1, 1]       1,049,088\n",
      "     BatchNorm2d-504            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-505            [-1, 512, 1, 1]               0\n",
      "          Conv2d-506            [-1, 512, 1, 1]       2,359,808\n",
      "     BatchNorm2d-507            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-508            [-1, 512, 1, 1]               0\n",
      "          Conv2d-509           [-1, 2048, 1, 1]       1,050,624\n",
      "     BatchNorm2d-510           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-511           [-1, 2048, 1, 1]               0\n",
      "           Block-512           [-1, 2048, 1, 1]               0\n",
      "AdaptiveAvgPool2d-513           [-1, 2048, 1, 1]               0\n",
      "          Linear-514                   [-1, 10]          20,490\n",
      "================================================================\n",
      "Total params: 58,240,010\n",
      "Trainable params: 58,240,010\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 12.39\n",
      "Params size (MB): 222.17\n",
      "Estimated Total Size (MB): 234.57\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Beginning training\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/100][T][1170]   Loss: 2.3627e+00   Top-1:  11.32   LR: 0.0001009        Epoch 1/100][T][0]   Loss: 2.5571e+00   Top-1:  14.06   LR: 0.0000011        \n",
      "[Epoch 1][V][78]   Loss: 2.2436e+00   Top-1:  17.15   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.362674827355793\n",
      "T Top-1\t11.318451110162254\n",
      "V Loss\t2.2435647583007814\n",
      "V Top-1\t17.15\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 17.15\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 2/100][T][1170]   Loss: 2.2694e+00   Top-1:  14.98   LR: 0.0002008        Epoch 2/100][T][0]   Loss: 2.3130e+00   Top-1:  14.06   LR: 0.0001010        \n",
      "[Epoch 2][V][78]   Loss: 2.0686e+00   Top-1:  26.33   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.2694201251566666\n",
      "T Top-1\t14.983187446626815\n",
      "V Loss\t2.068569027328491\n",
      "V Top-1\t26.33\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 26.33\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 3/100][T][1170]   Loss: 2.2091e+00   Top-1:  19.23   LR: 0.0003007        Epoch 3/100][T][0]   Loss: 2.2553e+00   Top-1:  13.28   LR: 0.0002009        \n",
      "[Epoch 3][V][78]   Loss: 1.9534e+00   Top-1:  34.07   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.209070778015228\n",
      "T Top-1\t19.228357173356105\n",
      "V Loss\t1.9534410598754883\n",
      "V Top-1\t34.07\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 34.07\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 4/100][T][1170]   Loss: 2.1513e+00   Top-1:  23.83   LR: 0.0004006        Epoch 4/100][T][0]   Loss: 2.2578e+00   Top-1:  27.34   LR: 0.0003008        \n",
      "[Epoch 4][V][78]   Loss: 1.7779e+00   Top-1:  40.01   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.1513268508837835\n",
      "T Top-1\t23.82645708795901\n",
      "V Loss\t1.7779292251586913\n",
      "V Top-1\t40.01\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 40.01\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 5/100][T][1170]   Loss: 2.1097e+00   Top-1:  26.12   LR: 0.0005005        Epoch 5/100][T][0]   Loss: 2.1531e+00   Top-1:  26.56   LR: 0.0004007        \n",
      "[Epoch 5][V][78]   Loss: 1.9257e+00   Top-1:  42.83   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.109744320660957\n",
      "T Top-1\t26.1181682322801\n",
      "V Loss\t1.9257490058898925\n",
      "V Top-1\t42.83\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 42.83\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 6/100][T][1170]   Loss: 2.1540e+00   Top-1:  24.49   LR: 0.0006004        Epoch 6/100][T][0]   Loss: 1.9290e+00   Top-1:  35.94   LR: 0.0005006        \n",
      "[Epoch 6][V][78]   Loss: 2.1340e+00   Top-1:  25.48   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.154042999730163\n",
      "T Top-1\t24.494956233988045\n",
      "V Loss\t2.1339608840942383\n",
      "V Top-1\t25.48\n",
      "\n",
      "Best acc1 42.83\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 7/100][T][1170]   Loss: 2.2291e+00   Top-1:  20.13   LR: 0.0007003        Epoch 7/100][T][0]   Loss: 2.2838e+00   Top-1:  16.41   LR: 0.0006005        \n",
      "[Epoch 7][V][78]   Loss: 2.2298e+00   Top-1:  34.86   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.2290620664365264\n",
      "T Top-1\t20.131698334756617\n",
      "V Loss\t2.229782966995239\n",
      "V Top-1\t34.86\n",
      "\n",
      "Best acc1 42.83\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 8/100][T][1170]   Loss: 2.2349e+00   Top-1:  19.53   LR: 0.0008002        Epoch 8/100][T][0]   Loss: 2.0661e+00   Top-1:  31.25   LR: 0.0007004        \n",
      "[Epoch 8][V][78]   Loss: 2.2817e+00   Top-1:  21.37   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.2349148476663356\n",
      "T Top-1\t19.525245516652433\n",
      "V Loss\t2.2817416370391848\n",
      "V Top-1\t21.37\n",
      "\n",
      "Best acc1 42.83\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 9/100][T][1170]   Loss: 2.2793e+00   Top-1:  16.57   LR: 0.0009001        Epoch 9/100][T][0]   Loss: 2.2710e+00   Top-1:  15.62   LR: 0.0008003        \n",
      "[Epoch 9][V][78]   Loss: 3.4537e+00   Top-1:  28.62   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.279265677185775\n",
      "T Top-1\t16.570372544833475\n",
      "V Loss\t3.453653303718567\n",
      "V Top-1\t28.62\n",
      "\n",
      "Best acc1 42.83\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 10/100][T][1170]   Loss: 2.3096e+00   Top-1:  14.07   LR: 0.0010000        poch 10/100][T][0]   Loss: 2.2140e+00   Top-1:  16.41   LR: 0.0009002        \n",
      "[Epoch 10][V][78]   Loss: 4.1985e+00   Top-1:  21.96   LR: 0.0010\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.309572309026547\n",
      "T Top-1\t14.065835824081981\n",
      "V Loss\t4.198499133300781\n",
      "V Top-1\t21.96\n",
      "\n",
      "Best acc1 42.83\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 11/100][T][1170]   Loss: 2.2291e+00   Top-1:  18.13   LR: 0.0009997        poch 11/100][T][0]   Loss: 2.2703e+00   Top-1:  14.84   LR: 0.0010000        \n",
      "[Epoch 11][V][78]   Loss: 2.6302e+00   Top-1:  30.82   LR: 0.0010\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.2290995281848205\n",
      "T Top-1\t18.130871050384286\n",
      "V Loss\t2.6302184955596926\n",
      "V Top-1\t30.82\n",
      "\n",
      "Best acc1 42.83\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 12/100][T][1170]   Loss: 2.1410e+00   Top-1:  22.08   LR: 0.0009988        poch 12/100][T][0]   Loss: 2.0895e+00   Top-1:  24.22   LR: 0.0009997        \n",
      "[Epoch 12][V][78]   Loss: 1.8772e+00   Top-1:  36.63   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.1409593608825253\n",
      "T Top-1\t22.08115392826644\n",
      "V Loss\t1.8771645782470703\n",
      "V Top-1\t36.63\n",
      "\n",
      "Best acc1 42.83\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 13/100][T][1170]   Loss: 2.0218e+00   Top-1:  27.97   LR: 0.0009973        poch 13/100][T][0]   Loss: 2.0304e+00   Top-1:  28.12   LR: 0.0009988        \n",
      "[Epoch 13][V][78]   Loss: 1.6221e+00   Top-1:  48.12   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.021765575869077\n",
      "T Top-1\t27.97021776259607\n",
      "V Loss\t1.6221358451843262\n",
      "V Top-1\t48.12\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 48.12\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 14/100][T][1170]   Loss: 1.9330e+00   Top-1:  33.25   LR: 0.0009951        poch 14/100][T][0]   Loss: 1.8866e+00   Top-1:  37.50   LR: 0.0009973        \n",
      "[Epoch 14][V][78]   Loss: 1.5909e+00   Top-1:  54.37   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.9329594908159682\n",
      "T Top-1\t33.24749146029035\n",
      "V Loss\t1.590940613555908\n",
      "V Top-1\t54.37\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 54.37\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 15/100][T][1170]   Loss: 1.8735e+00   Top-1:  36.02   LR: 0.0009924        poch 15/100][T][0]   Loss: 1.6162e+00   Top-1:  46.09   LR: 0.0009951        \n",
      "[Epoch 15][V][78]   Loss: 1.4415e+00   Top-1:  57.07   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.8734587209027207\n",
      "T Top-1\t36.020228437233136\n",
      "V Loss\t1.4415289974212646\n",
      "V Top-1\t57.07\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 57.07\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 16/100][T][1170]   Loss: 1.8092e+00   Top-1:  39.52   LR: 0.0009891        poch 16/100][T][0]   Loss: 2.0950e+00   Top-1:  28.12   LR: 0.0009924        \n",
      "[Epoch 16][V][78]   Loss: 1.3510e+00   Top-1:  62.25   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.8091568788028394\n",
      "T Top-1\t39.523510888129806\n",
      "V Loss\t1.351019655609131\n",
      "V Top-1\t62.25\n",
      "\n",
      "* Best model upate *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best acc1 62.25\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 17/100][T][1170]   Loss: 1.7742e+00   Top-1:  41.58   LR: 0.0009852        poch 17/100][T][0]   Loss: 1.4623e+00   Top-1:  58.59   LR: 0.0009891        \n",
      "[Epoch 17][V][78]   Loss: 1.3822e+00   Top-1:  62.88   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.774179898705063\n",
      "T Top-1\t41.57504269854825\n",
      "V Loss\t1.3822473230361938\n",
      "V Top-1\t62.88\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 62.88\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 18/100][T][1170]   Loss: 1.8239e+00   Top-1:  39.62   LR: 0.0009807        poch 18/100][T][0]   Loss: 1.9884e+00   Top-1:  43.75   LR: 0.0009852        \n",
      "[Epoch 18][V][78]   Loss: 3.6387e+00   Top-1:  51.82   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.8239090924177488\n",
      "T Top-1\t39.62225128095645\n",
      "V Loss\t3.6386564582824708\n",
      "V Top-1\t51.82\n",
      "\n",
      "Best acc1 62.88\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 19/100][T][1170]   Loss: 1.7316e+00   Top-1:  43.62   LR: 0.0009756        poch 19/100][T][0]   Loss: 2.1189e+00   Top-1:  17.97   LR: 0.0009806        \n",
      "[Epoch 19][V][78]   Loss: 1.2174e+00   Top-1:  68.24   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.7315614099282672\n",
      "T Top-1\t43.61523270708796\n",
      "V Loss\t1.217385750579834\n",
      "V Top-1\t68.24\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 68.24\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 20/100][T][1170]   Loss: 1.6575e+00   Top-1:  46.63   LR: 0.0009699        poch 20/100][T][0]   Loss: 1.4224e+00   Top-1:  57.81   LR: 0.0009755        \n",
      "[Epoch 20][V][78]   Loss: 1.1561e+00   Top-1:  71.49   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.6575335585897952\n",
      "T Top-1\t46.634820666097355\n",
      "V Loss\t1.1560926723480225\n",
      "V Top-1\t71.49\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 71.49\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 21/100][T][1170]   Loss: 1.6097e+00   Top-1:  49.93   LR: 0.0009636        poch 21/100][T][0]   Loss: 2.0811e+00   Top-1:  21.88   LR: 0.0009699        \n",
      "[Epoch 21][V][78]   Loss: 1.1990e+00   Top-1:  73.35   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.6096535223758088\n",
      "T Top-1\t49.93128202391119\n",
      "V Loss\t1.1989553400039672\n",
      "V Top-1\t73.35\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 73.35\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 22/100][T][1170]   Loss: 1.6113e+00   Top-1:  49.89   LR: 0.0009568        poch 22/100][T][0]   Loss: 1.3591e+00   Top-1:  59.38   LR: 0.0009636        \n",
      "[Epoch 22][V][78]   Loss: 1.6294e+00   Top-1:  49.26   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.6112555515857765\n",
      "T Top-1\t49.889250640478224\n",
      "V Loss\t1.6293780668258666\n",
      "V Top-1\t49.26\n",
      "\n",
      "Best acc1 73.35\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 23/100][T][1170]   Loss: 1.5981e+00   Top-1:  49.75   LR: 0.0009494        poch 23/100][T][0]   Loss: 1.8010e+00   Top-1:  45.31   LR: 0.0009568        \n",
      "[Epoch 23][V][78]   Loss: 1.0558e+00   Top-1:  76.12   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.5980544051586514\n",
      "T Top-1\t49.75048035866781\n",
      "V Loss\t1.055819623184204\n",
      "V Top-1\t76.12\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 76.12\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 24/100][T][1170]   Loss: 1.5431e+00   Top-1:  51.89   LR: 0.0009415        poch 24/100][T][0]   Loss: 1.2492e+00   Top-1:  68.75   LR: 0.0009494        \n",
      "[Epoch 24][V][78]   Loss: 1.0269e+00   Top-1:  77.54   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.5430639481361252\n",
      "T Top-1\t51.89074508966695\n",
      "V Loss\t1.0269382501602173\n",
      "V Top-1\t77.54\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 77.54\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 25/100][T][1170]   Loss: 1.5108e+00   Top-1:  53.45   LR: 0.0009331        poch 25/100][T][0]   Loss: 1.7815e+00   Top-1:  60.16   LR: 0.0009415        \n",
      "[Epoch 25][V][78]   Loss: 1.0219e+00   Top-1:  77.57   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.5107558909692285\n",
      "T Top-1\t53.44990926558497\n",
      "V Loss\t1.0218797561645507\n",
      "V Top-1\t77.57\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 77.57\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 26/100][T][1170]   Loss: 1.4846e+00   Top-1:  54.46   LR: 0.0009241        poch 26/100][T][0]   Loss: 1.1709e+00   Top-1:  67.97   LR: 0.0009331        \n",
      "[Epoch 26][V][78]   Loss: 1.0029e+00   Top-1:  78.46   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.4845957170685167\n",
      "T Top-1\t54.45732813834329\n",
      "V Loss\t1.0028762020111084\n",
      "V Top-1\t78.46\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 78.46\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 27/100][T][1170]   Loss: 1.4782e+00   Top-1:  54.78   LR: 0.0009146        poch 27/100][T][0]   Loss: 1.2557e+00   Top-1:  64.84   LR: 0.0009241        \n",
      "[Epoch 27][V][78]   Loss: 1.0129e+00   Top-1:  78.30   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.478233832970936\n",
      "T Top-1\t54.778234415029885\n",
      "V Loss\t1.0128909587860107\n",
      "V Top-1\t78.3\n",
      "\n",
      "Best acc1 78.46\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 28/100][T][1170]   Loss: 1.4602e+00   Top-1:  55.25   LR: 0.0009046        poch 28/100][T][0]   Loss: 1.9211e+00   Top-1:  50.00   LR: 0.0009146        \n",
      "[Epoch 28][V][78]   Loss: 9.5219e-01   Top-1:  80.33   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.4601615840914308\n",
      "T Top-1\t55.249919940222036\n",
      "V Loss\t0.9521867893218994\n",
      "V Top-1\t80.33\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 80.33\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 29/100][T][1170]   Loss: 1.4473e+00   Top-1:  55.98   LR: 0.0008941        poch 29/100][T][0]   Loss: 1.1019e+00   Top-1:  74.22   LR: 0.0009046        \n",
      "[Epoch 29][V][78]   Loss: 9.4997e-01   Top-1:  80.31   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.4472613141233346\n",
      "T Top-1\t55.982466908625106\n",
      "V Loss\t0.9499710037231446\n",
      "V Top-1\t80.31\n",
      "\n",
      "Best acc1 80.33\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 30/100][T][1170]   Loss: 1.4157e+00   Top-1:  57.38   LR: 0.0008831        poch 30/100][T][0]   Loss: 1.8209e+00   Top-1:  15.62   LR: 0.0008941        \n",
      "[Epoch 30][V][78]   Loss: 9.2356e-01   Top-1:  82.02   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.4156811793679156\n",
      "T Top-1\t57.38151152860803\n",
      "V Loss\t0.9235586263656617\n",
      "V Top-1\t82.02\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 82.02\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 31/100][T][1170]   Loss: 1.4157e+00   Top-1:  57.74   LR: 0.0008717        poch 31/100][T][0]   Loss: 1.8998e+00   Top-1:  55.47   LR: 0.0008831        \n",
      "[Epoch 31][V][78]   Loss: 9.6379e-01   Top-1:  80.49   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.415710773812106\n",
      "T Top-1\t57.7411133646456\n",
      "V Loss\t0.9637920183181763\n",
      "V Top-1\t80.49\n",
      "\n",
      "Best acc1 82.02\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 32/100][T][1170]   Loss: 1.4241e+00   Top-1:  56.98   LR: 0.0008598        poch 32/100][T][0]   Loss: 1.5750e+00   Top-1:  63.28   LR: 0.0008717        \n",
      "[Epoch 32][V][78]   Loss: 9.3874e-01   Top-1:  80.67   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T Loss\t1.4241183131907762\n",
      "T Top-1\t56.9772096498719\n",
      "V Loss\t0.9387430198669433\n",
      "V Top-1\t80.67\n",
      "\n",
      "Best acc1 82.02\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 33/100][T][1170]   Loss: 1.3909e+00   Top-1:  58.73   LR: 0.0008475        poch 33/100][T][0]   Loss: 1.9253e+00   Top-1:  47.66   LR: 0.0008598        \n",
      "[Epoch 33][V][78]   Loss: 8.8530e-01   Top-1:  83.07   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3909026408785943\n",
      "T Top-1\t58.733187446626815\n",
      "V Loss\t0.8853021242141723\n",
      "V Top-1\t83.07\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 83.07\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 34/100][T][1170]   Loss: 1.3760e+00   Top-1:  59.75   LR: 0.0008347        poch 34/100][T][0]   Loss: 1.0265e+00   Top-1:  75.78   LR: 0.0008475        \n",
      "[Epoch 34][V][78]   Loss: 9.0455e-01   Top-1:  82.75   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3760276255599462\n",
      "T Top-1\t59.74861229718189\n",
      "V Loss\t0.9045515462875366\n",
      "V Top-1\t82.75\n",
      "\n",
      "Best acc1 83.07\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 35/100][T][1170]   Loss: 1.3561e+00   Top-1:  60.84   LR: 0.0008216        poch 35/100][T][0]   Loss: 1.8963e+00   Top-1:  50.78   LR: 0.0008347        \n",
      "[Epoch 35][V][78]   Loss: 8.8045e-01   Top-1:  83.66   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3561022707176045\n",
      "T Top-1\t60.83542378309137\n",
      "V Loss\t0.8804539970397949\n",
      "V Top-1\t83.66\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 83.66\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 36/100][T][1170]   Loss: 1.3413e+00   Top-1:  60.73   LR: 0.0008080        poch 36/100][T][0]   Loss: 9.8118e-01   Top-1:  78.12   LR: 0.0008216        \n",
      "[Epoch 36][V][78]   Loss: 8.9634e-01   Top-1:  82.80   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3412952799251403\n",
      "T Top-1\t60.73401473099915\n",
      "V Loss\t0.8963382923126221\n",
      "V Top-1\t82.8\n",
      "\n",
      "Best acc1 83.66\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 37/100][T][1170]   Loss: 1.3425e+00   Top-1:  61.07   LR: 0.0007941        poch 37/100][T][0]   Loss: 1.6686e+00   Top-1:  14.06   LR: 0.0008080        \n",
      "[Epoch 37][V][78]   Loss: 8.6130e-01   Top-1:  84.18   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3424804593435624\n",
      "T Top-1\t61.071600128095646\n",
      "V Loss\t0.861296861076355\n",
      "V Top-1\t84.18\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 84.18\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 38/100][T][1170]   Loss: 1.3575e+00   Top-1:  60.04   LR: 0.0007798        poch 38/100][T][0]   Loss: 1.4536e+00   Top-1:  66.41   LR: 0.0007941        \n",
      "[Epoch 38][V][78]   Loss: 8.6271e-01   Top-1:  84.56   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3574625386019021\n",
      "T Top-1\t60.042164816396244\n",
      "V Loss\t0.8627084781646729\n",
      "V Top-1\t84.56\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 84.56\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 39/100][T][1170]   Loss: 1.3275e+00   Top-1:  61.08   LR: 0.0007652        poch 39/100][T][0]   Loss: 1.3104e+00   Top-1:  75.00   LR: 0.0007798        \n",
      "[Epoch 39][V][78]   Loss: 9.0204e-01   Top-1:  84.25   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3274761646044346\n",
      "T Top-1\t61.08227476515798\n",
      "V Loss\t0.9020414350509643\n",
      "V Top-1\t84.25\n",
      "\n",
      "Best acc1 84.56\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 40/100][T][1170]   Loss: 1.3257e+00   Top-1:  62.23   LR: 0.0007503        poch 40/100][T][0]   Loss: 1.8892e+00   Top-1:  35.94   LR: 0.0007652        \n",
      "[Epoch 40][V][78]   Loss: 8.6535e-01   Top-1:  84.04   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.325700869578566\n",
      "T Top-1\t62.22512809564475\n",
      "V Loss\t0.8653453549385071\n",
      "V Top-1\t84.04\n",
      "\n",
      "Best acc1 84.56\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 41/100][T][1170]   Loss: 1.3393e+00   Top-1:  61.39   LR: 0.0007350        poch 41/100][T][0]   Loss: 1.9408e+00   Top-1:  32.81   LR: 0.0007502        \n",
      "[Epoch 41][V][78]   Loss: 8.4859e-01   Top-1:  84.70   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.3392863432939597\n",
      "T Top-1\t61.38583475661827\n",
      "V Loss\t0.8485895433425903\n",
      "V Top-1\t84.7\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 84.70\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 42/100][T][1170]   Loss: 1.3137e+00   Top-1:  61.55   LR: 0.0007195        poch 42/100][T][0]   Loss: 1.4222e+00   Top-1:  21.88   LR: 0.0007350        \n",
      "[Epoch 42][V][78]   Loss: 8.5704e-01   Top-1:  85.09   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.313660322449128\n",
      "T Top-1\t61.54795580700256\n",
      "V Loss\t0.8570357870101929\n",
      "V Top-1\t85.09\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 85.09\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 43/100][T][1170]   Loss: 1.3133e+00   Top-1:  60.71   LR: 0.0007037        poch 43/100][T][0]   Loss: 1.7156e+00   Top-1:  17.19   LR: 0.0007195        \n",
      "[Epoch 43][V][78]   Loss: 8.3112e-01   Top-1:  86.06   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.313276281069732\n",
      "T Top-1\t60.71466695132366\n",
      "V Loss\t0.8311243934631347\n",
      "V Top-1\t86.06\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 86.06\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 44/100][T][1170]   Loss: 1.2976e+00   Top-1:  62.02   LR: 0.0006876        poch 44/100][T][0]   Loss: 1.9323e+00   Top-1:  36.72   LR: 0.0007037        \n",
      "[Epoch 44][V][78]   Loss: 8.1872e-01   Top-1:  86.08   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.297590285615978\n",
      "T Top-1\t62.01630550811272\n",
      "V Loss\t0.8187173274040223\n",
      "V Top-1\t86.08\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 86.08\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 45/100][T][1170]   Loss: 1.2876e+00   Top-1:  63.38   LR: 0.0006713        poch 45/100][T][0]   Loss: 1.5030e+00   Top-1:  74.22   LR: 0.0006876        \n",
      "[Epoch 45][V][78]   Loss: 8.2276e-01   Top-1:  86.28   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2876497070876887\n",
      "T Top-1\t63.375987403928264\n",
      "V Loss\t0.8227616161346436\n",
      "V Top-1\t86.28\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 86.28\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 46/100][T][1170]   Loss: 1.2881e+00   Top-1:  62.70   LR: 0.0006549        poch 46/100][T][0]   Loss: 1.0795e+00   Top-1:  73.44   LR: 0.0006713        \n",
      "[Epoch 46][V][78]   Loss: 8.2863e-01   Top-1:  86.44   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.288061136266902\n",
      "T Top-1\t62.70281810418446\n",
      "V Loss\t0.8286268033981323\n",
      "V Top-1\t86.44\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 86.44\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 47/100][T][1170]   Loss: 1.2773e+00   Top-1:  64.20   LR: 0.0006382        poch 47/100][T][0]   Loss: 9.2492e-01   Top-1:  81.25   LR: 0.0006548        \n",
      "[Epoch 47][V][78]   Loss: 8.1621e-01   Top-1:  86.64   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.277320680050235\n",
      "T Top-1\t64.19660012809564\n",
      "V Loss\t0.8162075853347779\n",
      "V Top-1\t86.64\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 86.64\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 48/100][T][1170]   Loss: 1.2789e+00   Top-1:  64.01   LR: 0.0006213        poch 48/100][T][0]   Loss: 9.7852e-01   Top-1:  78.91   LR: 0.0006382        \n",
      "[Epoch 48][V][78]   Loss: 8.2175e-01   Top-1:  85.64   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2789240938593036\n",
      "T Top-1\t64.00512382578992\n",
      "V Loss\t0.8217544353485108\n",
      "V Top-1\t85.64\n",
      "\n",
      "Best acc1 86.64\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 49/100][T][1170]   Loss: 1.2897e+00   Top-1:  63.51   LR: 0.0006044        poch 49/100][T][0]   Loss: 1.5707e+00   Top-1:  13.28   LR: 0.0006213        \n",
      "[Epoch 49][V][78]   Loss: 8.1407e-01   Top-1:  86.10   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2896736907103854\n",
      "T Top-1\t63.51142186165671\n",
      "V Loss\t0.8140697832107544\n",
      "V Top-1\t86.1\n",
      "\n",
      "Best acc1 86.64\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 50/100][T][1170]   Loss: 1.2442e+00   Top-1:  65.11   LR: 0.0005872        poch 50/100][T][0]   Loss: 1.6285e+00   Top-1:  62.50   LR: 0.0006043        \n",
      "[Epoch 50][V][78]   Loss: 8.1358e-01   Top-1:  86.43   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2441852763613304\n",
      "T Top-1\t65.10661293766012\n",
      "V Loss\t0.813578239440918\n",
      "V Top-1\t86.43\n",
      "\n",
      "Best acc1 86.64\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 51/100][T][1170]   Loss: 1.2617e+00   Top-1:  64.51   LR: 0.0005700        poch 51/100][T][0]   Loss: 9.5219e-01   Top-1:  81.25   LR: 0.0005872        \n",
      "[Epoch 51][V][78]   Loss: 8.0928e-01   Top-1:  87.34   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2616978152810152\n",
      "T Top-1\t64.51483774551666\n",
      "V Loss\t0.8092783414840699\n",
      "V Top-1\t87.34\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 87.34\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 52/100][T][1170]   Loss: 1.2605e+00   Top-1:  64.57   LR: 0.0005527        poch 52/100][T][0]   Loss: 1.5513e+00   Top-1:  66.41   LR: 0.0005700        \n",
      "[Epoch 52][V][78]   Loss: 7.9852e-01   Top-1:  87.31   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.260452514576362\n",
      "T Top-1\t64.56620943637917\n",
      "V Loss\t0.7985195150375366\n",
      "V Top-1\t87.31\n",
      "\n",
      "Best acc1 87.34\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 53/100][T][1170]   Loss: 1.2613e+00   Top-1:  64.87   LR: 0.0005353        poch 53/100][T][0]   Loss: 9.8222e-01   Top-1:  78.12   LR: 0.0005527        \n",
      "[Epoch 53][V][78]   Loss: 8.0994e-01   Top-1:  87.67   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.261345001645418\n",
      "T Top-1\t64.87043659265585\n",
      "V Loss\t0.8099416865348816\n",
      "V Top-1\t87.67\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 87.67\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 54/100][T][1170]   Loss: 1.2510e+00   Top-1:  65.43   LR: 0.0005179        poch 54/100][T][0]   Loss: 1.1532e+00   Top-1:   7.03   LR: 0.0005353        \n",
      "[Epoch 54][V][78]   Loss: 8.0173e-01   Top-1:  87.01   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2510237642376978\n",
      "T Top-1\t65.43218936806149\n",
      "V Loss\t0.8017334446907044\n",
      "V Top-1\t87.01\n",
      "\n",
      "Best acc1 87.67\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 55/100][T][1170]   Loss: 1.2362e+00   Top-1:  66.29   LR: 0.0005005        poch 55/100][T][0]   Loss: 1.1198e+00   Top-1:  81.25   LR: 0.0005179        \n",
      "[Epoch 55][V][78]   Loss: 7.7855e-01   Top-1:  87.80   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2361668649135136\n",
      "T Top-1\t66.28816182749786\n",
      "V Loss\t0.7785516963958741\n",
      "V Top-1\t87.8\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 87.80\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 56/100][T][1170]   Loss: 1.2220e+00   Top-1:  66.43   LR: 0.0004831        poch 56/100][T][0]   Loss: 8.5504e-01   Top-1:  84.38   LR: 0.0005005        \n",
      "[Epoch 56][V][78]   Loss: 7.9438e-01   Top-1:  87.63   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.221953170051135\n",
      "T Top-1\t66.43493808710504\n",
      "V Loss\t0.7943769002914429\n",
      "V Top-1\t87.63\n",
      "\n",
      "Best acc1 87.80\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 57/100][T][1170]   Loss: 1.2254e+00   Top-1:  66.03   LR: 0.0004657        poch 57/100][T][0]   Loss: 9.1765e-01   Top-1:  79.69   LR: 0.0004831        \n",
      "[Epoch 57][V][78]   Loss: 7.6924e-01   Top-1:  88.25   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.225434956542454\n",
      "T Top-1\t66.03063620836892\n",
      "V Loss\t0.7692368701934814\n",
      "V Top-1\t88.25\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 88.25\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 58/100][T][1170]   Loss: 1.2365e+00   Top-1:  65.77   LR: 0.0004483        poch 58/100][T][0]   Loss: 1.6409e+00   Top-1:  61.72   LR: 0.0004656        \n",
      "[Epoch 58][V][78]   Loss: 7.9741e-01   Top-1:  87.98   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2365048235038933\n",
      "T Top-1\t65.76844043552519\n",
      "V Loss\t0.7974127034187317\n",
      "V Top-1\t87.98\n",
      "\n",
      "Best acc1 88.25\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 59/100][T][1170]   Loss: 1.2446e+00   Top-1:  64.91   LR: 0.0004310        poch 59/100][T][0]   Loss: 1.8986e+00   Top-1:  44.53   LR: 0.0004483        \n",
      "[Epoch 59][V][78]   Loss: 7.8787e-01   Top-1:  87.72   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2446264129332454\n",
      "T Top-1\t64.90779782237404\n",
      "V Loss\t0.7878676310539245\n",
      "V Top-1\t87.72\n",
      "\n",
      "Best acc1 88.25\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 60/100][T][1170]   Loss: 1.2248e+00   Top-1:  65.52   LR: 0.0004138        poch 60/100][T][0]   Loss: 8.7482e-01   Top-1:  82.03   LR: 0.0004310        \n",
      "[Epoch 60][V][78]   Loss: 7.9464e-01   Top-1:  87.62   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2247833711382266\n",
      "T Top-1\t65.52092228864218\n",
      "V Loss\t0.7946441822052002\n",
      "V Top-1\t87.62\n",
      "\n",
      "Best acc1 88.25\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 61/100][T][1170]   Loss: 1.2131e+00   Top-1:  66.83   LR: 0.0003966        poch 61/100][T][0]   Loss: 1.7879e+00   Top-1:  41.41   LR: 0.0004137        \n",
      "[Epoch 61][V][78]   Loss: 8.1480e-01   Top-1:  87.76   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2130583798040395\n",
      "T Top-1\t66.83056682322801\n",
      "V Loss\t0.814801148223877\n",
      "V Top-1\t87.76\n",
      "\n",
      "Best acc1 88.25\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 62/100][T][1170]   Loss: 1.1958e+00   Top-1:  67.31   LR: 0.0003797        poch 62/100][T][0]   Loss: 8.2327e-01   Top-1:  87.50   LR: 0.0003966        \n",
      "[Epoch 62][V][78]   Loss: 7.4576e-01   Top-1:  89.06   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1958182588995878\n",
      "T Top-1\t67.31025832621691\n",
      "V Loss\t0.7457602855682373\n",
      "V Top-1\t89.06\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 89.06\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 63/100][T][1170]   Loss: 1.2032e+00   Top-1:  66.80   LR: 0.0003628        poch 63/100][T][0]   Loss: 1.8690e+00   Top-1:  43.75   LR: 0.0003796        \n",
      "[Epoch 63][V][78]   Loss: 7.5954e-01   Top-1:  89.46   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.2031670346105332\n",
      "T Top-1\t66.7958742527754\n",
      "V Loss\t0.7595398272514343\n",
      "V Top-1\t89.46\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 89.46\n",
      "********************************************************************************\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 64/100][T][1170]   Loss: 1.1987e+00   Top-1:  67.47   LR: 0.0003461        poch 64/100][T][0]   Loss: 1.2104e+00   Top-1:  85.16   LR: 0.0003628        \n",
      "[Epoch 64][V][78]   Loss: 7.6252e-01   Top-1:  89.57   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.198699737338303\n",
      "T Top-1\t67.4717122117848\n",
      "V Loss\t0.7625228090286255\n",
      "V Top-1\t89.57\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 89.57\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 65/100][T][1170]   Loss: 1.2027e+00   Top-1:  66.84   LR: 0.0003297        poch 65/100][T][0]   Loss: 8.6011e-01   Top-1:  84.38   LR: 0.0003461        \n",
      "[Epoch 65][V][78]   Loss: 7.3717e-01   Top-1:  89.64   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.202688014782157\n",
      "T Top-1\t66.83657130657558\n",
      "V Loss\t0.7371687825202942\n",
      "V Top-1\t89.64\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 89.64\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 66/100][T][1170]   Loss: 1.2024e+00   Top-1:  65.98   LR: 0.0003134        poch 66/100][T][0]   Loss: 1.7282e+00   Top-1:  17.97   LR: 0.0003296        \n",
      "[Epoch 66][V][78]   Loss: 7.6070e-01   Top-1:  89.12   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.202395958843443\n",
      "T Top-1\t65.9812660119556\n",
      "V Loss\t0.7606985811233521\n",
      "V Top-1\t89.12\n",
      "\n",
      "Best acc1 89.64\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 67/100][T][1170]   Loss: 1.1980e+00   Top-1:  66.80   LR: 0.0002973        poch 67/100][T][0]   Loss: 8.0081e-01   Top-1:  88.28   LR: 0.0003134        \n",
      "[Epoch 67][V][78]   Loss: 7.5324e-01   Top-1:  89.18   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1980374173462442\n",
      "T Top-1\t66.79921007685739\n",
      "V Loss\t0.7532422052383423\n",
      "V Top-1\t89.18\n",
      "\n",
      "Best acc1 89.64\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 68/100][T][1170]   Loss: 1.1892e+00   Top-1:  67.50   LR: 0.0002815        poch 68/100][T][0]   Loss: 7.6456e-01   Top-1:  86.72   LR: 0.0002973        \n",
      "[Epoch 68][V][78]   Loss: 7.6088e-01   Top-1:  89.47   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1892134915754187\n",
      "T Top-1\t67.49639730999147\n",
      "V Loss\t0.7608773880004883\n",
      "V Top-1\t89.47\n",
      "\n",
      "Best acc1 89.64\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 69/100][T][1170]   Loss: 1.1672e+00   Top-1:  68.26   LR: 0.0002660        poch 69/100][T][0]   Loss: 9.8746e-01   Top-1:  79.69   LR: 0.0002815        \n",
      "[Epoch 69][V][78]   Loss: 7.3653e-01   Top-1:  89.71   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1671897559263684\n",
      "T Top-1\t68.25829953031597\n",
      "V Loss\t0.7365258525848388\n",
      "V Top-1\t89.71\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 89.71\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 70/100][T][1170]   Loss: 1.1925e+00   Top-1:  67.32   LR: 0.0002508        poch 70/100][T][0]   Loss: 1.7416e+00   Top-1:  57.03   LR: 0.0002660        \n",
      "[Epoch 70][V][78]   Loss: 7.2631e-01   Top-1:  90.02   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1924894562040405\n",
      "T Top-1\t67.32160012809564\n",
      "V Loss\t0.7263051658630371\n",
      "V Top-1\t90.02\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 90.02\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 71/100][T][1170]   Loss: 1.1712e+00   Top-1:  68.14   LR: 0.0002358        poch 71/100][T][0]   Loss: 1.6585e+00   Top-1:  71.09   LR: 0.0002507        \n",
      "[Epoch 71][V][78]   Loss: 7.2650e-01   Top-1:  90.01   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1712036234104766\n",
      "T Top-1\t68.13820986336465\n",
      "V Loss\t0.7265008719444275\n",
      "V Top-1\t90.01\n",
      "\n",
      "Best acc1 90.02\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 72/100][T][1170]   Loss: 1.1637e+00   Top-1:  68.99   LR: 0.0002212        poch 72/100][T][0]   Loss: 8.5303e-01   Top-1:  85.16   LR: 0.0002358        \n",
      "[Epoch 72][V][78]   Loss: 7.2160e-01   Top-1:  90.12   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1636678121926742\n",
      "T Top-1\t68.98550918018788\n",
      "V Loss\t0.7216008781433105\n",
      "V Top-1\t90.12\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 90.12\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 73/100][T][1170]   Loss: 1.1618e+00   Top-1:  68.41   LR: 0.0002069        poch 73/100][T][0]   Loss: 8.8230e-01   Top-1:  83.59   LR: 0.0002212        \n",
      "[Epoch 73][V][78]   Loss: 7.3153e-01   Top-1:  90.18   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1617916805636261\n",
      "T Top-1\t68.4124146029035\n",
      "V Loss\t0.7315253698348999\n",
      "V Top-1\t90.18\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 90.18\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 74/100][T][1170]   Loss: 1.1624e+00   Top-1:  68.45   LR: 0.0001930        poch 74/100][T][0]   Loss: 1.8240e+00   Top-1:  54.69   LR: 0.0002069        \n",
      "[Epoch 74][V][78]   Loss: 7.3605e-01   Top-1:  90.33   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.162408571229034\n",
      "T Top-1\t68.45311165670367\n",
      "V Loss\t0.7360465030670166\n",
      "V Top-1\t90.33\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 90.33\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 75/100][T][1170]   Loss: 1.1500e+00   Top-1:  69.18   LR: 0.0001794        poch 75/100][T][0]   Loss: 8.4685e-01   Top-1:  84.38   LR: 0.0001930        \n",
      "[Epoch 75][V][78]   Loss: 7.2376e-01   Top-1:  90.49   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1499909956571284\n",
      "T Top-1\t69.18432429547396\n",
      "V Loss\t0.723763049697876\n",
      "V Top-1\t90.49\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 90.49\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 76/100][T][1170]   Loss: 1.1287e+00   Top-1:  69.93   LR: 0.0001663        poch 76/100][T][0]   Loss: 1.1279e+00   Top-1:  92.19   LR: 0.0001794        \n",
      "[Epoch 76][V][78]   Loss: 7.1912e-01   Top-1:  90.86   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.12873691936317\n",
      "T Top-1\t69.92954739538855\n",
      "V Loss\t0.7191247779846192\n",
      "V Top-1\t90.86\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 90.86\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 77/100][T][1170]   Loss: 1.1232e+00   Top-1:  70.23   LR: 0.0001535        poch 77/100][T][0]   Loss: 1.3640e+00   Top-1:  77.34   LR: 0.0001663        \n",
      "[Epoch 77][V][78]   Loss: 7.2706e-01   Top-1:  90.67   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.123192612275825\n",
      "T Top-1\t70.23043872758326\n",
      "V Loss\t0.7270644764900207\n",
      "V Top-1\t90.67\n",
      "\n",
      "Best acc1 90.86\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 78/100][T][1170]   Loss: 1.1379e+00   Top-1:  69.79   LR: 0.0001412        poch 78/100][T][0]   Loss: 8.5154e-01   Top-1:  84.38   LR: 0.0001535        \n",
      "[Epoch 78][V][78]   Loss: 7.1982e-01   Top-1:  90.92   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1379230616440843\n",
      "T Top-1\t69.79211144321093\n",
      "V Loss\t0.7198188161849975\n",
      "V Top-1\t90.92\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 90.92\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 79/100][T][1170]   Loss: 1.1141e+00   Top-1:  70.64   LR: 0.0001293        poch 79/100][T][0]   Loss: 1.3549e+00   Top-1:  75.00   LR: 0.0001412        \n",
      "[Epoch 79][V][78]   Loss: 7.2115e-01   Top-1:  91.02   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1140616777918464\n",
      "T Top-1\t70.64274658411614\n",
      "V Loss\t0.7211515531539917\n",
      "V Top-1\t91.02\n",
      "\n",
      "* Best model upate *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best acc1 91.02\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 80/100][T][1170]   Loss: 1.1281e+00   Top-1:  69.93   LR: 0.0001179        poch 80/100][T][0]   Loss: 1.7839e+00   Top-1:  58.59   LR: 0.0001293        \n",
      "[Epoch 80][V][78]   Loss: 7.2044e-01   Top-1:  91.00   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1280823080688334\n",
      "T Top-1\t69.92621157130658\n",
      "V Loss\t0.7204424595832825\n",
      "V Top-1\t91.0\n",
      "\n",
      "Best acc1 91.02\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 81/100][T][1170]   Loss: 1.1219e+00   Top-1:  71.96   LR: 0.0001069        poch 81/100][T][0]   Loss: 7.5435e-01   Top-1:  91.41   LR: 0.0001179        \n",
      "[Epoch 81][V][78]   Loss: 7.1076e-01   Top-1:  91.25   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1219166646545307\n",
      "T Top-1\t71.95706127241674\n",
      "V Loss\t0.7107573156356811\n",
      "V Top-1\t91.25\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 91.25\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 82/100][T][1170]   Loss: 1.1212e+00   Top-1:  70.91   LR: 0.0000964        poch 82/100][T][0]   Loss: 8.2264e-01   Top-1:  85.16   LR: 0.0001069        \n",
      "[Epoch 82][V][78]   Loss: 7.1046e-01   Top-1:  91.43   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1211834411963146\n",
      "T Top-1\t70.91027967549104\n",
      "V Loss\t0.7104587613105774\n",
      "V Top-1\t91.43\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 91.43\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 83/100][T][1170]   Loss: 1.0999e+00   Top-1:  70.73   LR: 0.0000864        poch 83/100][T][0]   Loss: 7.5107e-01   Top-1:  89.84   LR: 0.0000964        \n",
      "[Epoch 83][V][78]   Loss: 7.1696e-01   Top-1:  91.07   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.099927257361522\n",
      "T Top-1\t70.73014517506405\n",
      "V Loss\t0.716963530254364\n",
      "V Top-1\t91.07\n",
      "\n",
      "Best acc1 91.43\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 84/100][T][1170]   Loss: 1.1014e+00   Top-1:  71.94   LR: 0.0000769        poch 84/100][T][0]   Loss: 6.9150e-01   Top-1:  90.62   LR: 0.0000864        \n",
      "[Epoch 84][V][78]   Loss: 6.9977e-01   Top-1:  91.77   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1014460465217837\n",
      "T Top-1\t71.93838065755764\n",
      "V Loss\t0.6997664610862732\n",
      "V Top-1\t91.77\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 91.77\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 85/100][T][1170]   Loss: 1.1008e+00   Top-1:  70.98   LR: 0.0000679        poch 85/100][T][0]   Loss: 7.6301e-01   Top-1:  89.84   LR: 0.0000769        \n",
      "[Epoch 85][V][78]   Loss: 6.9792e-01   Top-1:  91.49   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1007523135076112\n",
      "T Top-1\t70.98233347566183\n",
      "V Loss\t0.6979199412345887\n",
      "V Top-1\t91.49\n",
      "\n",
      "Best acc1 91.77\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 86/100][T][1170]   Loss: 1.1089e+00   Top-1:  71.83   LR: 0.0000595        poch 86/100][T][0]   Loss: 8.2698e-01   Top-1:  94.53   LR: 0.0000679        \n",
      "[Epoch 86][V][78]   Loss: 6.9284e-01   Top-1:  91.87   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.1088502545014698\n",
      "T Top-1\t71.83296861656704\n",
      "V Loss\t0.6928350214004516\n",
      "V Top-1\t91.87\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 91.87\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 87/100][T][1170]   Loss: 1.0667e+00   Top-1:  73.26   LR: 0.0000516        poch 87/100][T][0]   Loss: 8.2772e-01   Top-1:   5.47   LR: 0.0000595        \n",
      "[Epoch 87][V][78]   Loss: 7.0545e-01   Top-1:  91.67   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0666554094277112\n",
      "T Top-1\t73.25869982920581\n",
      "V Loss\t0.7054471803665161\n",
      "V Top-1\t91.67\n",
      "\n",
      "Best acc1 91.87\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 88/100][T][1170]   Loss: 1.0980e+00   Top-1:  72.37   LR: 0.0000442        poch 88/100][T][0]   Loss: 8.1571e-01   Top-1:  85.94   LR: 0.0000515        \n",
      "[Epoch 88][V][78]   Loss: 7.1616e-01   Top-1:  91.62   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0979629695975404\n",
      "T Top-1\t72.3713706233988\n",
      "V Loss\t0.7161627433776856\n",
      "V Top-1\t91.62\n",
      "\n",
      "Best acc1 91.87\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 89/100][T][1170]   Loss: 1.0692e+00   Top-1:  72.74   LR: 0.0000374        poch 89/100][T][0]   Loss: 7.1123e-01   Top-1:  92.97   LR: 0.0000442        \n",
      "[Epoch 89][V][78]   Loss: 7.0215e-01   Top-1:  91.97   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0692085731650498\n",
      "T Top-1\t72.73964560204954\n",
      "V Loss\t0.7021451488494873\n",
      "V Top-1\t91.97\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 91.97\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 90/100][T][1170]   Loss: 1.0867e+00   Top-1:  71.67   LR: 0.0000311        poch 90/100][T][0]   Loss: 6.8726e-01   Top-1:  93.75   LR: 0.0000374        \n",
      "[Epoch 90][V][78]   Loss: 7.0997e-01   Top-1:  91.77   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0866590077097247\n",
      "T Top-1\t71.66551024765158\n",
      "V Loss\t0.7099700414657593\n",
      "V Top-1\t91.77\n",
      "\n",
      "Best acc1 91.97\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 91/100][T][1170]   Loss: 1.0806e+00   Top-1:  72.77   LR: 0.0000254        poch 91/100][T][0]   Loss: 6.6525e-01   Top-1:  92.97   LR: 0.0000311        \n",
      "[Epoch 91][V][78]   Loss: 7.0551e-01   Top-1:  92.19   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0805794853211264\n",
      "T Top-1\t72.77033518360376\n",
      "V Loss\t0.70551022605896\n",
      "V Top-1\t92.19\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 92.19\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 92/100][T][1170]   Loss: 1.0991e+00   Top-1:  72.52   LR: 0.0000203        poch 92/100][T][0]   Loss: 1.1029e+00   Top-1:  80.47   LR: 0.0000254        \n",
      "[Epoch 92][V][78]   Loss: 6.9320e-01   Top-1:  91.84   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0990583026745093\n",
      "T Top-1\t72.52481853116994\n",
      "V Loss\t0.6932014673233032\n",
      "V Top-1\t91.84\n",
      "\n",
      "Best acc1 92.19\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 93/100][T][1170]   Loss: 1.0804e+00   Top-1:  72.06   LR: 0.0000158        poch 93/100][T][0]   Loss: 1.6706e+00   Top-1:  66.41   LR: 0.0000203        \n",
      "[Epoch 93][V][78]   Loss: 6.8915e-01   Top-1:  91.87   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.080354574040202\n",
      "T Top-1\t72.05980465414176\n",
      "V Loss\t0.6891470171928405\n",
      "V Top-1\t91.87\n",
      "\n",
      "Best acc1 92.19\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 94/100][T][1170]   Loss: 1.0762e+00   Top-1:  73.09   LR: 0.0000119        poch 94/100][T][0]   Loss: 6.7833e-01   Top-1:  95.31   LR: 0.0000158        \n",
      "[Epoch 94][V][78]   Loss: 6.9564e-01   Top-1:  92.01   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0762320010199902\n",
      "T Top-1\t73.08923996584116\n",
      "V Loss\t0.6956435322761536\n",
      "V Top-1\t92.01\n",
      "\n",
      "Best acc1 92.19\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 95/100][T][1170]   Loss: 1.0665e+00   Top-1:  73.50   LR: 0.0000086        poch 95/100][T][0]   Loss: 1.7996e+00   Top-1:  32.03   LR: 0.0000119        \n",
      "[Epoch 95][V][78]   Loss: 6.8656e-01   Top-1:  91.98   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0664579554259725\n",
      "T Top-1\t73.49887916310846\n",
      "V Loss\t0.686555153465271\n",
      "V Top-1\t91.98\n",
      "\n",
      "Best acc1 92.19\n",
      "********************************************************************************\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 96/100][T][1170]   Loss: 1.0756e+00   Top-1:  72.94   LR: 0.0000059        poch 96/100][T][0]   Loss: 7.3669e-01   Top-1:  89.84   LR: 0.0000086        \n",
      "[Epoch 96][V][78]   Loss: 6.9228e-01   Top-1:  92.04   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0755660044749917\n",
      "T Top-1\t72.94446520068318\n",
      "V Loss\t0.6922847143173217\n",
      "V Top-1\t92.04\n",
      "\n",
      "Best acc1 92.19\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 97/100][T][1170]   Loss: 1.0769e+00   Top-1:  73.15   LR: 0.0000037        poch 97/100][T][0]   Loss: 1.7540e+00   Top-1:  58.59   LR: 0.0000059        \n",
      "[Epoch 97][V][78]   Loss: 6.9145e-01   Top-1:  91.87   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0768858476849725\n",
      "T Top-1\t73.1539549530316\n",
      "V Loss\t0.691449906539917\n",
      "V Top-1\t91.87\n",
      "\n",
      "Best acc1 92.19\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 98/100][T][1170]   Loss: 1.0908e+00   Top-1:  72.68   LR: 0.0000022        poch 98/100][T][0]   Loss: 1.7462e+00   Top-1:  28.91   LR: 0.0000037        \n",
      "[Epoch 98][V][78]   Loss: 6.8626e-01   Top-1:  91.97   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.090798838845081\n",
      "T Top-1\t72.67893360375747\n",
      "V Loss\t0.6862590266227722\n",
      "V Top-1\t91.97\n",
      "\n",
      "Best acc1 92.19\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 99/100][T][1170]   Loss: 1.0773e+00   Top-1:  73.18   LR: 0.0000013        poch 99/100][T][0]   Loss: 7.7772e-01   Top-1:  88.28   LR: 0.0000022        \n",
      "[Epoch 99][V][78]   Loss: 6.8998e-01   Top-1:  92.03   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0773130519383998\n",
      "T Top-1\t73.18397736976942\n",
      "V Loss\t0.6899814109802246\n",
      "V Top-1\t92.03\n",
      "\n",
      "Best acc1 92.19\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 100/100][T][1170]   Loss: 1.0903e+00   Top-1:  72.40   LR: 0.0000010        och 100/100][T][0]   Loss: 7.1430e-01   Top-1:  90.62   LR: 0.0000013        \n",
      "[Epoch 100][V][78]   Loss: 6.9147e-01   Top-1:  91.97   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t1.0903114741119333\n",
      "T Top-1\t72.40005871050384\n",
      "V Loss\t0.6914704957962036\n",
      "V Top-1\t91.97\n",
      "\n",
      "Best acc1 92.19\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "100%|| 100/100 [7:26:00<00:00, 267.60s/it]\n",
      "\u001b[31m********************************************************************************\n",
      "best top-1: 92.19, final top-1: 91.97\n",
      "********************************************************************************\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python main.py --model resnet --dataset CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "901113e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m********************************************************************************\n",
      "CIFAR100\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[32m********************************************************************************\n",
      "Creating model: resnet-Base--CIFAR100-LR[0.001]-Seed0\n",
      "Number of params: 58,424,420\n",
      "Initial learning rate: 0.001000\n",
      "Start training for 100 epochs\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "label smoothing used\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Stochastic depth(0.1) used \n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Cutmix used\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Mixup used\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Repeated Aug(3) used\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Autoaugmentation used\n",
      "CIFAR Policy\n",
      "********************************************************************************\u001b[0m\n",
      "\u001b[33m********************************************************************************\n",
      "Random erasing(0.25) used \n",
      "********************************************************************************\u001b[0m\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,472\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]           4,160\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,928\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "           Conv2d-11            [-1, 256, 8, 8]          16,640\n",
      "      BatchNorm2d-12            [-1, 256, 8, 8]             512\n",
      "           Conv2d-13            [-1, 256, 8, 8]          16,640\n",
      "      BatchNorm2d-14            [-1, 256, 8, 8]             512\n",
      "             ReLU-15            [-1, 256, 8, 8]               0\n",
      "            Block-16            [-1, 256, 8, 8]               0\n",
      "           Conv2d-17             [-1, 64, 8, 8]          16,448\n",
      "      BatchNorm2d-18             [-1, 64, 8, 8]             128\n",
      "             ReLU-19             [-1, 64, 8, 8]               0\n",
      "           Conv2d-20             [-1, 64, 8, 8]          36,928\n",
      "      BatchNorm2d-21             [-1, 64, 8, 8]             128\n",
      "             ReLU-22             [-1, 64, 8, 8]               0\n",
      "           Conv2d-23            [-1, 256, 8, 8]          16,640\n",
      "      BatchNorm2d-24            [-1, 256, 8, 8]             512\n",
      "             ReLU-25            [-1, 256, 8, 8]               0\n",
      "            Block-26            [-1, 256, 8, 8]               0\n",
      "           Conv2d-27             [-1, 64, 8, 8]          16,448\n",
      "      BatchNorm2d-28             [-1, 64, 8, 8]             128\n",
      "             ReLU-29             [-1, 64, 8, 8]               0\n",
      "           Conv2d-30             [-1, 64, 8, 8]          36,928\n",
      "      BatchNorm2d-31             [-1, 64, 8, 8]             128\n",
      "             ReLU-32             [-1, 64, 8, 8]               0\n",
      "           Conv2d-33            [-1, 256, 8, 8]          16,640\n",
      "      BatchNorm2d-34            [-1, 256, 8, 8]             512\n",
      "             ReLU-35            [-1, 256, 8, 8]               0\n",
      "            Block-36            [-1, 256, 8, 8]               0\n",
      "           Conv2d-37            [-1, 128, 8, 8]          32,896\n",
      "      BatchNorm2d-38            [-1, 128, 8, 8]             256\n",
      "             ReLU-39            [-1, 128, 8, 8]               0\n",
      "           Conv2d-40            [-1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-41            [-1, 128, 4, 4]             256\n",
      "             ReLU-42            [-1, 128, 4, 4]               0\n",
      "           Conv2d-43            [-1, 512, 4, 4]          66,048\n",
      "      BatchNorm2d-44            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-45            [-1, 512, 4, 4]         131,584\n",
      "      BatchNorm2d-46            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-47            [-1, 512, 4, 4]               0\n",
      "            Block-48            [-1, 512, 4, 4]               0\n",
      "           Conv2d-49            [-1, 128, 4, 4]          65,664\n",
      "      BatchNorm2d-50            [-1, 128, 4, 4]             256\n",
      "             ReLU-51            [-1, 128, 4, 4]               0\n",
      "           Conv2d-52            [-1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-53            [-1, 128, 4, 4]             256\n",
      "             ReLU-54            [-1, 128, 4, 4]               0\n",
      "           Conv2d-55            [-1, 512, 4, 4]          66,048\n",
      "      BatchNorm2d-56            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-57            [-1, 512, 4, 4]               0\n",
      "            Block-58            [-1, 512, 4, 4]               0\n",
      "           Conv2d-59            [-1, 128, 4, 4]          65,664\n",
      "      BatchNorm2d-60            [-1, 128, 4, 4]             256\n",
      "             ReLU-61            [-1, 128, 4, 4]               0\n",
      "           Conv2d-62            [-1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-63            [-1, 128, 4, 4]             256\n",
      "             ReLU-64            [-1, 128, 4, 4]               0\n",
      "           Conv2d-65            [-1, 512, 4, 4]          66,048\n",
      "      BatchNorm2d-66            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-67            [-1, 512, 4, 4]               0\n",
      "            Block-68            [-1, 512, 4, 4]               0\n",
      "           Conv2d-69            [-1, 128, 4, 4]          65,664\n",
      "      BatchNorm2d-70            [-1, 128, 4, 4]             256\n",
      "             ReLU-71            [-1, 128, 4, 4]               0\n",
      "           Conv2d-72            [-1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-73            [-1, 128, 4, 4]             256\n",
      "             ReLU-74            [-1, 128, 4, 4]               0\n",
      "           Conv2d-75            [-1, 512, 4, 4]          66,048\n",
      "      BatchNorm2d-76            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-77            [-1, 512, 4, 4]               0\n",
      "            Block-78            [-1, 512, 4, 4]               0\n",
      "           Conv2d-79            [-1, 128, 4, 4]          65,664\n",
      "      BatchNorm2d-80            [-1, 128, 4, 4]             256\n",
      "             ReLU-81            [-1, 128, 4, 4]               0\n",
      "           Conv2d-82            [-1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-83            [-1, 128, 4, 4]             256\n",
      "             ReLU-84            [-1, 128, 4, 4]               0\n",
      "           Conv2d-85            [-1, 512, 4, 4]          66,048\n",
      "      BatchNorm2d-86            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-87            [-1, 512, 4, 4]               0\n",
      "            Block-88            [-1, 512, 4, 4]               0\n",
      "           Conv2d-89            [-1, 128, 4, 4]          65,664\n",
      "      BatchNorm2d-90            [-1, 128, 4, 4]             256\n",
      "             ReLU-91            [-1, 128, 4, 4]               0\n",
      "           Conv2d-92            [-1, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-93            [-1, 128, 4, 4]             256\n",
      "             ReLU-94            [-1, 128, 4, 4]               0\n",
      "           Conv2d-95            [-1, 512, 4, 4]          66,048\n",
      "      BatchNorm2d-96            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-97            [-1, 512, 4, 4]               0\n",
      "            Block-98            [-1, 512, 4, 4]               0\n",
      "           Conv2d-99            [-1, 128, 4, 4]          65,664\n",
      "     BatchNorm2d-100            [-1, 128, 4, 4]             256\n",
      "            ReLU-101            [-1, 128, 4, 4]               0\n",
      "          Conv2d-102            [-1, 128, 4, 4]         147,584\n",
      "     BatchNorm2d-103            [-1, 128, 4, 4]             256\n",
      "            ReLU-104            [-1, 128, 4, 4]               0\n",
      "          Conv2d-105            [-1, 512, 4, 4]          66,048\n",
      "     BatchNorm2d-106            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-107            [-1, 512, 4, 4]               0\n",
      "           Block-108            [-1, 512, 4, 4]               0\n",
      "          Conv2d-109            [-1, 128, 4, 4]          65,664\n",
      "     BatchNorm2d-110            [-1, 128, 4, 4]             256\n",
      "            ReLU-111            [-1, 128, 4, 4]               0\n",
      "          Conv2d-112            [-1, 128, 4, 4]         147,584\n",
      "     BatchNorm2d-113            [-1, 128, 4, 4]             256\n",
      "            ReLU-114            [-1, 128, 4, 4]               0\n",
      "          Conv2d-115            [-1, 512, 4, 4]          66,048\n",
      "     BatchNorm2d-116            [-1, 512, 4, 4]           1,024\n",
      "            ReLU-117            [-1, 512, 4, 4]               0\n",
      "           Block-118            [-1, 512, 4, 4]               0\n",
      "          Conv2d-119            [-1, 256, 4, 4]         131,328\n",
      "     BatchNorm2d-120            [-1, 256, 4, 4]             512\n",
      "            ReLU-121            [-1, 256, 4, 4]               0\n",
      "          Conv2d-122            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-123            [-1, 256, 2, 2]             512\n",
      "            ReLU-124            [-1, 256, 2, 2]               0\n",
      "          Conv2d-125           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-126           [-1, 1024, 2, 2]           2,048\n",
      "          Conv2d-127           [-1, 1024, 2, 2]         525,312\n",
      "     BatchNorm2d-128           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-129           [-1, 1024, 2, 2]               0\n",
      "           Block-130           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-131            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-132            [-1, 256, 2, 2]             512\n",
      "            ReLU-133            [-1, 256, 2, 2]               0\n",
      "          Conv2d-134            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-135            [-1, 256, 2, 2]             512\n",
      "            ReLU-136            [-1, 256, 2, 2]               0\n",
      "          Conv2d-137           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-138           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-139           [-1, 1024, 2, 2]               0\n",
      "           Block-140           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-141            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-142            [-1, 256, 2, 2]             512\n",
      "            ReLU-143            [-1, 256, 2, 2]               0\n",
      "          Conv2d-144            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-145            [-1, 256, 2, 2]             512\n",
      "            ReLU-146            [-1, 256, 2, 2]               0\n",
      "          Conv2d-147           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-148           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-149           [-1, 1024, 2, 2]               0\n",
      "           Block-150           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-151            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-152            [-1, 256, 2, 2]             512\n",
      "            ReLU-153            [-1, 256, 2, 2]               0\n",
      "          Conv2d-154            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-155            [-1, 256, 2, 2]             512\n",
      "            ReLU-156            [-1, 256, 2, 2]               0\n",
      "          Conv2d-157           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-158           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-159           [-1, 1024, 2, 2]               0\n",
      "           Block-160           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-161            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-162            [-1, 256, 2, 2]             512\n",
      "            ReLU-163            [-1, 256, 2, 2]               0\n",
      "          Conv2d-164            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-165            [-1, 256, 2, 2]             512\n",
      "            ReLU-166            [-1, 256, 2, 2]               0\n",
      "          Conv2d-167           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-168           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-169           [-1, 1024, 2, 2]               0\n",
      "           Block-170           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-171            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-172            [-1, 256, 2, 2]             512\n",
      "            ReLU-173            [-1, 256, 2, 2]               0\n",
      "          Conv2d-174            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-175            [-1, 256, 2, 2]             512\n",
      "            ReLU-176            [-1, 256, 2, 2]               0\n",
      "          Conv2d-177           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-178           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-179           [-1, 1024, 2, 2]               0\n",
      "           Block-180           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-181            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-182            [-1, 256, 2, 2]             512\n",
      "            ReLU-183            [-1, 256, 2, 2]               0\n",
      "          Conv2d-184            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-185            [-1, 256, 2, 2]             512\n",
      "            ReLU-186            [-1, 256, 2, 2]               0\n",
      "          Conv2d-187           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-188           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-189           [-1, 1024, 2, 2]               0\n",
      "           Block-190           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-191            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-192            [-1, 256, 2, 2]             512\n",
      "            ReLU-193            [-1, 256, 2, 2]               0\n",
      "          Conv2d-194            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-195            [-1, 256, 2, 2]             512\n",
      "            ReLU-196            [-1, 256, 2, 2]               0\n",
      "          Conv2d-197           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-198           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-199           [-1, 1024, 2, 2]               0\n",
      "           Block-200           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-201            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-202            [-1, 256, 2, 2]             512\n",
      "            ReLU-203            [-1, 256, 2, 2]               0\n",
      "          Conv2d-204            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-205            [-1, 256, 2, 2]             512\n",
      "            ReLU-206            [-1, 256, 2, 2]               0\n",
      "          Conv2d-207           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-208           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-209           [-1, 1024, 2, 2]               0\n",
      "           Block-210           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-211            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-212            [-1, 256, 2, 2]             512\n",
      "            ReLU-213            [-1, 256, 2, 2]               0\n",
      "          Conv2d-214            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-215            [-1, 256, 2, 2]             512\n",
      "            ReLU-216            [-1, 256, 2, 2]               0\n",
      "          Conv2d-217           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-218           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-219           [-1, 1024, 2, 2]               0\n",
      "           Block-220           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-221            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-222            [-1, 256, 2, 2]             512\n",
      "            ReLU-223            [-1, 256, 2, 2]               0\n",
      "          Conv2d-224            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-225            [-1, 256, 2, 2]             512\n",
      "            ReLU-226            [-1, 256, 2, 2]               0\n",
      "          Conv2d-227           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-228           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-229           [-1, 1024, 2, 2]               0\n",
      "           Block-230           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-231            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-232            [-1, 256, 2, 2]             512\n",
      "            ReLU-233            [-1, 256, 2, 2]               0\n",
      "          Conv2d-234            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-235            [-1, 256, 2, 2]             512\n",
      "            ReLU-236            [-1, 256, 2, 2]               0\n",
      "          Conv2d-237           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-238           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-239           [-1, 1024, 2, 2]               0\n",
      "           Block-240           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-241            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-242            [-1, 256, 2, 2]             512\n",
      "            ReLU-243            [-1, 256, 2, 2]               0\n",
      "          Conv2d-244            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-245            [-1, 256, 2, 2]             512\n",
      "            ReLU-246            [-1, 256, 2, 2]               0\n",
      "          Conv2d-247           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-248           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-249           [-1, 1024, 2, 2]               0\n",
      "           Block-250           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-251            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-252            [-1, 256, 2, 2]             512\n",
      "            ReLU-253            [-1, 256, 2, 2]               0\n",
      "          Conv2d-254            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-255            [-1, 256, 2, 2]             512\n",
      "            ReLU-256            [-1, 256, 2, 2]               0\n",
      "          Conv2d-257           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-258           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-259           [-1, 1024, 2, 2]               0\n",
      "           Block-260           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-261            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-262            [-1, 256, 2, 2]             512\n",
      "            ReLU-263            [-1, 256, 2, 2]               0\n",
      "          Conv2d-264            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-265            [-1, 256, 2, 2]             512\n",
      "            ReLU-266            [-1, 256, 2, 2]               0\n",
      "          Conv2d-267           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-268           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-269           [-1, 1024, 2, 2]               0\n",
      "           Block-270           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-271            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-272            [-1, 256, 2, 2]             512\n",
      "            ReLU-273            [-1, 256, 2, 2]               0\n",
      "          Conv2d-274            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-275            [-1, 256, 2, 2]             512\n",
      "            ReLU-276            [-1, 256, 2, 2]               0\n",
      "          Conv2d-277           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-278           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-279           [-1, 1024, 2, 2]               0\n",
      "           Block-280           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-281            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-282            [-1, 256, 2, 2]             512\n",
      "            ReLU-283            [-1, 256, 2, 2]               0\n",
      "          Conv2d-284            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-285            [-1, 256, 2, 2]             512\n",
      "            ReLU-286            [-1, 256, 2, 2]               0\n",
      "          Conv2d-287           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-288           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-289           [-1, 1024, 2, 2]               0\n",
      "           Block-290           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-291            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-292            [-1, 256, 2, 2]             512\n",
      "            ReLU-293            [-1, 256, 2, 2]               0\n",
      "          Conv2d-294            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-295            [-1, 256, 2, 2]             512\n",
      "            ReLU-296            [-1, 256, 2, 2]               0\n",
      "          Conv2d-297           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-298           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-299           [-1, 1024, 2, 2]               0\n",
      "           Block-300           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-301            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-302            [-1, 256, 2, 2]             512\n",
      "            ReLU-303            [-1, 256, 2, 2]               0\n",
      "          Conv2d-304            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-305            [-1, 256, 2, 2]             512\n",
      "            ReLU-306            [-1, 256, 2, 2]               0\n",
      "          Conv2d-307           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-308           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-309           [-1, 1024, 2, 2]               0\n",
      "           Block-310           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-311            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-312            [-1, 256, 2, 2]             512\n",
      "            ReLU-313            [-1, 256, 2, 2]               0\n",
      "          Conv2d-314            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-315            [-1, 256, 2, 2]             512\n",
      "            ReLU-316            [-1, 256, 2, 2]               0\n",
      "          Conv2d-317           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-318           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-319           [-1, 1024, 2, 2]               0\n",
      "           Block-320           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-321            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-322            [-1, 256, 2, 2]             512\n",
      "            ReLU-323            [-1, 256, 2, 2]               0\n",
      "          Conv2d-324            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-325            [-1, 256, 2, 2]             512\n",
      "            ReLU-326            [-1, 256, 2, 2]               0\n",
      "          Conv2d-327           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-328           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-329           [-1, 1024, 2, 2]               0\n",
      "           Block-330           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-331            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-332            [-1, 256, 2, 2]             512\n",
      "            ReLU-333            [-1, 256, 2, 2]               0\n",
      "          Conv2d-334            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-335            [-1, 256, 2, 2]             512\n",
      "            ReLU-336            [-1, 256, 2, 2]               0\n",
      "          Conv2d-337           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-338           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-339           [-1, 1024, 2, 2]               0\n",
      "           Block-340           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-341            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-342            [-1, 256, 2, 2]             512\n",
      "            ReLU-343            [-1, 256, 2, 2]               0\n",
      "          Conv2d-344            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-345            [-1, 256, 2, 2]             512\n",
      "            ReLU-346            [-1, 256, 2, 2]               0\n",
      "          Conv2d-347           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-348           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-349           [-1, 1024, 2, 2]               0\n",
      "           Block-350           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-351            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-352            [-1, 256, 2, 2]             512\n",
      "            ReLU-353            [-1, 256, 2, 2]               0\n",
      "          Conv2d-354            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-355            [-1, 256, 2, 2]             512\n",
      "            ReLU-356            [-1, 256, 2, 2]               0\n",
      "          Conv2d-357           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-358           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-359           [-1, 1024, 2, 2]               0\n",
      "           Block-360           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-361            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-362            [-1, 256, 2, 2]             512\n",
      "            ReLU-363            [-1, 256, 2, 2]               0\n",
      "          Conv2d-364            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-365            [-1, 256, 2, 2]             512\n",
      "            ReLU-366            [-1, 256, 2, 2]               0\n",
      "          Conv2d-367           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-368           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-369           [-1, 1024, 2, 2]               0\n",
      "           Block-370           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-371            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-372            [-1, 256, 2, 2]             512\n",
      "            ReLU-373            [-1, 256, 2, 2]               0\n",
      "          Conv2d-374            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-375            [-1, 256, 2, 2]             512\n",
      "            ReLU-376            [-1, 256, 2, 2]               0\n",
      "          Conv2d-377           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-378           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-379           [-1, 1024, 2, 2]               0\n",
      "           Block-380           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-381            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-382            [-1, 256, 2, 2]             512\n",
      "            ReLU-383            [-1, 256, 2, 2]               0\n",
      "          Conv2d-384            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-385            [-1, 256, 2, 2]             512\n",
      "            ReLU-386            [-1, 256, 2, 2]               0\n",
      "          Conv2d-387           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-388           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-389           [-1, 1024, 2, 2]               0\n",
      "           Block-390           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-391            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-392            [-1, 256, 2, 2]             512\n",
      "            ReLU-393            [-1, 256, 2, 2]               0\n",
      "          Conv2d-394            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-395            [-1, 256, 2, 2]             512\n",
      "            ReLU-396            [-1, 256, 2, 2]               0\n",
      "          Conv2d-397           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-398           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-399           [-1, 1024, 2, 2]               0\n",
      "           Block-400           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-401            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-402            [-1, 256, 2, 2]             512\n",
      "            ReLU-403            [-1, 256, 2, 2]               0\n",
      "          Conv2d-404            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-405            [-1, 256, 2, 2]             512\n",
      "            ReLU-406            [-1, 256, 2, 2]               0\n",
      "          Conv2d-407           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-408           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-409           [-1, 1024, 2, 2]               0\n",
      "           Block-410           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-411            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-412            [-1, 256, 2, 2]             512\n",
      "            ReLU-413            [-1, 256, 2, 2]               0\n",
      "          Conv2d-414            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-415            [-1, 256, 2, 2]             512\n",
      "            ReLU-416            [-1, 256, 2, 2]               0\n",
      "          Conv2d-417           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-418           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-419           [-1, 1024, 2, 2]               0\n",
      "           Block-420           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-421            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-422            [-1, 256, 2, 2]             512\n",
      "            ReLU-423            [-1, 256, 2, 2]               0\n",
      "          Conv2d-424            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-425            [-1, 256, 2, 2]             512\n",
      "            ReLU-426            [-1, 256, 2, 2]               0\n",
      "          Conv2d-427           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-428           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-429           [-1, 1024, 2, 2]               0\n",
      "           Block-430           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-431            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-432            [-1, 256, 2, 2]             512\n",
      "            ReLU-433            [-1, 256, 2, 2]               0\n",
      "          Conv2d-434            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-435            [-1, 256, 2, 2]             512\n",
      "            ReLU-436            [-1, 256, 2, 2]               0\n",
      "          Conv2d-437           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-438           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-439           [-1, 1024, 2, 2]               0\n",
      "           Block-440           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-441            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-442            [-1, 256, 2, 2]             512\n",
      "            ReLU-443            [-1, 256, 2, 2]               0\n",
      "          Conv2d-444            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-445            [-1, 256, 2, 2]             512\n",
      "            ReLU-446            [-1, 256, 2, 2]               0\n",
      "          Conv2d-447           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-448           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-449           [-1, 1024, 2, 2]               0\n",
      "           Block-450           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-451            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-452            [-1, 256, 2, 2]             512\n",
      "            ReLU-453            [-1, 256, 2, 2]               0\n",
      "          Conv2d-454            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-455            [-1, 256, 2, 2]             512\n",
      "            ReLU-456            [-1, 256, 2, 2]               0\n",
      "          Conv2d-457           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-458           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-459           [-1, 1024, 2, 2]               0\n",
      "           Block-460           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-461            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-462            [-1, 256, 2, 2]             512\n",
      "            ReLU-463            [-1, 256, 2, 2]               0\n",
      "          Conv2d-464            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-465            [-1, 256, 2, 2]             512\n",
      "            ReLU-466            [-1, 256, 2, 2]               0\n",
      "          Conv2d-467           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-468           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-469           [-1, 1024, 2, 2]               0\n",
      "           Block-470           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-471            [-1, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-472            [-1, 256, 2, 2]             512\n",
      "            ReLU-473            [-1, 256, 2, 2]               0\n",
      "          Conv2d-474            [-1, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-475            [-1, 256, 2, 2]             512\n",
      "            ReLU-476            [-1, 256, 2, 2]               0\n",
      "          Conv2d-477           [-1, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-478           [-1, 1024, 2, 2]           2,048\n",
      "            ReLU-479           [-1, 1024, 2, 2]               0\n",
      "           Block-480           [-1, 1024, 2, 2]               0\n",
      "          Conv2d-481            [-1, 512, 2, 2]         524,800\n",
      "     BatchNorm2d-482            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-483            [-1, 512, 2, 2]               0\n",
      "          Conv2d-484            [-1, 512, 1, 1]       2,359,808\n",
      "     BatchNorm2d-485            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-486            [-1, 512, 1, 1]               0\n",
      "          Conv2d-487           [-1, 2048, 1, 1]       1,050,624\n",
      "     BatchNorm2d-488           [-1, 2048, 1, 1]           4,096\n",
      "          Conv2d-489           [-1, 2048, 1, 1]       2,099,200\n",
      "     BatchNorm2d-490           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-491           [-1, 2048, 1, 1]               0\n",
      "           Block-492           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-493            [-1, 512, 1, 1]       1,049,088\n",
      "     BatchNorm2d-494            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-495            [-1, 512, 1, 1]               0\n",
      "          Conv2d-496            [-1, 512, 1, 1]       2,359,808\n",
      "     BatchNorm2d-497            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-498            [-1, 512, 1, 1]               0\n",
      "          Conv2d-499           [-1, 2048, 1, 1]       1,050,624\n",
      "     BatchNorm2d-500           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-501           [-1, 2048, 1, 1]               0\n",
      "           Block-502           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-503            [-1, 512, 1, 1]       1,049,088\n",
      "     BatchNorm2d-504            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-505            [-1, 512, 1, 1]               0\n",
      "          Conv2d-506            [-1, 512, 1, 1]       2,359,808\n",
      "     BatchNorm2d-507            [-1, 512, 1, 1]           1,024\n",
      "            ReLU-508            [-1, 512, 1, 1]               0\n",
      "          Conv2d-509           [-1, 2048, 1, 1]       1,050,624\n",
      "     BatchNorm2d-510           [-1, 2048, 1, 1]           4,096\n",
      "            ReLU-511           [-1, 2048, 1, 1]               0\n",
      "           Block-512           [-1, 2048, 1, 1]               0\n",
      "AdaptiveAvgPool2d-513           [-1, 2048, 1, 1]               0\n",
      "          Linear-514                  [-1, 100]         204,900\n",
      "================================================================\n",
      "Total params: 58,424,420\n",
      "Trainable params: 58,424,420\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 12.40\n",
      "Params size (MB): 222.87\n",
      "Estimated Total Size (MB): 235.28\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Beginning training\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/100][T][1170]   Loss: 4.6941e+00   Top-1:   1.19   LR: 0.0001009        Epoch 1/100][T][0]   Loss: 4.9695e+00   Top-1:   0.78   LR: 0.0000011        \n",
      "[Epoch 1][V][78]   Loss: 4.5671e+00   Top-1:   2.05   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t4.694054272111718\n",
      "T Top-1\t1.1875533731853116\n",
      "V Loss\t4.567076784515381\n",
      "V Top-1\t2.05\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 2.05\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 2/100][T][1170]   Loss: 4.6035e+00   Top-1:   1.76   LR: 0.0002008        Epoch 2/100][T][0]   Loss: 4.6356e+00   Top-1:   0.00   LR: 0.0001010        \n",
      "[Epoch 2][V][78]   Loss: 4.4340e+00   Top-1:   4.17   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t4.603502374139227\n",
      "T Top-1\t1.760647950469684\n",
      "V Loss\t4.433975808715821\n",
      "V Top-1\t4.17\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 4.17\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 3/100][T][1170]   Loss: 4.5188e+00   Top-1:   3.15   LR: 0.0003007        Epoch 3/100][T][0]   Loss: 4.5071e+00   Top-1:   1.56   LR: 0.0002009        \n",
      "[Epoch 3][V][78]   Loss: 5.4306e+00   Top-1:   6.79   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t4.518810450979831\n",
      "T Top-1\t3.149685098206661\n",
      "V Loss\t5.43063782119751\n",
      "V Top-1\t6.79\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 6.79\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 4/100][T][1170]   Loss: 4.4686e+00   Top-1:   4.59   LR: 0.0004006        Epoch 4/100][T][0]   Loss: 4.5525e+00   Top-1:   3.91   LR: 0.0003008        \n",
      "[Epoch 4][V][78]   Loss: 4.6312e+00   Top-1:   9.61   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t4.468563829498551\n",
      "T Top-1\t4.586090947907771\n",
      "V Loss\t4.631226658630371\n",
      "V Top-1\t9.61\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 9.61\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 5/100][T][1170]   Loss: 4.4021e+00   Top-1:   5.68   LR: 0.0005005        Epoch 5/100][T][0]   Loss: 4.1895e+00   Top-1:   6.25   LR: 0.0004007        \n",
      "[Epoch 5][V][78]   Loss: 5.6119e+00   Top-1:  11.94   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t4.402115402417138\n",
      "T Top-1\t5.6789069171648165\n",
      "V Loss\t5.6119352687835695\n",
      "V Top-1\t11.94\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 11.94\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 6/100][T][1170]   Loss: 4.3036e+00   Top-1:   7.40   LR: 0.0006004        Epoch 6/100][T][0]   Loss: 4.4429e+00   Top-1:   7.81   LR: 0.0005006        \n",
      "[Epoch 6][V][78]   Loss: 6.8337e+00   Top-1:  16.45   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t4.303557213303576\n",
      "T Top-1\t7.398190649017933\n",
      "V Loss\t6.833692725372314\n",
      "V Top-1\t16.45\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 16.45\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 7/100][T][1170]   Loss: 4.3974e+00   Top-1:   6.91   LR: 0.0007003        Epoch 7/100][T][0]   Loss: 4.3843e+00   Top-1:   6.25   LR: 0.0006005        \n",
      "[Epoch 7][V][78]   Loss: 4.1079e+00   Top-1:  15.83   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t4.3973947467201295\n",
      "T Top-1\t6.90515584970111\n",
      "V Loss\t4.107945838165283\n",
      "V Top-1\t15.83\n",
      "\n",
      "Best acc1 16.45\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 8/100][T][1170]   Loss: 4.3098e+00   Top-1:   7.47   LR: 0.0008002        Epoch 8/100][T][0]   Loss: 4.2576e+00   Top-1:   7.81   LR: 0.0007004        \n",
      "[Epoch 8][V][78]   Loss: 3.7870e+00   Top-1:  16.01   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t4.309833767065933\n",
      "T Top-1\t7.472245943637916\n",
      "V Loss\t3.7870122772216797\n",
      "V Top-1\t16.01\n",
      "\n",
      "Best acc1 16.45\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 9/100][T][1170]   Loss: 4.1811e+00   Top-1:   9.25   LR: 0.0009001        Epoch 9/100][T][0]   Loss: 4.0003e+00   Top-1:  10.94   LR: 0.0008003        \n",
      "[Epoch 9][V][78]   Loss: 3.5277e+00   Top-1:  20.12   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t4.181084259059264\n",
      "T Top-1\t9.24890584970111\n",
      "V Loss\t3.5277209789276123\n",
      "V Top-1\t20.12\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 20.12\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 10/100][T][1170]   Loss: 4.0260e+00   Top-1:  11.63   LR: 0.0010000        poch 10/100][T][0]   Loss: 3.9281e+00   Top-1:  10.94   LR: 0.0009002        \n",
      "[Epoch 10][V][78]   Loss: 3.3661e+00   Top-1:  24.90   LR: 0.0010\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t4.025986073059877\n",
      "T Top-1\t11.634020068317676\n",
      "V Loss\t3.3660969871521\n",
      "V Top-1\t24.9\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 24.90\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 11/100][T][1170]   Loss: 3.9044e+00   Top-1:  14.11   LR: 0.0009997        poch 11/100][T][0]   Loss: 3.8388e+00   Top-1:  15.62   LR: 0.0010000        \n",
      "[Epoch 11][V][78]   Loss: 3.2126e+00   Top-1:  27.77   LR: 0.0010\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.904352542378778\n",
      "T Top-1\t14.112537361229718\n",
      "V Loss\t3.2126225978851317\n",
      "V Top-1\t27.77\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 27.77\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 12/100][T][1170]   Loss: 3.9189e+00   Top-1:  13.99   LR: 0.0009988        poch 12/100][T][0]   Loss: 3.7060e+00   Top-1:  20.31   LR: 0.0009997        \n",
      "[Epoch 12][V][78]   Loss: 3.2423e+00   Top-1:  27.76   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.918854421473893\n",
      "T Top-1\t13.990446199829206\n",
      "V Loss\t3.242312212371826\n",
      "V Top-1\t27.76\n",
      "\n",
      "Best acc1 27.77\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 13/100][T][1170]   Loss: 3.7838e+00   Top-1:  16.29   LR: 0.0009973        poch 13/100][T][0]   Loss: 3.5071e+00   Top-1:  22.66   LR: 0.0009988        \n",
      "[Epoch 13][V][78]   Loss: 3.2664e+00   Top-1:  31.81   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.783794480648741\n",
      "T Top-1\t16.28549316823228\n",
      "V Loss\t3.2664395885467528\n",
      "V Top-1\t31.81\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 31.81\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 14/100][T][1170]   Loss: 3.6991e+00   Top-1:  18.67   LR: 0.0009951        poch 14/100][T][0]   Loss: 3.5915e+00   Top-1:  20.31   LR: 0.0009973        \n",
      "[Epoch 14][V][78]   Loss: 2.9209e+00   Top-1:  35.45   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.6991161548816476\n",
      "T Top-1\t18.665937233134073\n",
      "V Loss\t2.9208749061584474\n",
      "V Top-1\t35.45\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 35.45\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 15/100][T][1170]   Loss: 3.6035e+00   Top-1:  20.57   LR: 0.0009924        poch 15/100][T][0]   Loss: 3.3594e+00   Top-1:  24.22   LR: 0.0009951        \n",
      "[Epoch 15][V][78]   Loss: 2.7679e+00   Top-1:  39.26   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.6034825733846114\n",
      "T Top-1\t20.571359948761742\n",
      "V Loss\t2.767865414428711\n",
      "V Top-1\t39.26\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 39.26\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 16/100][T][1170]   Loss: 3.5286e+00   Top-1:  22.13   LR: 0.0009891        poch 16/100][T][0]   Loss: 4.2984e+00   Top-1:   8.59   LR: 0.0009924        \n",
      "[Epoch 16][V][78]   Loss: 2.7271e+00   Top-1:  40.35   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.528647521334159\n",
      "T Top-1\t22.134527113578137\n",
      "V Loss\t2.727057872772217\n",
      "V Top-1\t40.35\n",
      "\n",
      "* Best model upate *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best acc1 40.35\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 17/100][T][1170]   Loss: 3.4594e+00   Top-1:  23.71   LR: 0.0009852        poch 17/100][T][0]   Loss: 2.9882e+00   Top-1:  32.81   LR: 0.0009891        \n",
      "[Epoch 17][V][78]   Loss: 3.8785e+00   Top-1:  40.29   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.459377846282125\n",
      "T Top-1\t23.708368915456873\n",
      "V Loss\t3.878541297149658\n",
      "V Top-1\t40.29\n",
      "\n",
      "Best acc1 40.35\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 18/100][T][1170]   Loss: 3.4261e+00   Top-1:  24.73   LR: 0.0009807        poch 18/100][T][0]   Loss: 3.8133e+00   Top-1:  27.34   LR: 0.0009852        \n",
      "[Epoch 18][V][78]   Loss: 2.7177e+00   Top-1:  41.20   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.426127993073044\n",
      "T Top-1\t24.728463919726728\n",
      "V Loss\t2.717652040481567\n",
      "V Top-1\t41.2\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 41.20\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 19/100][T][1170]   Loss: 3.3881e+00   Top-1:  25.64   LR: 0.0009756        poch 19/100][T][0]   Loss: 4.1260e+00   Top-1:   7.81   LR: 0.0009806        \n",
      "[Epoch 19][V][78]   Loss: 2.7486e+00   Top-1:  43.88   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.388076398435767\n",
      "T Top-1\t25.639811058923996\n",
      "V Loss\t2.7486155406951904\n",
      "V Top-1\t43.88\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 43.88\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 20/100][T][1170]   Loss: 3.4101e+00   Top-1:  24.80   LR: 0.0009699        poch 20/100][T][0]   Loss: 2.9412e+00   Top-1:  32.03   LR: 0.0009755        \n",
      "[Epoch 20][V][78]   Loss: 2.8426e+00   Top-1:  44.47   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.4100647966236663\n",
      "T Top-1\t24.79985055508113\n",
      "V Loss\t2.8425811698913575\n",
      "V Top-1\t44.47\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 44.47\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 21/100][T][1170]   Loss: 3.3903e+00   Top-1:  26.09   LR: 0.0009636        poch 21/100][T][0]   Loss: 4.2756e+00   Top-1:   4.69   LR: 0.0009699        \n",
      "[Epoch 21][V][78]   Loss: 2.4724e+00   Top-1:  47.03   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.3903411686878546\n",
      "T Top-1\t26.086144321093084\n",
      "V Loss\t2.4723693744659423\n",
      "V Top-1\t47.03\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 47.03\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 22/100][T][1170]   Loss: 3.3363e+00   Top-1:  27.54   LR: 0.0009568        poch 22/100][T][0]   Loss: 2.7727e+00   Top-1:  38.28   LR: 0.0009636        \n",
      "[Epoch 22][V][78]   Loss: 2.7895e+00   Top-1:  41.27   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.33633460823437\n",
      "T Top-1\t27.540563620836892\n",
      "V Loss\t2.7894599769592285\n",
      "V Top-1\t41.27\n",
      "\n",
      "Best acc1 47.03\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 23/100][T][1170]   Loss: 3.3040e+00   Top-1:  27.71   LR: 0.0009494        poch 23/100][T][0]   Loss: 3.1413e+00   Top-1:  31.25   LR: 0.0009568        \n",
      "[Epoch 23][V][78]   Loss: 2.6436e+00   Top-1:  42.67   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.303990358577438\n",
      "T Top-1\t27.70935631938514\n",
      "V Loss\t2.64356862411499\n",
      "V Top-1\t42.67\n",
      "\n",
      "Best acc1 47.03\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 24/100][T][1170]   Loss: 3.3691e+00   Top-1:  26.16   LR: 0.0009415        poch 24/100][T][0]   Loss: 2.8647e+00   Top-1:  33.59   LR: 0.0009494        \n",
      "[Epoch 24][V][78]   Loss: 2.5247e+00   Top-1:  47.64   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.3691056033263953\n",
      "T Top-1\t26.15552946199829\n",
      "V Loss\t2.524737464141846\n",
      "V Top-1\t47.64\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 47.64\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 25/100][T][1170]   Loss: 3.4487e+00   Top-1:  24.49   LR: 0.0009331        poch 25/100][T][0]   Loss: 3.9929e+00   Top-1:  15.62   LR: 0.0009415        \n",
      "[Epoch 25][V][78]   Loss: 2.5042e+00   Top-1:  46.41   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.4486558738679176\n",
      "T Top-1\t24.492954739538856\n",
      "V Loss\t2.504177878952026\n",
      "V Top-1\t46.41\n",
      "\n",
      "Best acc1 47.64\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 26/100][T][1170]   Loss: 3.2734e+00   Top-1:  28.17   LR: 0.0009241        poch 26/100][T][0]   Loss: 2.9093e+00   Top-1:  35.16   LR: 0.0009331        \n",
      "[Epoch 26][V][78]   Loss: 2.5384e+00   Top-1:  46.53   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.273380043565667\n",
      "T Top-1\t28.169032877882152\n",
      "V Loss\t2.5383930828094483\n",
      "V Top-1\t46.53\n",
      "\n",
      "Best acc1 47.64\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 27/100][T][1170]   Loss: 3.3519e+00   Top-1:  26.31   LR: 0.0009146        poch 27/100][T][0]   Loss: 2.7732e+00   Top-1:  42.97   LR: 0.0009241        \n",
      "[Epoch 27][V][78]   Loss: 2.6143e+00   Top-1:  43.62   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.3519114574543134\n",
      "T Top-1\t26.31231319385141\n",
      "V Loss\t2.614325086975098\n",
      "V Top-1\t43.62\n",
      "\n",
      "Best acc1 47.64\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 28/100][T][1170]   Loss: 3.3307e+00   Top-1:  26.49   LR: 0.0009046        poch 28/100][T][0]   Loss: 3.9754e+00   Top-1:  24.22   LR: 0.0009146        \n",
      "[Epoch 28][V][78]   Loss: 2.5046e+00   Top-1:  46.80   LR: 0.0009\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.3307040934884373\n",
      "T Top-1\t26.490446199829204\n",
      "V Loss\t2.5045773979187014\n",
      "V Top-1\t46.8\n",
      "\n",
      "Best acc1 47.64\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 29/100][T][1170]   Loss: 3.4801e+00   Top-1:  23.56   LR: 0.0008941        poch 29/100][T][0]   Loss: 2.8411e+00   Top-1:  38.28   LR: 0.0009046        \n",
      "[Epoch 29][V][78]   Loss: 2.7180e+00   Top-1:  40.69   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.4801020314813575\n",
      "T Top-1\t23.564928479931684\n",
      "V Loss\t2.7180250019073484\n",
      "V Top-1\t40.69\n",
      "\n",
      "Best acc1 47.64\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 30/100][T][1170]   Loss: 3.3298e+00   Top-1:  26.62   LR: 0.0008831        poch 30/100][T][0]   Loss: 3.9764e+00   Top-1:   6.25   LR: 0.0008941        \n",
      "[Epoch 30][V][78]   Loss: 2.4799e+00   Top-1:  48.88   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.329846336737746\n",
      "T Top-1\t26.619876174210077\n",
      "V Loss\t2.4798600635528563\n",
      "V Top-1\t48.88\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 48.88\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 31/100][T][1170]   Loss: 3.3703e+00   Top-1:  26.13   LR: 0.0008717        poch 31/100][T][0]   Loss: 3.8541e+00   Top-1:  20.31   LR: 0.0008831        \n",
      "[Epoch 31][V][78]   Loss: 2.3954e+00   Top-1:  49.62   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.370289206606623\n",
      "T Top-1\t26.13484735269001\n",
      "V Loss\t2.395421049499512\n",
      "V Top-1\t49.62\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 49.62\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 32/100][T][1170]   Loss: 3.2712e+00   Top-1:  28.35   LR: 0.0008598        poch 32/100][T][0]   Loss: 3.4545e+00   Top-1:  31.25   LR: 0.0008717        \n",
      "[Epoch 32][V][78]   Loss: 2.3365e+00   Top-1:  51.25   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.2712246549119306\n",
      "T Top-1\t28.35250320239112\n",
      "V Loss\t2.3365392837524412\n",
      "V Top-1\t51.25\n",
      "\n",
      "* Best model upate *\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best acc1 51.25\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 33/100][T][1170]   Loss: 3.0964e+00   Top-1:  32.80   LR: 0.0008475        poch 33/100][T][0]   Loss: 4.1496e+00   Top-1:  20.31   LR: 0.0008598        \n",
      "[Epoch 33][V][78]   Loss: 2.3267e+00   Top-1:  51.82   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.0964409857097595\n",
      "T Top-1\t32.801158198121264\n",
      "V Loss\t2.3267239402770996\n",
      "V Top-1\t51.82\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 51.82\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 34/100][T][1170]   Loss: 3.0515e+00   Top-1:  33.93   LR: 0.0008347        poch 34/100][T][0]   Loss: 2.4727e+00   Top-1:  49.22   LR: 0.0008475        \n",
      "[Epoch 34][V][78]   Loss: 2.2910e+00   Top-1:  52.15   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t3.0515109252766806\n",
      "T Top-1\t33.934004056362085\n",
      "V Loss\t2.290971593093872\n",
      "V Top-1\t52.15\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 52.15\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 35/100][T][1170]   Loss: 2.9982e+00   Top-1:  35.24   LR: 0.0008216        poch 35/100][T][0]   Loss: 3.9577e+00   Top-1:  26.56   LR: 0.0008347        \n",
      "[Epoch 35][V][78]   Loss: 2.2348e+00   Top-1:  53.87   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.9981609349776086\n",
      "T Top-1\t35.244315755764305\n",
      "V Loss\t2.234787621688843\n",
      "V Top-1\t53.87\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 53.87\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 36/100][T][1170]   Loss: 2.9640e+00   Top-1:  35.86   LR: 0.0008080        poch 36/100][T][0]   Loss: 2.4544e+00   Top-1:  50.00   LR: 0.0008216        \n",
      "[Epoch 36][V][78]   Loss: 2.2444e+00   Top-1:  54.49   LR: 0.0008\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.9639955880191975\n",
      "T Top-1\t35.86344470538002\n",
      "V Loss\t2.2443634189605715\n",
      "V Top-1\t54.49\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 54.49\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 37/100][T][1170]   Loss: 2.9573e+00   Top-1:  36.03   LR: 0.0007941        poch 37/100][T][0]   Loss: 3.3491e+00   Top-1:   0.78   LR: 0.0008080        \n",
      "[Epoch 37][V][78]   Loss: 2.2051e+00   Top-1:  55.33   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.9572507182723125\n",
      "T Top-1\t36.032237403928264\n",
      "V Loss\t2.205108604049683\n",
      "V Top-1\t55.33\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 55.33\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 38/100][T][1170]   Loss: 2.9656e+00   Top-1:  36.05   LR: 0.0007798        poch 38/100][T][0]   Loss: 3.0665e+00   Top-1:  41.41   LR: 0.0007941        \n",
      "[Epoch 38][V][78]   Loss: 2.1908e+00   Top-1:  55.74   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.96561643672132\n",
      "T Top-1\t36.048916524338175\n",
      "V Loss\t2.1907859615325926\n",
      "V Top-1\t55.74\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 55.74\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 39/100][T][1170]   Loss: 2.8998e+00   Top-1:  37.17   LR: 0.0007652        poch 39/100][T][0]   Loss: 2.9305e+00   Top-1:  42.19   LR: 0.0007798        \n",
      "[Epoch 39][V][78]   Loss: 2.2778e+00   Top-1:  53.25   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.8998069494428234\n",
      "T Top-1\t37.16775192143467\n",
      "V Loss\t2.277775316429138\n",
      "V Top-1\t53.25\n",
      "\n",
      "Best acc1 55.74\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 40/100][T][1170]   Loss: 2.8833e+00   Top-1:  38.54   LR: 0.0007503        poch 40/100][T][0]   Loss: 3.9342e+00   Top-1:  13.28   LR: 0.0007652        \n",
      "[Epoch 40][V][78]   Loss: 2.1529e+00   Top-1:  56.59   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.883316298817896\n",
      "T Top-1\t38.53810845431256\n",
      "V Loss\t2.1529269172668455\n",
      "V Top-1\t56.59\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 56.59\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 41/100][T][1170]   Loss: 2.8883e+00   Top-1:  38.58   LR: 0.0007350        poch 41/100][T][0]   Loss: 4.1381e+00   Top-1:   6.25   LR: 0.0007502        \n",
      "[Epoch 41][V][78]   Loss: 2.1425e+00   Top-1:  56.72   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.8883395887255974\n",
      "T Top-1\t38.57613684884714\n",
      "V Loss\t2.14254494972229\n",
      "V Top-1\t56.72\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 56.72\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 42/100][T][1170]   Loss: 2.8431e+00   Top-1:  38.99   LR: 0.0007195        poch 42/100][T][0]   Loss: 3.2381e+00   Top-1:   3.91   LR: 0.0007350        \n",
      "[Epoch 42][V][78]   Loss: 2.1281e+00   Top-1:  57.58   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.843114225402233\n",
      "T Top-1\t38.98977903501281\n",
      "V Loss\t2.128058783721924\n",
      "V Top-1\t57.58\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 57.58\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 43/100][T][1170]   Loss: 2.8237e+00   Top-1:  38.64   LR: 0.0007037        poch 43/100][T][0]   Loss: 3.4869e+00   Top-1:   6.25   LR: 0.0007195        \n",
      "[Epoch 43][V][78]   Loss: 2.2000e+00   Top-1:  55.45   LR: 0.0007\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.8236912079696266\n",
      "T Top-1\t38.63818317677199\n",
      "V Loss\t2.2000468994140623\n",
      "V Top-1\t55.45\n",
      "\n",
      "Best acc1 57.58\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 44/100][T][1170]   Loss: 2.7740e+00   Top-1:  40.46   LR: 0.0006876        poch 44/100][T][0]   Loss: 4.0233e+00   Top-1:  17.97   LR: 0.0007037        \n",
      "[Epoch 44][V][78]   Loss: 2.1025e+00   Top-1:  57.99   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.773982755451707\n",
      "T Top-1\t40.464213279248504\n",
      "V Loss\t2.102522910308838\n",
      "V Top-1\t57.99\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 57.99\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 45/100][T][1170]   Loss: 2.7611e+00   Top-1:  41.56   LR: 0.0006713        poch 45/100][T][0]   Loss: 3.0356e+00   Top-1:  46.88   LR: 0.0006876        \n",
      "[Epoch 45][V][78]   Loss: 2.1343e+00   Top-1:  57.82   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.7610914443518544\n",
      "T Top-1\t41.55569491887276\n",
      "V Loss\t2.1343366565704347\n",
      "V Top-1\t57.82\n",
      "\n",
      "Best acc1 57.99\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 46/100][T][1170]   Loss: 2.7304e+00   Top-1:  41.99   LR: 0.0006549        poch 46/100][T][0]   Loss: 2.0412e+00   Top-1:  60.16   LR: 0.0006713        \n",
      "[Epoch 46][V][78]   Loss: 2.0984e+00   Top-1:  58.39   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.7303853599970203\n",
      "T Top-1\t41.98735055508113\n",
      "V Loss\t2.0984347846984863\n",
      "V Top-1\t58.39\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 58.39\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 47/100][T][1170]   Loss: 2.7085e+00   Top-1:  43.43   LR: 0.0006382        poch 47/100][T][0]   Loss: 2.1297e+00   Top-1:  55.47   LR: 0.0006548        \n",
      "[Epoch 47][V][78]   Loss: 2.0657e+00   Top-1:  59.87   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.708525618154915\n",
      "T Top-1\t43.42775939368062\n",
      "V Loss\t2.0656989570617674\n",
      "V Top-1\t59.87\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 59.87\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 48/100][T][1170]   Loss: 2.7012e+00   Top-1:  43.52   LR: 0.0006213        poch 48/100][T][0]   Loss: 1.9947e+00   Top-1:  64.84   LR: 0.0006382        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 48][V][78]   Loss: 2.0390e+00   Top-1:  59.89   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.701198714701769\n",
      "T Top-1\t43.5191609735269\n",
      "V Loss\t2.0389665306091307\n",
      "V Top-1\t59.89\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 59.89\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 49/100][T][1170]   Loss: 2.7150e+00   Top-1:  43.52   LR: 0.0006044        poch 49/100][T][0]   Loss: 3.4065e+00   Top-1:   3.12   LR: 0.0006213        \n",
      "[Epoch 49][V][78]   Loss: 2.0479e+00   Top-1:  59.59   LR: 0.0006\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.7150106210162965\n",
      "T Top-1\t43.52182963279248\n",
      "V Loss\t2.047885785675049\n",
      "V Top-1\t59.59\n",
      "\n",
      "Best acc1 59.89\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 50/100][T][1170]   Loss: 2.6100e+00   Top-1:  45.70   LR: 0.0005872        poch 50/100][T][0]   Loss: 3.4819e+00   Top-1:  39.06   LR: 0.0006043        \n",
      "[Epoch 50][V][78]   Loss: 2.0706e+00   Top-1:  59.67   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.6100178771707156\n",
      "T Top-1\t45.69945559350982\n",
      "V Loss\t2.070579395103455\n",
      "V Top-1\t59.67\n",
      "\n",
      "Best acc1 59.89\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 51/100][T][1170]   Loss: 2.6280e+00   Top-1:  45.46   LR: 0.0005700        poch 51/100][T][0]   Loss: 2.1194e+00   Top-1:  60.16   LR: 0.0005872        \n",
      "[Epoch 51][V][78]   Loss: 2.0825e+00   Top-1:  58.97   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.6280263918877464\n",
      "T Top-1\t45.45994342442357\n",
      "V Loss\t2.082504968070984\n",
      "V Top-1\t58.97\n",
      "\n",
      "Best acc1 59.89\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 52/100][T][1170]   Loss: 2.6272e+00   Top-1:  45.62   LR: 0.0005527        poch 52/100][T][0]   Loss: 3.0229e+00   Top-1:  43.75   LR: 0.0005700        \n",
      "[Epoch 52][V][78]   Loss: 2.0237e+00   Top-1:  60.74   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.627211051247855\n",
      "T Top-1\t45.61672715627669\n",
      "V Loss\t2.023744162750244\n",
      "V Top-1\t60.74\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 60.74\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 53/100][T][1170]   Loss: 2.6165e+00   Top-1:  46.43   LR: 0.0005353        poch 53/100][T][0]   Loss: 2.1024e+00   Top-1:  56.25   LR: 0.0005527        \n",
      "[Epoch 53][V][78]   Loss: 2.0586e+00   Top-1:  60.25   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.616510327711358\n",
      "T Top-1\t46.42733240819812\n",
      "V Loss\t2.058571253967285\n",
      "V Top-1\t60.25\n",
      "\n",
      "Best acc1 60.74\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 54/100][T][1170]   Loss: 2.5917e+00   Top-1:  47.11   LR: 0.0005179        poch 54/100][T][0]   Loss: 2.4279e+00   Top-1:   1.56   LR: 0.0005353        \n",
      "[Epoch 54][V][78]   Loss: 2.0228e+00   Top-1:  60.88   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.59167711038451\n",
      "T Top-1\t47.10784052092229\n",
      "V Loss\t2.0228106616973878\n",
      "V Top-1\t60.88\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 60.88\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 55/100][T][1170]   Loss: 2.5535e+00   Top-1:  48.22   LR: 0.0005005        poch 55/100][T][0]   Loss: 2.2472e+00   Top-1:  63.28   LR: 0.0005179        \n",
      "[Epoch 55][V][78]   Loss: 1.9987e+00   Top-1:  61.39   LR: 0.0005\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.553493111893013\n",
      "T Top-1\t48.21933710503843\n",
      "V Loss\t1.9986718601226807\n",
      "V Top-1\t61.39\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 61.39\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 56/100][T][1170]   Loss: 2.5073e+00   Top-1:  49.00   LR: 0.0004831        poch 56/100][T][0]   Loss: 1.8208e+00   Top-1:  64.84   LR: 0.0005005        \n",
      "[Epoch 56][V][78]   Loss: 2.0091e+00   Top-1:  61.77   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.5073214589175152\n",
      "T Top-1\t49.00392292912041\n",
      "V Loss\t2.0091101234436035\n",
      "V Top-1\t61.77\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 61.77\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 57/100][T][1170]   Loss: 2.5007e+00   Top-1:  49.20   LR: 0.0004657        poch 57/100][T][0]   Loss: 1.9604e+00   Top-1:  67.97   LR: 0.0004831        \n",
      "[Epoch 57][V][78]   Loss: 1.9934e+00   Top-1:  61.84   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.5007335182747\n",
      "T Top-1\t49.19940222032451\n",
      "V Loss\t1.9933773063659668\n",
      "V Top-1\t61.84\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 61.84\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 58/100][T][1170]   Loss: 2.5255e+00   Top-1:  48.95   LR: 0.0004483        poch 58/100][T][0]   Loss: 3.3357e+00   Top-1:  39.84   LR: 0.0004656        \n",
      "[Epoch 58][V][78]   Loss: 2.0133e+00   Top-1:  61.75   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.5254573231574104\n",
      "T Top-1\t48.953218403074295\n",
      "V Loss\t2.0133360092163084\n",
      "V Top-1\t61.75\n",
      "\n",
      "Best acc1 61.84\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 59/100][T][1170]   Loss: 2.5371e+00   Top-1:  48.31   LR: 0.0004310        poch 59/100][T][0]   Loss: 3.9776e+00   Top-1:  11.72   LR: 0.0004483        \n",
      "[Epoch 59][V][78]   Loss: 2.0084e+00   Top-1:  61.43   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.5371223477396976\n",
      "T Top-1\t48.30873719043552\n",
      "V Loss\t2.0083507469177246\n",
      "V Top-1\t61.43\n",
      "\n",
      "Best acc1 61.84\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 60/100][T][1170]   Loss: 2.4788e+00   Top-1:  49.60   LR: 0.0004138        poch 60/100][T][0]   Loss: 1.6650e+00   Top-1:  73.44   LR: 0.0004310        \n",
      "[Epoch 60][V][78]   Loss: 2.0059e+00   Top-1:  61.89   LR: 0.0004\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.4788000086858473\n",
      "T Top-1\t49.59836678052946\n",
      "V Loss\t2.0059287899017333\n",
      "V Top-1\t61.89\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 61.89\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 61/100][T][1170]   Loss: 2.4594e+00   Top-1:  50.94   LR: 0.0003966        poch 61/100][T][0]   Loss: 3.8451e+00   Top-1:  20.31   LR: 0.0004137        \n",
      "[Epoch 61][V][78]   Loss: 2.0193e+00   Top-1:  62.45   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.459377888631454\n",
      "T Top-1\t50.93736656703672\n",
      "V Loss\t2.0192568008422853\n",
      "V Top-1\t62.45\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 62.45\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 62/100][T][1170]   Loss: 2.4077e+00   Top-1:  52.05   LR: 0.0003797        poch 62/100][T][0]   Loss: 1.4777e+00   Top-1:  76.56   LR: 0.0003966        \n",
      "[Epoch 62][V][78]   Loss: 1.9509e+00   Top-1:  63.00   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.407743538372747\n",
      "T Top-1\t52.04819598633647\n",
      "V Loss\t1.9509001194000244\n",
      "V Top-1\t63.0\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 63.00\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 63/100][T][1170]   Loss: 2.4296e+00   Top-1:  51.43   LR: 0.0003628        poch 63/100][T][0]   Loss: 3.6681e+00   Top-1:  33.59   LR: 0.0003796        \n",
      "[Epoch 63][V][78]   Loss: 1.9668e+00   Top-1:  63.08   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.4295684951274032\n",
      "T Top-1\t51.43173569598633\n",
      "V Loss\t1.966826484298706\n",
      "V Top-1\t63.08\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 63.08\n",
      "********************************************************************************\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 64/100][T][1170]   Loss: 2.3995e+00   Top-1:  52.73   LR: 0.0003461        poch 64/100][T][0]   Loss: 2.3155e+00   Top-1:  64.84   LR: 0.0003628        \n",
      "[Epoch 64][V][78]   Loss: 1.9440e+00   Top-1:  63.56   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.3994969510299136\n",
      "T Top-1\t52.72736976942784\n",
      "V Loss\t1.944049277305603\n",
      "V Top-1\t63.56\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 63.56\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 65/100][T][1170]   Loss: 2.4030e+00   Top-1:  52.26   LR: 0.0003297        poch 65/100][T][0]   Loss: 1.5972e+00   Top-1:  75.00   LR: 0.0003461        \n",
      "[Epoch 65][V][78]   Loss: 1.9358e+00   Top-1:  63.70   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.4029868884945813\n",
      "T Top-1\t52.26302305721605\n",
      "V Loss\t1.9357986879348754\n",
      "V Top-1\t63.7\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 63.70\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 66/100][T][1170]   Loss: 2.4070e+00   Top-1:  51.78   LR: 0.0003134        poch 66/100][T][0]   Loss: 3.2189e+00   Top-1:   5.47   LR: 0.0003296        \n",
      "[Epoch 66][V][78]   Loss: 1.9396e+00   Top-1:  63.91   LR: 0.0003\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.406956591838247\n",
      "T Top-1\t51.777994235695985\n",
      "V Loss\t1.939638618850708\n",
      "V Top-1\t63.91\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 63.91\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 67/100][T][1170]   Loss: 2.3831e+00   Top-1:  52.93   LR: 0.0002973        poch 67/100][T][0]   Loss: 1.6940e+00   Top-1:  70.31   LR: 0.0003134        \n",
      "[Epoch 67][V][78]   Loss: 1.9417e+00   Top-1:  63.81   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.383056916506447\n",
      "T Top-1\t52.92618488471392\n",
      "V Loss\t1.9416521322250366\n",
      "V Top-1\t63.81\n",
      "\n",
      "Best acc1 63.91\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 68/100][T][1170]   Loss: 2.3610e+00   Top-1:  53.95   LR: 0.0002815        poch 68/100][T][0]   Loss: 1.6301e+00   Top-1:  82.03   LR: 0.0002973        \n",
      "[Epoch 68][V][78]   Loss: 1.9335e+00   Top-1:  64.70   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.3609744788845006\n",
      "T Top-1\t53.945612724167376\n",
      "V Loss\t1.9335323902130126\n",
      "V Top-1\t64.7\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 64.70\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 69/100][T][1170]   Loss: 2.3135e+00   Top-1:  54.82   LR: 0.0002660        poch 69/100][T][0]   Loss: 1.5628e+00   Top-1:  75.78   LR: 0.0002815        \n",
      "[Epoch 69][V][78]   Loss: 1.9124e+00   Top-1:  64.42   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.3135409880458138\n",
      "T Top-1\t54.81692997438087\n",
      "V Loss\t1.9124465774536132\n",
      "V Top-1\t64.42\n",
      "\n",
      "Best acc1 64.70\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 70/100][T][1170]   Loss: 2.3509e+00   Top-1:  54.40   LR: 0.0002508        poch 70/100][T][0]   Loss: 3.5315e+00   Top-1:  38.28   LR: 0.0002660        \n",
      "[Epoch 70][V][78]   Loss: 1.9063e+00   Top-1:  65.06   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.3508697419366094\n",
      "T Top-1\t54.400619128949614\n",
      "V Loss\t1.9063271102905273\n",
      "V Top-1\t65.06\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 65.06\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 71/100][T][1170]   Loss: 2.2980e+00   Top-1:  55.49   LR: 0.0002358        poch 71/100][T][0]   Loss: 3.2950e+00   Top-1:  49.22   LR: 0.0002507        \n",
      "[Epoch 71][V][78]   Loss: 1.8944e+00   Top-1:  65.18   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.2980355258480696\n",
      "T Top-1\t55.49210076857387\n",
      "V Loss\t1.8943564962387085\n",
      "V Top-1\t65.18\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 65.18\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 72/100][T][1170]   Loss: 2.2801e+00   Top-1:  56.73   LR: 0.0002212        poch 72/100][T][0]   Loss: 1.4070e+00   Top-1:  83.59   LR: 0.0002358        \n",
      "[Epoch 72][V][78]   Loss: 1.9135e+00   Top-1:  64.70   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.2801130476616875\n",
      "T Top-1\t56.73369449188728\n",
      "V Loss\t1.9135376573562621\n",
      "V Top-1\t64.7\n",
      "\n",
      "Best acc1 65.18\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 73/100][T][1170]   Loss: 2.2864e+00   Top-1:  55.86   LR: 0.0002069        poch 73/100][T][0]   Loss: 1.4683e+00   Top-1:  77.34   LR: 0.0002212        \n",
      "[Epoch 73][V][78]   Loss: 1.8982e+00   Top-1:  65.11   LR: 0.0002\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.286398222057561\n",
      "T Top-1\t55.86437873612297\n",
      "V Loss\t1.8982069520950318\n",
      "V Top-1\t65.11\n",
      "\n",
      "Best acc1 65.18\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 74/100][T][1170]   Loss: 2.2792e+00   Top-1:  56.35   LR: 0.0001930        poch 74/100][T][0]   Loss: 3.6442e+00   Top-1:  33.59   LR: 0.0002069        \n",
      "[Epoch 74][V][78]   Loss: 1.8974e+00   Top-1:  65.31   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.2792477734978642\n",
      "T Top-1\t56.34940755764304\n",
      "V Loss\t1.897407628250122\n",
      "V Top-1\t65.31\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 65.31\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 75/100][T][1170]   Loss: 2.2480e+00   Top-1:  57.22   LR: 0.0001794        poch 75/100][T][0]   Loss: 1.5387e+00   Top-1:  75.00   LR: 0.0001930        \n",
      "[Epoch 75][V][78]   Loss: 1.8832e+00   Top-1:  65.70   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.24797033153977\n",
      "T Top-1\t57.222059137489325\n",
      "V Loss\t1.883217226409912\n",
      "V Top-1\t65.7\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 65.70\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 76/100][T][1170]   Loss: 2.1923e+00   Top-1:  58.75   LR: 0.0001663        poch 76/100][T][0]   Loss: 2.3261e+00   Top-1:  72.66   LR: 0.0001794        \n",
      "[Epoch 76][V][78]   Loss: 1.8936e+00   Top-1:  65.76   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.192250292286926\n",
      "T Top-1\t58.74519641332195\n",
      "V Loss\t1.8936018642425536\n",
      "V Top-1\t65.76\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 65.76\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 77/100][T][1170]   Loss: 2.1762e+00   Top-1:  59.29   LR: 0.0001535        poch 77/100][T][0]   Loss: 2.5588e+00   Top-1:  72.66   LR: 0.0001663        \n",
      "[Epoch 77][V][78]   Loss: 1.8769e+00   Top-1:  66.08   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.1761784664491217\n",
      "T Top-1\t59.285599914602905\n",
      "V Loss\t1.8769309032440185\n",
      "V Top-1\t66.08\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 66.08\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 78/100][T][1170]   Loss: 2.2057e+00   Top-1:  58.60   LR: 0.0001412        poch 78/100][T][0]   Loss: 1.4727e+00   Top-1:  79.69   LR: 0.0001535        \n",
      "[Epoch 78][V][78]   Loss: 1.8885e+00   Top-1:  66.05   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.2056884419500675\n",
      "T Top-1\t58.59708582408198\n",
      "V Loss\t1.8885385890960693\n",
      "V Top-1\t66.05\n",
      "\n",
      "Best acc1 66.08\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 79/100][T][1170]   Loss: 2.1487e+00   Top-1:  59.79   LR: 0.0001293        poch 79/100][T][0]   Loss: 2.5578e+00   Top-1:  61.72   LR: 0.0001412        \n",
      "[Epoch 79][V][78]   Loss: 1.8905e+00   Top-1:  66.07   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.1487413698745326\n",
      "T Top-1\t59.78664069171648\n",
      "V Loss\t1.8904924404144288\n",
      "V Top-1\t66.07\n",
      "\n",
      "Best acc1 66.08\n",
      "********************************************************************************\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 80/100][T][1170]   Loss: 2.1826e+00   Top-1:  59.08   LR: 0.0001179        poch 80/100][T][0]   Loss: 3.4834e+00   Top-1:  43.75   LR: 0.0001293        \n",
      "[Epoch 80][V][78]   Loss: 1.8917e+00   Top-1:  65.82   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.182644553005339\n",
      "T Top-1\t59.08011315115286\n",
      "V Loss\t1.891701413345337\n",
      "V Top-1\t65.82\n",
      "\n",
      "Best acc1 66.08\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 81/100][T][1170]   Loss: 2.1707e+00   Top-1:  61.05   LR: 0.0001069        poch 81/100][T][0]   Loss: 1.5161e+00   Top-1:  77.34   LR: 0.0001179        \n",
      "[Epoch 81][V][78]   Loss: 1.8732e+00   Top-1:  66.55   LR: 0.0001\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.170652601578417\n",
      "T Top-1\t61.053586678052945\n",
      "V Loss\t1.873231018447876\n",
      "V Top-1\t66.55\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 66.55\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 82/100][T][1170]   Loss: 2.1647e+00   Top-1:  60.43   LR: 0.0000964        poch 82/100][T][0]   Loss: 1.3932e+00   Top-1:  82.03   LR: 0.0001069        \n",
      "[Epoch 82][V][78]   Loss: 1.8626e+00   Top-1:  66.78   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.1646503443192664\n",
      "T Top-1\t60.42978757472246\n",
      "V Loss\t1.8626174606323243\n",
      "V Top-1\t66.78\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 66.78\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 83/100][T][1170]   Loss: 2.1123e+00   Top-1:  60.63   LR: 0.0000864        poch 83/100][T][0]   Loss: 1.3976e+00   Top-1:  83.59   LR: 0.0000964        \n",
      "[Epoch 83][V][78]   Loss: 1.8790e+00   Top-1:  66.48   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.1123035766445195\n",
      "T Top-1\t60.63460717335611\n",
      "V Loss\t1.87898825340271\n",
      "V Top-1\t66.48\n",
      "\n",
      "Best acc1 66.78\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 84/100][T][1170]   Loss: 2.1126e+00   Top-1:  62.05   LR: 0.0000769        poch 84/100][T][0]   Loss: 1.3152e+00   Top-1:  84.38   LR: 0.0000864        \n",
      "[Epoch 84][V][78]   Loss: 1.8606e+00   Top-1:  66.92   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.1125877057253857\n",
      "T Top-1\t62.053666737830916\n",
      "V Loss\t1.8605879852294922\n",
      "V Top-1\t66.92\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 66.92\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 85/100][T][1170]   Loss: 2.1087e+00   Top-1:  61.28   LR: 0.0000679        poch 85/100][T][0]   Loss: 1.3101e+00   Top-1:  85.16   LR: 0.0000769        \n",
      "[Epoch 85][V][78]   Loss: 1.8525e+00   Top-1:  67.03   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.108669002782983\n",
      "T Top-1\t61.279755550811274\n",
      "V Loss\t1.8524507003784179\n",
      "V Top-1\t67.03\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 67.03\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 86/100][T][1170]   Loss: 2.1205e+00   Top-1:  62.33   LR: 0.0000595        poch 86/100][T][0]   Loss: 1.8103e+00   Top-1:  78.12   LR: 0.0000679        \n",
      "[Epoch 86][V][78]   Loss: 1.8502e+00   Top-1:  67.29   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.1205120084633897\n",
      "T Top-1\t62.32787147736977\n",
      "V Loss\t1.8502313709259033\n",
      "V Top-1\t67.29\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 67.29\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 87/100][T][1170]   Loss: 2.0355e+00   Top-1:  63.83   LR: 0.0000516        poch 87/100][T][0]   Loss: 1.6352e+00   Top-1:   0.00   LR: 0.0000595        \n",
      "[Epoch 87][V][78]   Loss: 1.8634e+00   Top-1:  67.30   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.035548927843011\n",
      "T Top-1\t63.82965947907771\n",
      "V Loss\t1.8633763820648193\n",
      "V Top-1\t67.3\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 67.30\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 88/100][T][1170]   Loss: 2.0938e+00   Top-1:  62.86   LR: 0.0000442        poch 88/100][T][0]   Loss: 1.3175e+00   Top-1:  83.59   LR: 0.0000515        \n",
      "[Epoch 88][V][78]   Loss: 1.8667e+00   Top-1:  67.09   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.0937897774218293\n",
      "T Top-1\t62.86093616567037\n",
      "V Loss\t1.8667499336242677\n",
      "V Top-1\t67.09\n",
      "\n",
      "Best acc1 67.30\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 89/100][T][1170]   Loss: 2.0308e+00   Top-1:  63.72   LR: 0.0000374        poch 89/100][T][0]   Loss: 1.3442e+00   Top-1:  83.59   LR: 0.0000442        \n",
      "[Epoch 89][V][78]   Loss: 1.8591e+00   Top-1:  67.32   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.0307884086002965\n",
      "T Top-1\t63.720244449188726\n",
      "V Loss\t1.8590775955200196\n",
      "V Top-1\t67.32\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 67.32\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 90/100][T][1170]   Loss: 2.0705e+00   Top-1:  62.27   LR: 0.0000311        poch 90/100][T][0]   Loss: 1.3320e+00   Top-1:  86.72   LR: 0.0000374        \n",
      "[Epoch 90][V][78]   Loss: 1.8571e+00   Top-1:  67.52   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.0704710026463315\n",
      "T Top-1\t62.27449829205807\n",
      "V Loss\t1.857131424331665\n",
      "V Top-1\t67.52\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 67.52\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 91/100][T][1170]   Loss: 2.0579e+00   Top-1:  63.46   LR: 0.0000254        poch 91/100][T][0]   Loss: 1.2804e+00   Top-1:  84.38   LR: 0.0000311        \n",
      "[Epoch 91][V][78]   Loss: 1.8578e+00   Top-1:  67.44   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.0579407780115466\n",
      "T Top-1\t63.45671434671221\n",
      "V Loss\t1.8577815269470215\n",
      "V Top-1\t67.44\n",
      "\n",
      "Best acc1 67.52\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 92/100][T][1170]   Loss: 2.1047e+00   Top-1:  62.71   LR: 0.0000203        poch 92/100][T][0]   Loss: 1.8972e+00   Top-1:  77.34   LR: 0.0000254        \n",
      "[Epoch 92][V][78]   Loss: 1.8327e+00   Top-1:  68.06   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.1046904458844895\n",
      "T Top-1\t62.70548676345004\n",
      "V Loss\t1.8327301370620728\n",
      "V Top-1\t68.06\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 68.06\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 93/100][T][1170]   Loss: 2.0607e+00   Top-1:  62.81   LR: 0.0000158        poch 93/100][T][0]   Loss: 3.5518e+00   Top-1:  31.25   LR: 0.0000203        \n",
      "[Epoch 93][V][78]   Loss: 1.8383e+00   Top-1:  67.75   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.060691735970475\n",
      "T Top-1\t62.813567463706235\n",
      "V Loss\t1.8383052890777587\n",
      "V Top-1\t67.75\n",
      "\n",
      "Best acc1 68.06\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 94/100][T][1170]   Loss: 2.0494e+00   Top-1:  64.02   LR: 0.0000119        poch 94/100][T][0]   Loss: 1.4011e+00   Top-1:  88.28   LR: 0.0000158        \n",
      "[Epoch 94][V][78]   Loss: 1.8398e+00   Top-1:  67.65   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.049361078785995\n",
      "T Top-1\t64.01779995730145\n",
      "V Loss\t1.8398004501342773\n",
      "V Top-1\t67.65\n",
      "\n",
      "Best acc1 68.06\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 95/100][T][1170]   Loss: 2.0201e+00   Top-1:  64.66   LR: 0.0000086        poch 95/100][T][0]   Loss: 3.7076e+00   Top-1:  12.50   LR: 0.0000119        \n",
      "[Epoch 95][V][78]   Loss: 1.8301e+00   Top-1:  68.01   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.020117363086834\n",
      "T Top-1\t64.65560952177626\n",
      "V Loss\t1.8300656475067139\n",
      "V Top-1\t68.01\n",
      "\n",
      "Best acc1 68.06\n",
      "********************************************************************************\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 96/100][T][1170]   Loss: 2.0453e+00   Top-1:  64.04   LR: 0.0000059        poch 96/100][T][0]   Loss: 1.3929e+00   Top-1:  82.03   LR: 0.0000086        \n",
      "[Epoch 96][V][78]   Loss: 1.8357e+00   Top-1:  67.98   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.0453321741341934\n",
      "T Top-1\t64.03581340734415\n",
      "V Loss\t1.8356722034454345\n",
      "V Top-1\t67.98\n",
      "\n",
      "Best acc1 68.06\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 97/100][T][1170]   Loss: 2.0485e+00   Top-1:  63.96   LR: 0.0000037        poch 97/100][T][0]   Loss: 3.6277e+00   Top-1:  29.69   LR: 0.0000059        \n",
      "[Epoch 97][V][78]   Loss: 1.8300e+00   Top-1:  68.20   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.0484756939844835\n",
      "T Top-1\t63.956420794193\n",
      "V Loss\t1.8299925727844237\n",
      "V Top-1\t68.2\n",
      "\n",
      "* Best model upate *\n",
      "Best acc1 68.20\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 98/100][T][1170]   Loss: 2.0794e+00   Top-1:  63.46   LR: 0.0000022        poch 98/100][T][0]   Loss: 3.5719e+00   Top-1:  12.50   LR: 0.0000037        \n",
      "[Epoch 98][V][78]   Loss: 1.8309e+00   Top-1:  67.95   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.079406459447566\n",
      "T Top-1\t63.46071733561059\n",
      "V Loss\t1.8308588829040526\n",
      "V Top-1\t67.95\n",
      "\n",
      "Best acc1 68.20\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 99/100][T][1170]   Loss: 2.0433e+00   Top-1:  64.23   LR: 0.0000013        poch 99/100][T][0]   Loss: 1.2294e+00   Top-1:  85.94   LR: 0.0000022        \n",
      "[Epoch 99][V][78]   Loss: 1.8382e+00   Top-1:  68.01   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.043344089661166\n",
      "T Top-1\t64.22995836891546\n",
      "V Loss\t1.8382494184494018\n",
      "V Top-1\t68.01\n",
      "\n",
      "Best acc1 68.20\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "[Epoch 100/100][T][1170]   Loss: 2.0727e+00   Top-1:  63.34   LR: 0.0000010        och 100/100][T][0]   Loss: 1.2766e+00   Top-1:  88.28   LR: 0.0000013        \n",
      "[Epoch 100][V][78]   Loss: 1.8358e+00   Top-1:  67.96   LR: 0.0000\n",
      "\n",
      "\u001b[34m\n",
      "********************************************************************************\n",
      "T Loss\t2.072735129107175\n",
      "T Top-1\t63.33996050384287\n",
      "V Loss\t1.835841138267517\n",
      "V Top-1\t67.96\n",
      "\n",
      "Best acc1 68.20\n",
      "********************************************************************************\n",
      "\u001b[0m\n",
      "100%|| 100/100 [7:25:58<00:00, 267.59s/it]\n",
      "\u001b[31m********************************************************************************\n",
      "best top-1: 68.20, final top-1: 67.96\n",
      "********************************************************************************\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python main.py --model resnet --dataset CIFAR100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
